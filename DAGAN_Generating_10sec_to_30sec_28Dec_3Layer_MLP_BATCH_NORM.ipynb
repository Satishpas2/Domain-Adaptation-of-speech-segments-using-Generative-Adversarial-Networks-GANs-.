{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('/home/satishk/GAN_lre/gan_csv/GAN_10sec_ivectors_X_train_29Dec.csv')\n",
    "#train_afds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lre = train_lre.iloc[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('/home/satishk/GAN_lre/gan_csv/GAN_30sec_ivectors_Y_train_29Dec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "y_train=y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (114301, 500)\n",
      "114301 train 10sec\n",
      "114301  train 30sec\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "#X_val /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train 10sec')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "print(y_train.shape[0], ' train 30sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the Dataset\n",
    "X_train,  y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(FC_Network, self).__init__()\n",
    "\n",
    "        D_in = D_out = 500        # param['patch_length'] * param['n_channels'] * (param['n_fft'] / 2 + 1)\n",
    "        num_nodes_fnn = 512\n",
    "        self.layer_1 = torch.nn.Linear(D_in, num_nodes_fnn)\n",
    "        self.bn_1 = torch.nn.BatchNorm1d(num_nodes_fnn)\n",
    "        self.layer_2 = torch.nn.Linear(num_nodes_fnn, num_nodes_fnn)\n",
    "        self.bn_2 = torch.nn.BatchNorm1d(num_nodes_fnn)\n",
    "        self.layer_3 = torch.nn.Linear(num_nodes_fnn, num_nodes_fnn)\n",
    "        self.bn_3 = torch.nn.BatchNorm1d(num_nodes_fnn)\n",
    "        self.layer_4 = torch.nn.Linear(num_nodes_fnn,num_nodes_fnn)\n",
    "        self.bn_4 = torch.nn.BatchNorm1d(num_nodes_fnn)\n",
    "        self.output_layer = torch.nn.Linear(num_nodes_fnn, D_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        #out = x.view(x.size(0), -1)\n",
    "        out = self.bn_1(self.relu(self.layer_1(out)))\n",
    "        out = self.bn_2(self.relu(self.layer_2(out)))\n",
    "        out = self.bn_3(self.relu(self.layer_3(out)))\n",
    "        out = self.bn_4(self.relu(self.layer_4(out)))\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        #out = (self.relu(self.layer_1(out)))\n",
    "        #out = (self.relu(self.layer_2(out)))\n",
    "        #out = (self.relu(self.layer_3(out)))\n",
    "        #out = (self.relu(self.layer_4(out)))\n",
    "        #out = (self.output_layer(out))\n",
    "        #out = out.view(x.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = FC_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of FC_Network(\n",
       "  (layer_1): Linear(in_features=500, out_features=512)\n",
       "  (bn_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer_2): Linear(in_features=512, out_features=512)\n",
       "  (bn_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer_3): Linear(in_features=512, out_features=512)\n",
       "  (bn_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer_4): Linear(in_features=512, out_features=512)\n",
       "  (bn_4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (output_layer): Linear(in_features=512, out_features=500)\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Implementing Generator as Encoder-Decoder pair inspired from DAGAN base paper\n",
    "\n",
    "#Encoder should be able to take a batch of input(of dim 500-ivector) and be able to produce its representation r\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(500, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "    def forward(self, x):\n",
    "        return self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "    \n",
    "#We will use Decoder as Generator    \n",
    "#Decoder should be able to take input of dim 128(r) and 100(z),concatenate r and z and \n",
    "#produce an output od dim 500-ivector    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(228,256 )\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512,500)\n",
    "    def forward(self, inputs):\n",
    "        #inputs = torch.cat([z, x], 1)\n",
    "        return F.sigmoid(F.relu(self.fc2(F.relu(self.fc1(inputs)))))\n",
    "\n",
    "#input to Generator alias Decoder is inputs = torch.cat([z, r], 1)\n",
    "#Where r is the output of encoder given x i.e., representation of x encoded by encoder\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#As we defined the Encoder and Decoder we now Implement the Generator \n",
    "\n",
    "class Generator(nn.Module):\n",
    "                                \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = Encoder()\n",
    "        self.fc2 = Decoder()\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        inputs = torch.cat([z, x], 1)\n",
    "        return self.fc2(self.fc1(inputs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Implementation of Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #self.inputs = torch.cat([z, c], 1)\n",
    "        self.fc1 = torch.nn.Linear(500, 512)\n",
    "        self.fc2 = torch.nn.Linear(512,512)\n",
    "        self.fc3 = torch.nn.Linear(512,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #inputs = torch.cat([X, c], 1)\n",
    "        return F.sigmoid(self.fc3(F.relu(self.fc2(F.relu(self.fc1(x))))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "G.modules\n",
    "\n",
    "D\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizers\n",
    "G_solver = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "#D_solver = torch.optim.Adam(D.parameters(), lr=learning_rate/2, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114301"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114301"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "G_loss: [ 0.83523405]\n",
      "epoch: 1\n",
      "G_loss: [ 0.83495981]\n",
      "epoch: 2\n",
      "G_loss: [ 0.83470392]\n",
      "epoch: 3\n",
      "G_loss: [ 0.83439136]\n",
      "epoch: 4\n",
      "G_loss: [ 0.83399147]\n",
      "epoch: 5\n",
      "G_loss: [ 0.83351159]\n",
      "epoch: 6\n",
      "G_loss: [ 0.83307064]\n",
      "epoch: 7\n",
      "G_loss: [ 0.8327632]\n",
      "epoch: 8\n",
      "G_loss: [ 0.8324762]\n",
      "epoch: 9\n",
      "G_loss: [ 0.83221877]\n",
      "epoch: 10\n",
      "G_loss: [ 0.83195835]\n",
      "epoch: 11\n",
      "G_loss: [ 0.83167452]\n",
      "epoch: 12\n",
      "G_loss: [ 0.83137286]\n",
      "epoch: 13\n",
      "G_loss: [ 0.83109105]\n",
      "epoch: 14\n",
      "G_loss: [ 0.83077073]\n",
      "epoch: 15\n",
      "G_loss: [ 0.83045936]\n",
      "epoch: 16\n",
      "G_loss: [ 0.83013326]\n",
      "epoch: 17\n",
      "G_loss: [ 0.8298192]\n",
      "epoch: 18\n",
      "G_loss: [ 0.82954907]\n",
      "epoch: 19\n",
      "G_loss: [ 0.82929784]\n",
      "epoch: 20\n",
      "G_loss: [ 0.82902992]\n",
      "epoch: 21\n",
      "G_loss: [ 0.82880223]\n",
      "epoch: 22\n",
      "G_loss: [ 0.82851022]\n",
      "epoch: 23\n",
      "G_loss: [ 0.82820642]\n",
      "epoch: 24\n",
      "G_loss: [ 0.82798886]\n",
      "epoch: 25\n",
      "G_loss: [ 0.8277989]\n",
      "epoch: 26\n",
      "G_loss: [ 0.8276667]\n",
      "epoch: 27\n",
      "G_loss: [ 0.82746792]\n",
      "epoch: 28\n",
      "G_loss: [ 0.82726955]\n",
      "epoch: 29\n",
      "G_loss: [ 0.82703024]\n",
      "epoch: 30\n",
      "G_loss: [ 0.82677889]\n",
      "epoch: 31\n",
      "G_loss: [ 0.82654834]\n",
      "epoch: 32\n",
      "G_loss: [ 0.82636195]\n",
      "epoch: 33\n",
      "G_loss: [ 0.8261978]\n",
      "epoch: 34\n",
      "G_loss: [ 0.82603127]\n",
      "epoch: 35\n",
      "G_loss: [ 0.82589704]\n",
      "epoch: 36\n",
      "G_loss: [ 0.82573646]\n",
      "epoch: 37\n",
      "G_loss: [ 0.82560563]\n",
      "epoch: 38\n",
      "G_loss: [ 0.82546765]\n",
      "epoch: 39\n",
      "G_loss: [ 0.82522905]\n",
      "epoch: 40\n",
      "G_loss: [ 0.82496125]\n",
      "epoch: 41\n",
      "G_loss: [ 0.82470834]\n",
      "epoch: 42\n",
      "G_loss: [ 0.82454675]\n",
      "epoch: 43\n",
      "G_loss: [ 0.82439607]\n",
      "epoch: 44\n",
      "G_loss: [ 0.82426447]\n",
      "epoch: 45\n",
      "G_loss: [ 0.82413703]\n",
      "epoch: 46\n",
      "G_loss: [ 0.82401735]\n",
      "epoch: 47\n",
      "G_loss: [ 0.82391208]\n",
      "epoch: 48\n",
      "G_loss: [ 0.82378721]\n",
      "epoch: 49\n",
      "G_loss: [ 0.82366699]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mb_size = 256\n",
    "batch_size = mb_size\n",
    "# Start training\n",
    "for epoch in range(50):\n",
    "    \n",
    "    \n",
    "\n",
    "    print('epoch:',epoch)\n",
    "    #for i in range(XX_train):\n",
    "    # Build mini-batch dataset\n",
    "    #batch_size = images.size(0)\n",
    "    #images = to_var(images.view(batch_size, -1))\n",
    "\n",
    "    it=0\n",
    "    while it+batch_size < len(X_train) :\n",
    "        \n",
    "\n",
    "        start= it\n",
    "        end= it + batch_size\n",
    "\n",
    "\n",
    "        #z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        X = X_train[start:end]\n",
    "\n",
    "        c = y_train[start:end]\n",
    "        X = Variable(torch.from_numpy(X))\n",
    "        c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "\n",
    "        # Dicriminator forward-loss-backward-update\n",
    "        G_sample = G(X)\n",
    "        #D_real = D(X, c)\n",
    "        #D_fake = D(G_sample, c)\n",
    "\n",
    "        #D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "        #D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "        #D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        #D_loss.backward()\n",
    "        #D_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        #D.zero_grad()\n",
    "\n",
    "        # Generator forward-loss-backward-update\n",
    "        #z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        #G_sample = G(z, c)\n",
    "        #D_fake = D(G_sample, c)\n",
    "        G_loss = criterion(G_sample, c)\n",
    "        #G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "        \n",
    "        G_solver.zero_grad()\n",
    "        # Housekeeping - reset gradient\n",
    "        #D.zero_grad()\n",
    "\n",
    "        #Print and plot every now and then\n",
    "        #if it % 2 == 0:\n",
    "\n",
    "        #print('Iter-{}; G_loss: {}'.format(it, G_loss.data.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "        it+= batch_size\n",
    "    print('G_loss: {}'.format(G_loss.data.numpy()))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = mb_size\n",
    "# Start training\n",
    "for epoch in range(10):\n",
    "    \n",
    "    \n",
    "\n",
    "    print('epoch:',epoch)\n",
    "    #for i in range(XX_train):\n",
    "    # Build mini-batch dataset\n",
    "    #batch_size = images.size(0)\n",
    "    #images = to_var(images.view(batch_size, -1))\n",
    "\n",
    "    it=0\n",
    "    while it+batch_size < len(X_train) :\n",
    "        \n",
    "\n",
    "        start= it\n",
    "        end= it + batch_size\n",
    "\n",
    "\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        X = X_train[start:end]\n",
    "\n",
    "        c = Y_train[start:end]\n",
    "        X = Variable(torch.from_numpy(X))\n",
    "        c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "\n",
    "        # Dicriminator forward-loss-backward-update\n",
    "        G_sample = G(z, c)\n",
    "        D_real = D(X, c)\n",
    "        D_fake = D(G_sample, c)\n",
    "\n",
    "        D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "        D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Generator forward-loss-backward-update\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        G_sample = G(z, c)\n",
    "        D_fake = D(G_sample, c)\n",
    "\n",
    "        G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        D.zero_grad()\n",
    "\n",
    "        #Print and plot every now and then\n",
    "        #if it % 2 == 0:\n",
    "\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "        it+= batch_size\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF=pd.DataFrame()\n",
    "samples_per_class = 1000\n",
    "#c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "#c[:, np.random.randint(0, 10)] = 1.\n",
    "for i in range(14):\n",
    "    #print(i)\n",
    "    c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "    c[:, i] = 1.\n",
    "    c_df=pd.DataFrame(c)\n",
    "    df_SF = df_SF.append(c_df,ignore_index = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = df_SF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen = Variable(torch.randn(df_SF.shape[0], Z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = Variable(torch.from_numpy(c_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = G(z_gen, c_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the generated iVectors we will try to check the acc by MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = samples.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1 = c_gen.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "Y_train = pd.DataFrame(Y_train)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = pd.DataFrame(train_X1)\n",
    "train_y1 = pd.DataFrame(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train.append(train_X1, ignore_index=True)\n",
    "train_y = Y_train.append(train_y1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X,  train_y = shuffle(train_X, train_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.values\n",
    "train_y = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_filepath = '/home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5'\n",
    "saveBestModel = ModelCheckpoint(best_weights_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epoch=30"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Checking Baseline Accuracy with only training data\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Baseline ERROR %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train_X1 and train_y1 are the augmented data alone to check accuracy only on augmented data \n",
    "#feed the model.fit only with these\n",
    "train_X1 = train_X1.values\n",
    "train_y1 = train_y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Accuracy with training+augmented data train_X and train_y are 'train + augmented' data\n",
    "history = model.fit(train_X, train_y, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame label accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('ERROR after Data Augmentation %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
