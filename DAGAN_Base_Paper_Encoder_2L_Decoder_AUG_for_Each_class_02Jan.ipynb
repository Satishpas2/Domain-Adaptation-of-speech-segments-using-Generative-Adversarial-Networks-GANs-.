{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lre = pd.read_csv('/home/satishk/GAN_lre/gan_csv/GAN_30sec_ivectors_X_train_04Jan_labels_ids_combined_extraction.csv')\n",
    "#train_afds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lre = train_lre.iloc[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>ids</th>\n",
       "      <th>year</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.639420</td>\n",
       "      <td>0.345684</td>\n",
       "      <td>-0.517645</td>\n",
       "      <td>-0.737002</td>\n",
       "      <td>1.313001</td>\n",
       "      <td>1.655732</td>\n",
       "      <td>0.615168</td>\n",
       "      <td>0.799339</td>\n",
       "      <td>1.648419</td>\n",
       "      <td>1.314986</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.322407</td>\n",
       "      <td>-0.717348</td>\n",
       "      <td>-3.843951</td>\n",
       "      <td>-1.471274</td>\n",
       "      <td>0.945421</td>\n",
       "      <td>zkllk</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>eng-usg</td>\n",
       "      <td>zkllk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113610</td>\n",
       "      <td>0.738034</td>\n",
       "      <td>0.584856</td>\n",
       "      <td>-0.248521</td>\n",
       "      <td>0.256616</td>\n",
       "      <td>1.060159</td>\n",
       "      <td>-0.416211</td>\n",
       "      <td>0.133670</td>\n",
       "      <td>0.188327</td>\n",
       "      <td>0.805241</td>\n",
       "      <td>...</td>\n",
       "      <td>1.965741</td>\n",
       "      <td>3.525555</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>1.676099</td>\n",
       "      <td>-1.116371</td>\n",
       "      <td>lid05e1_lid00562</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>eng-usg</td>\n",
       "      <td>lid05e1_lid00562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.061305</td>\n",
       "      <td>-0.140605</td>\n",
       "      <td>-0.627104</td>\n",
       "      <td>-0.682908</td>\n",
       "      <td>1.337613</td>\n",
       "      <td>1.261543</td>\n",
       "      <td>-0.651291</td>\n",
       "      <td>0.307040</td>\n",
       "      <td>-0.980717</td>\n",
       "      <td>0.335530</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.018432</td>\n",
       "      <td>-1.841361</td>\n",
       "      <td>0.814595</td>\n",
       "      <td>-0.200731</td>\n",
       "      <td>1.180795</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.958049</td>\n",
       "      <td>-0.608315</td>\n",
       "      <td>-0.113532</td>\n",
       "      <td>0.167272</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>1.116094</td>\n",
       "      <td>-0.847214</td>\n",
       "      <td>-0.041357</td>\n",
       "      <td>-0.542335</td>\n",
       "      <td>0.199105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548767</td>\n",
       "      <td>-1.254447</td>\n",
       "      <td>1.644647</td>\n",
       "      <td>0.555434</td>\n",
       "      <td>1.159140</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.842436</td>\n",
       "      <td>-0.417598</td>\n",
       "      <td>-0.427428</td>\n",
       "      <td>-0.485898</td>\n",
       "      <td>1.204330</td>\n",
       "      <td>1.439188</td>\n",
       "      <td>-1.056642</td>\n",
       "      <td>0.111603</td>\n",
       "      <td>-0.404313</td>\n",
       "      <td>-0.307075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297026</td>\n",
       "      <td>-1.858708</td>\n",
       "      <td>0.209784</td>\n",
       "      <td>0.583939</td>\n",
       "      <td>1.398789</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.639420  0.345684 -0.517645 -0.737002  1.313001  1.655732  0.615168   \n",
       "1  0.113610  0.738034  0.584856 -0.248521  0.256616  1.060159 -0.416211   \n",
       "2  2.061305 -0.140605 -0.627104 -0.682908  1.337613  1.261543 -0.651291   \n",
       "3  1.958049 -0.608315 -0.113532  0.167272  0.911100  1.116094 -0.847214   \n",
       "4  1.842436 -0.417598 -0.427428 -0.485898  1.204330  1.439188 -1.056642   \n",
       "\n",
       "          7         8         9        ...              495       496  \\\n",
       "0  0.799339  1.648419  1.314986        ...        -1.322407 -0.717348   \n",
       "1  0.133670  0.188327  0.805241        ...         1.965741  3.525555   \n",
       "2  0.307040 -0.980717  0.335530        ...        -3.018432 -1.841361   \n",
       "3 -0.041357 -0.542335  0.199105        ...        -0.548767 -1.254447   \n",
       "4  0.111603 -0.404313 -0.307075        ...         0.297026 -1.858708   \n",
       "\n",
       "        497       498       499               ids        year  data     lang  \\\n",
       "0 -3.843951 -1.471274  0.945421             zkllk  LDC2017E22  data  eng-usg   \n",
       "1  0.006925  1.676099 -1.116371  lid05e1_lid00562  LDC2017E22  data  eng-usg   \n",
       "2  0.814595 -0.200731  1.180795        fla_0240-a  LDC2017E22  data  ara-apc   \n",
       "3  1.644647  0.555434  1.159140        fla_0240-a  LDC2017E22  data  ara-apc   \n",
       "4  0.209784  0.583939  1.398789        fla_0240-a  LDC2017E22  data  ara-apc   \n",
       "\n",
       "            lang_id  \n",
       "0             zkllk  \n",
       "1  lid05e1_lid00562  \n",
       "2        fla_0240-a  \n",
       "3        fla_0240-a  \n",
       "4        fla_0240-a  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lre.groupby(['langid']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing a single class\n",
    "train_lre = train_lre.loc[train_lre['lang'] == 'ara-apc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>ids</th>\n",
       "      <th>year</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.061305</td>\n",
       "      <td>-0.140605</td>\n",
       "      <td>-0.627104</td>\n",
       "      <td>-0.682908</td>\n",
       "      <td>1.337613</td>\n",
       "      <td>1.261543</td>\n",
       "      <td>-0.651291</td>\n",
       "      <td>0.307040</td>\n",
       "      <td>-0.980717</td>\n",
       "      <td>0.335530</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.018432</td>\n",
       "      <td>-1.841361</td>\n",
       "      <td>0.814595</td>\n",
       "      <td>-0.200731</td>\n",
       "      <td>1.180795</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.958049</td>\n",
       "      <td>-0.608315</td>\n",
       "      <td>-0.113532</td>\n",
       "      <td>0.167272</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>1.116094</td>\n",
       "      <td>-0.847214</td>\n",
       "      <td>-0.041357</td>\n",
       "      <td>-0.542335</td>\n",
       "      <td>0.199105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548767</td>\n",
       "      <td>-1.254447</td>\n",
       "      <td>1.644647</td>\n",
       "      <td>0.555434</td>\n",
       "      <td>1.159140</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.842436</td>\n",
       "      <td>-0.417598</td>\n",
       "      <td>-0.427428</td>\n",
       "      <td>-0.485898</td>\n",
       "      <td>1.204330</td>\n",
       "      <td>1.439188</td>\n",
       "      <td>-1.056642</td>\n",
       "      <td>0.111603</td>\n",
       "      <td>-0.404313</td>\n",
       "      <td>-0.307075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297026</td>\n",
       "      <td>-1.858708</td>\n",
       "      <td>0.209784</td>\n",
       "      <td>0.583939</td>\n",
       "      <td>1.398789</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.953034</td>\n",
       "      <td>-0.596223</td>\n",
       "      <td>-0.535057</td>\n",
       "      <td>-0.371161</td>\n",
       "      <td>0.592551</td>\n",
       "      <td>1.366468</td>\n",
       "      <td>-1.361346</td>\n",
       "      <td>0.299445</td>\n",
       "      <td>-0.305046</td>\n",
       "      <td>-0.537489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723645</td>\n",
       "      <td>-2.709617</td>\n",
       "      <td>1.595344</td>\n",
       "      <td>-0.267579</td>\n",
       "      <td>0.810446</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.990754</td>\n",
       "      <td>-0.430986</td>\n",
       "      <td>-0.371372</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>1.007754</td>\n",
       "      <td>1.237592</td>\n",
       "      <td>-0.698143</td>\n",
       "      <td>-0.091484</td>\n",
       "      <td>-0.547547</td>\n",
       "      <td>-0.248731</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.230570</td>\n",
       "      <td>-3.184290</td>\n",
       "      <td>2.312549</td>\n",
       "      <td>-1.312105</td>\n",
       "      <td>1.701691</td>\n",
       "      <td>fla_0240-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0240-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.466107</td>\n",
       "      <td>-0.884151</td>\n",
       "      <td>-0.357131</td>\n",
       "      <td>-0.577324</td>\n",
       "      <td>-0.220201</td>\n",
       "      <td>1.070911</td>\n",
       "      <td>0.060307</td>\n",
       "      <td>0.799765</td>\n",
       "      <td>-0.783718</td>\n",
       "      <td>-0.765216</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.309657</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>-1.560878</td>\n",
       "      <td>-1.698744</td>\n",
       "      <td>-0.523284</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.516523</td>\n",
       "      <td>-0.627713</td>\n",
       "      <td>-0.105311</td>\n",
       "      <td>-0.322296</td>\n",
       "      <td>-0.553714</td>\n",
       "      <td>1.011796</td>\n",
       "      <td>0.757867</td>\n",
       "      <td>1.013907</td>\n",
       "      <td>-1.236593</td>\n",
       "      <td>-0.911828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579321</td>\n",
       "      <td>-0.415504</td>\n",
       "      <td>-0.113781</td>\n",
       "      <td>-0.765633</td>\n",
       "      <td>-0.917091</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.725326</td>\n",
       "      <td>-0.702411</td>\n",
       "      <td>-0.345675</td>\n",
       "      <td>-0.701975</td>\n",
       "      <td>0.130706</td>\n",
       "      <td>1.177092</td>\n",
       "      <td>0.764530</td>\n",
       "      <td>1.189533</td>\n",
       "      <td>-1.282171</td>\n",
       "      <td>-1.082967</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.098577</td>\n",
       "      <td>0.045006</td>\n",
       "      <td>-0.065501</td>\n",
       "      <td>-0.826183</td>\n",
       "      <td>0.936744</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.641780</td>\n",
       "      <td>-0.386634</td>\n",
       "      <td>-0.121360</td>\n",
       "      <td>-0.916782</td>\n",
       "      <td>0.394523</td>\n",
       "      <td>1.070178</td>\n",
       "      <td>1.007852</td>\n",
       "      <td>1.380711</td>\n",
       "      <td>-1.707224</td>\n",
       "      <td>-1.127637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284225</td>\n",
       "      <td>-0.634877</td>\n",
       "      <td>1.705949</td>\n",
       "      <td>0.416899</td>\n",
       "      <td>-1.166602</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.578299</td>\n",
       "      <td>-0.598327</td>\n",
       "      <td>-0.278434</td>\n",
       "      <td>-0.740745</td>\n",
       "      <td>-0.022859</td>\n",
       "      <td>1.239188</td>\n",
       "      <td>0.778646</td>\n",
       "      <td>1.184929</td>\n",
       "      <td>-1.456520</td>\n",
       "      <td>-0.931703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582347</td>\n",
       "      <td>-0.709782</td>\n",
       "      <td>-1.074811</td>\n",
       "      <td>0.894534</td>\n",
       "      <td>1.752265</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.672127</td>\n",
       "      <td>-0.861736</td>\n",
       "      <td>-0.339636</td>\n",
       "      <td>-0.154396</td>\n",
       "      <td>-0.494528</td>\n",
       "      <td>1.110264</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.924851</td>\n",
       "      <td>-1.020621</td>\n",
       "      <td>-1.233343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.052173</td>\n",
       "      <td>-0.343894</td>\n",
       "      <td>-0.670818</td>\n",
       "      <td>1.035438</td>\n",
       "      <td>-0.093136</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.743000</td>\n",
       "      <td>-0.311738</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.386368</td>\n",
       "      <td>-0.423488</td>\n",
       "      <td>0.935386</td>\n",
       "      <td>0.303447</td>\n",
       "      <td>1.620867</td>\n",
       "      <td>-0.658274</td>\n",
       "      <td>-1.129338</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.745930</td>\n",
       "      <td>-1.223212</td>\n",
       "      <td>0.818071</td>\n",
       "      <td>-0.038440</td>\n",
       "      <td>2.021269</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.640753</td>\n",
       "      <td>-0.459186</td>\n",
       "      <td>-0.401053</td>\n",
       "      <td>-0.540534</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>1.096528</td>\n",
       "      <td>0.976757</td>\n",
       "      <td>1.216016</td>\n",
       "      <td>-1.263392</td>\n",
       "      <td>-0.517696</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.816436</td>\n",
       "      <td>1.020891</td>\n",
       "      <td>2.095646</td>\n",
       "      <td>-0.707287</td>\n",
       "      <td>0.815685</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.613265</td>\n",
       "      <td>-0.541661</td>\n",
       "      <td>-0.056440</td>\n",
       "      <td>0.017932</td>\n",
       "      <td>-0.516878</td>\n",
       "      <td>0.855285</td>\n",
       "      <td>0.820343</td>\n",
       "      <td>0.775795</td>\n",
       "      <td>-1.310810</td>\n",
       "      <td>-0.871257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611984</td>\n",
       "      <td>1.610372</td>\n",
       "      <td>-0.755875</td>\n",
       "      <td>-0.155681</td>\n",
       "      <td>-0.354927</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.597133</td>\n",
       "      <td>-0.405047</td>\n",
       "      <td>-0.210105</td>\n",
       "      <td>-0.312792</td>\n",
       "      <td>-0.244037</td>\n",
       "      <td>0.881740</td>\n",
       "      <td>0.549423</td>\n",
       "      <td>0.744476</td>\n",
       "      <td>-1.494494</td>\n",
       "      <td>-1.283757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230264</td>\n",
       "      <td>0.149276</td>\n",
       "      <td>0.371657</td>\n",
       "      <td>-0.421422</td>\n",
       "      <td>-0.564310</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.669301</td>\n",
       "      <td>-0.457039</td>\n",
       "      <td>-0.070314</td>\n",
       "      <td>-0.546059</td>\n",
       "      <td>-0.462010</td>\n",
       "      <td>1.072645</td>\n",
       "      <td>0.876348</td>\n",
       "      <td>1.564480</td>\n",
       "      <td>-1.407819</td>\n",
       "      <td>-1.305188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901232</td>\n",
       "      <td>-0.325534</td>\n",
       "      <td>-1.193512</td>\n",
       "      <td>-0.697982</td>\n",
       "      <td>0.672191</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.787922</td>\n",
       "      <td>-0.172470</td>\n",
       "      <td>-0.123671</td>\n",
       "      <td>-0.219415</td>\n",
       "      <td>-0.139755</td>\n",
       "      <td>1.013841</td>\n",
       "      <td>0.897521</td>\n",
       "      <td>1.747409</td>\n",
       "      <td>-1.457581</td>\n",
       "      <td>-1.142132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124980</td>\n",
       "      <td>1.850188</td>\n",
       "      <td>1.517514</td>\n",
       "      <td>0.647885</td>\n",
       "      <td>-0.320918</td>\n",
       "      <td>fla_0073-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0073-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.124002</td>\n",
       "      <td>-0.415169</td>\n",
       "      <td>-0.087091</td>\n",
       "      <td>-0.039534</td>\n",
       "      <td>-1.062281</td>\n",
       "      <td>1.317047</td>\n",
       "      <td>-1.045120</td>\n",
       "      <td>0.859918</td>\n",
       "      <td>1.494985</td>\n",
       "      <td>-0.072534</td>\n",
       "      <td>...</td>\n",
       "      <td>1.724883</td>\n",
       "      <td>1.997481</td>\n",
       "      <td>1.341118</td>\n",
       "      <td>1.944841</td>\n",
       "      <td>0.744884</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.992214</td>\n",
       "      <td>-0.628076</td>\n",
       "      <td>-0.051747</td>\n",
       "      <td>0.235573</td>\n",
       "      <td>-1.335204</td>\n",
       "      <td>1.179586</td>\n",
       "      <td>-0.787103</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>1.335336</td>\n",
       "      <td>-0.923129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474895</td>\n",
       "      <td>-1.573427</td>\n",
       "      <td>0.032893</td>\n",
       "      <td>0.912990</td>\n",
       "      <td>-0.282434</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.741754</td>\n",
       "      <td>-0.881252</td>\n",
       "      <td>-0.384082</td>\n",
       "      <td>0.414502</td>\n",
       "      <td>-1.561445</td>\n",
       "      <td>1.122099</td>\n",
       "      <td>-1.473811</td>\n",
       "      <td>0.434503</td>\n",
       "      <td>2.450365</td>\n",
       "      <td>-0.348727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807964</td>\n",
       "      <td>1.480003</td>\n",
       "      <td>0.440999</td>\n",
       "      <td>2.856270</td>\n",
       "      <td>2.313866</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.973936</td>\n",
       "      <td>-0.256964</td>\n",
       "      <td>0.063980</td>\n",
       "      <td>-0.077695</td>\n",
       "      <td>-0.445644</td>\n",
       "      <td>1.174091</td>\n",
       "      <td>-0.774289</td>\n",
       "      <td>1.137862</td>\n",
       "      <td>1.770896</td>\n",
       "      <td>0.571725</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.957440</td>\n",
       "      <td>-0.985129</td>\n",
       "      <td>-0.201866</td>\n",
       "      <td>-0.053799</td>\n",
       "      <td>2.761286</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.144487</td>\n",
       "      <td>-0.318105</td>\n",
       "      <td>-0.133646</td>\n",
       "      <td>0.304825</td>\n",
       "      <td>-1.416811</td>\n",
       "      <td>0.907990</td>\n",
       "      <td>-0.275413</td>\n",
       "      <td>0.272377</td>\n",
       "      <td>1.869421</td>\n",
       "      <td>0.527438</td>\n",
       "      <td>...</td>\n",
       "      <td>1.265860</td>\n",
       "      <td>0.890102</td>\n",
       "      <td>0.261814</td>\n",
       "      <td>-1.096945</td>\n",
       "      <td>3.023777</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.916529</td>\n",
       "      <td>-0.556581</td>\n",
       "      <td>-0.483805</td>\n",
       "      <td>0.245689</td>\n",
       "      <td>-0.958924</td>\n",
       "      <td>0.615802</td>\n",
       "      <td>-0.631454</td>\n",
       "      <td>0.150644</td>\n",
       "      <td>1.678537</td>\n",
       "      <td>-0.183832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546908</td>\n",
       "      <td>1.278314</td>\n",
       "      <td>-0.149836</td>\n",
       "      <td>1.055085</td>\n",
       "      <td>0.666255</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>20110526_170002_2773-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.703802</td>\n",
       "      <td>-0.145396</td>\n",
       "      <td>-0.768933</td>\n",
       "      <td>-0.373392</td>\n",
       "      <td>1.012472</td>\n",
       "      <td>0.822703</td>\n",
       "      <td>0.366804</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>-0.400537</td>\n",
       "      <td>-0.783406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.797966</td>\n",
       "      <td>-1.614497</td>\n",
       "      <td>0.922434</td>\n",
       "      <td>-0.415321</td>\n",
       "      <td>-1.492054</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.995850</td>\n",
       "      <td>-0.235449</td>\n",
       "      <td>-0.757233</td>\n",
       "      <td>-0.757882</td>\n",
       "      <td>1.237943</td>\n",
       "      <td>0.997631</td>\n",
       "      <td>-0.410037</td>\n",
       "      <td>0.375156</td>\n",
       "      <td>-0.597088</td>\n",
       "      <td>-0.423280</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.054940</td>\n",
       "      <td>-0.400615</td>\n",
       "      <td>1.116520</td>\n",
       "      <td>-1.325486</td>\n",
       "      <td>0.230663</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.737229</td>\n",
       "      <td>-0.506690</td>\n",
       "      <td>-0.506285</td>\n",
       "      <td>-0.694673</td>\n",
       "      <td>1.226271</td>\n",
       "      <td>0.873060</td>\n",
       "      <td>-0.887662</td>\n",
       "      <td>-0.190512</td>\n",
       "      <td>-0.159313</td>\n",
       "      <td>-0.883016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648937</td>\n",
       "      <td>-1.301743</td>\n",
       "      <td>0.267589</td>\n",
       "      <td>-1.427142</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.650092</td>\n",
       "      <td>-1.009415</td>\n",
       "      <td>-0.651374</td>\n",
       "      <td>-0.412005</td>\n",
       "      <td>0.890934</td>\n",
       "      <td>0.898319</td>\n",
       "      <td>-1.087856</td>\n",
       "      <td>-0.213814</td>\n",
       "      <td>0.582044</td>\n",
       "      <td>-1.416173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743525</td>\n",
       "      <td>0.925039</td>\n",
       "      <td>0.096341</td>\n",
       "      <td>0.317783</td>\n",
       "      <td>-1.042110</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.965209</td>\n",
       "      <td>-1.026842</td>\n",
       "      <td>-0.350471</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>-0.238627</td>\n",
       "      <td>0.768176</td>\n",
       "      <td>-1.289087</td>\n",
       "      <td>-0.121159</td>\n",
       "      <td>0.539304</td>\n",
       "      <td>-0.828773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>-1.110531</td>\n",
       "      <td>1.327542</td>\n",
       "      <td>0.129050</td>\n",
       "      <td>-1.899957</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.729052</td>\n",
       "      <td>-1.036946</td>\n",
       "      <td>-0.450903</td>\n",
       "      <td>-0.024285</td>\n",
       "      <td>-0.032510</td>\n",
       "      <td>0.661935</td>\n",
       "      <td>-1.819420</td>\n",
       "      <td>-0.259435</td>\n",
       "      <td>0.505672</td>\n",
       "      <td>-2.055947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627368</td>\n",
       "      <td>-3.575395</td>\n",
       "      <td>2.449936</td>\n",
       "      <td>0.215291</td>\n",
       "      <td>-0.964219</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.430638</td>\n",
       "      <td>-1.224465</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>-0.181670</td>\n",
       "      <td>0.773702</td>\n",
       "      <td>1.169713</td>\n",
       "      <td>-1.058302</td>\n",
       "      <td>-0.194160</td>\n",
       "      <td>-0.259012</td>\n",
       "      <td>-1.926087</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.104187</td>\n",
       "      <td>0.141284</td>\n",
       "      <td>0.973415</td>\n",
       "      <td>-0.269860</td>\n",
       "      <td>-0.276838</td>\n",
       "      <td>fla_0412-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0412-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114056</th>\n",
       "      <td>-0.229178</td>\n",
       "      <td>-0.913385</td>\n",
       "      <td>-0.755061</td>\n",
       "      <td>-0.695674</td>\n",
       "      <td>1.387606</td>\n",
       "      <td>0.642274</td>\n",
       "      <td>2.044730</td>\n",
       "      <td>-0.202899</td>\n",
       "      <td>-2.309105</td>\n",
       "      <td>-0.397000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879567</td>\n",
       "      <td>1.591590</td>\n",
       "      <td>-1.044593</td>\n",
       "      <td>1.310788</td>\n",
       "      <td>-1.565817</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114057</th>\n",
       "      <td>-0.159421</td>\n",
       "      <td>-1.020609</td>\n",
       "      <td>-1.098956</td>\n",
       "      <td>-0.633035</td>\n",
       "      <td>1.480968</td>\n",
       "      <td>1.137694</td>\n",
       "      <td>1.311504</td>\n",
       "      <td>0.146375</td>\n",
       "      <td>-1.473602</td>\n",
       "      <td>0.647600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.914937</td>\n",
       "      <td>2.217254</td>\n",
       "      <td>1.480719</td>\n",
       "      <td>-0.166892</td>\n",
       "      <td>1.536081</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114058</th>\n",
       "      <td>-0.145513</td>\n",
       "      <td>-0.936435</td>\n",
       "      <td>-0.649152</td>\n",
       "      <td>-0.594719</td>\n",
       "      <td>0.060615</td>\n",
       "      <td>0.836248</td>\n",
       "      <td>1.439989</td>\n",
       "      <td>0.263596</td>\n",
       "      <td>-1.374492</td>\n",
       "      <td>0.842728</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.243502</td>\n",
       "      <td>-0.031903</td>\n",
       "      <td>0.084165</td>\n",
       "      <td>-0.505835</td>\n",
       "      <td>-1.012982</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114059</th>\n",
       "      <td>-0.357385</td>\n",
       "      <td>-1.204487</td>\n",
       "      <td>-0.410454</td>\n",
       "      <td>-0.887540</td>\n",
       "      <td>0.361747</td>\n",
       "      <td>1.082004</td>\n",
       "      <td>1.291034</td>\n",
       "      <td>0.687474</td>\n",
       "      <td>-1.446936</td>\n",
       "      <td>-0.123939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608805</td>\n",
       "      <td>1.065250</td>\n",
       "      <td>0.727062</td>\n",
       "      <td>-0.517500</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114060</th>\n",
       "      <td>-0.372510</td>\n",
       "      <td>-1.003788</td>\n",
       "      <td>-1.202037</td>\n",
       "      <td>-0.808508</td>\n",
       "      <td>1.698299</td>\n",
       "      <td>1.201384</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>-0.231405</td>\n",
       "      <td>-1.883240</td>\n",
       "      <td>-0.222860</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.589069</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>0.420181</td>\n",
       "      <td>-0.676640</td>\n",
       "      <td>-1.406064</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114061</th>\n",
       "      <td>-0.335305</td>\n",
       "      <td>-1.440970</td>\n",
       "      <td>-0.755028</td>\n",
       "      <td>-0.487089</td>\n",
       "      <td>0.612376</td>\n",
       "      <td>1.279199</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>0.393183</td>\n",
       "      <td>-1.191804</td>\n",
       "      <td>-0.165754</td>\n",
       "      <td>...</td>\n",
       "      <td>1.312090</td>\n",
       "      <td>-0.551421</td>\n",
       "      <td>1.184907</td>\n",
       "      <td>0.766099</td>\n",
       "      <td>-1.531552</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114062</th>\n",
       "      <td>0.062634</td>\n",
       "      <td>-0.923104</td>\n",
       "      <td>-0.908086</td>\n",
       "      <td>-0.843761</td>\n",
       "      <td>0.793762</td>\n",
       "      <td>0.920865</td>\n",
       "      <td>1.132383</td>\n",
       "      <td>-0.113445</td>\n",
       "      <td>-1.260730</td>\n",
       "      <td>0.528146</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.417656</td>\n",
       "      <td>1.377984</td>\n",
       "      <td>0.689489</td>\n",
       "      <td>-1.266281</td>\n",
       "      <td>-0.262855</td>\n",
       "      <td>fla_0016-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0016-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114063</th>\n",
       "      <td>-0.352595</td>\n",
       "      <td>-0.722480</td>\n",
       "      <td>-0.785234</td>\n",
       "      <td>-0.335814</td>\n",
       "      <td>-0.093107</td>\n",
       "      <td>0.649774</td>\n",
       "      <td>2.258405</td>\n",
       "      <td>-1.211995</td>\n",
       "      <td>-1.165709</td>\n",
       "      <td>-0.461915</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.184612</td>\n",
       "      <td>-0.350654</td>\n",
       "      <td>-1.306569</td>\n",
       "      <td>-0.758846</td>\n",
       "      <td>-2.350675</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114064</th>\n",
       "      <td>-0.246982</td>\n",
       "      <td>-0.681214</td>\n",
       "      <td>-1.088126</td>\n",
       "      <td>-0.957096</td>\n",
       "      <td>0.734624</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>2.152219</td>\n",
       "      <td>-2.277984</td>\n",
       "      <td>-2.082111</td>\n",
       "      <td>-0.129629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137722</td>\n",
       "      <td>-1.780250</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>-0.576582</td>\n",
       "      <td>-0.352981</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114065</th>\n",
       "      <td>-0.375504</td>\n",
       "      <td>-0.677951</td>\n",
       "      <td>-0.925499</td>\n",
       "      <td>-0.865084</td>\n",
       "      <td>0.104830</td>\n",
       "      <td>1.138165</td>\n",
       "      <td>2.081274</td>\n",
       "      <td>-1.538388</td>\n",
       "      <td>-1.018548</td>\n",
       "      <td>-0.031013</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.133016</td>\n",
       "      <td>-0.535820</td>\n",
       "      <td>0.531809</td>\n",
       "      <td>-1.900438</td>\n",
       "      <td>-1.103557</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040714_185618-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114101</th>\n",
       "      <td>-0.165550</td>\n",
       "      <td>-1.335665</td>\n",
       "      <td>-0.612321</td>\n",
       "      <td>-0.950682</td>\n",
       "      <td>0.857310</td>\n",
       "      <td>1.756027</td>\n",
       "      <td>1.306508</td>\n",
       "      <td>0.403337</td>\n",
       "      <td>1.613370</td>\n",
       "      <td>-1.736646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544383</td>\n",
       "      <td>1.700441</td>\n",
       "      <td>1.133453</td>\n",
       "      <td>-0.235122</td>\n",
       "      <td>-3.576728</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114102</th>\n",
       "      <td>0.111276</td>\n",
       "      <td>-1.401036</td>\n",
       "      <td>-0.540194</td>\n",
       "      <td>-0.071190</td>\n",
       "      <td>0.026178</td>\n",
       "      <td>1.287818</td>\n",
       "      <td>0.295184</td>\n",
       "      <td>0.412685</td>\n",
       "      <td>0.982379</td>\n",
       "      <td>-1.970097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343128</td>\n",
       "      <td>1.131743</td>\n",
       "      <td>0.119886</td>\n",
       "      <td>0.137870</td>\n",
       "      <td>-3.398634</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114103</th>\n",
       "      <td>0.316114</td>\n",
       "      <td>-1.561466</td>\n",
       "      <td>-0.092197</td>\n",
       "      <td>-0.065841</td>\n",
       "      <td>-0.372313</td>\n",
       "      <td>1.049140</td>\n",
       "      <td>0.753803</td>\n",
       "      <td>0.504618</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>-1.573862</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022234</td>\n",
       "      <td>0.619942</td>\n",
       "      <td>-0.516427</td>\n",
       "      <td>2.204022</td>\n",
       "      <td>-3.818742</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114104</th>\n",
       "      <td>-0.324709</td>\n",
       "      <td>-1.262340</td>\n",
       "      <td>-0.265428</td>\n",
       "      <td>-0.349591</td>\n",
       "      <td>0.410519</td>\n",
       "      <td>1.237145</td>\n",
       "      <td>1.155542</td>\n",
       "      <td>0.326916</td>\n",
       "      <td>0.411005</td>\n",
       "      <td>-2.171659</td>\n",
       "      <td>...</td>\n",
       "      <td>2.128831</td>\n",
       "      <td>1.049835</td>\n",
       "      <td>-1.546098</td>\n",
       "      <td>-0.392640</td>\n",
       "      <td>-1.700719</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114105</th>\n",
       "      <td>-0.078987</td>\n",
       "      <td>-1.252219</td>\n",
       "      <td>-0.523565</td>\n",
       "      <td>-0.417998</td>\n",
       "      <td>0.663165</td>\n",
       "      <td>1.365662</td>\n",
       "      <td>0.751715</td>\n",
       "      <td>0.631796</td>\n",
       "      <td>0.657306</td>\n",
       "      <td>-1.980506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>1.572448</td>\n",
       "      <td>-2.584998</td>\n",
       "      <td>0.349739</td>\n",
       "      <td>-1.340583</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>arb_lev-20040730_193237-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114107</th>\n",
       "      <td>1.020952</td>\n",
       "      <td>-1.715842</td>\n",
       "      <td>-1.427847</td>\n",
       "      <td>-0.394863</td>\n",
       "      <td>0.412751</td>\n",
       "      <td>1.281553</td>\n",
       "      <td>0.289828</td>\n",
       "      <td>-0.857346</td>\n",
       "      <td>0.230864</td>\n",
       "      <td>1.584924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967137</td>\n",
       "      <td>-2.712715</td>\n",
       "      <td>-0.244639</td>\n",
       "      <td>0.799438</td>\n",
       "      <td>0.556199</td>\n",
       "      <td>fla_0786-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0786-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114108</th>\n",
       "      <td>0.507351</td>\n",
       "      <td>-0.884475</td>\n",
       "      <td>-1.557367</td>\n",
       "      <td>-0.775401</td>\n",
       "      <td>1.452330</td>\n",
       "      <td>1.321919</td>\n",
       "      <td>1.215645</td>\n",
       "      <td>-1.062804</td>\n",
       "      <td>0.651167</td>\n",
       "      <td>1.275090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141686</td>\n",
       "      <td>-1.543074</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>1.319193</td>\n",
       "      <td>1.154548</td>\n",
       "      <td>fla_0786-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0786-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114109</th>\n",
       "      <td>-0.058735</td>\n",
       "      <td>-0.894696</td>\n",
       "      <td>-1.390107</td>\n",
       "      <td>-0.480259</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>1.296501</td>\n",
       "      <td>1.110812</td>\n",
       "      <td>-1.381067</td>\n",
       "      <td>1.805082</td>\n",
       "      <td>-0.077510</td>\n",
       "      <td>...</td>\n",
       "      <td>1.318819</td>\n",
       "      <td>-0.739370</td>\n",
       "      <td>0.560318</td>\n",
       "      <td>1.527020</td>\n",
       "      <td>1.036619</td>\n",
       "      <td>fla_0786-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0786-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114110</th>\n",
       "      <td>0.689914</td>\n",
       "      <td>-0.691733</td>\n",
       "      <td>-1.962290</td>\n",
       "      <td>-1.341922</td>\n",
       "      <td>1.944916</td>\n",
       "      <td>1.142389</td>\n",
       "      <td>1.416393</td>\n",
       "      <td>-0.444577</td>\n",
       "      <td>-0.377445</td>\n",
       "      <td>1.309382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457459</td>\n",
       "      <td>-0.844212</td>\n",
       "      <td>0.142029</td>\n",
       "      <td>1.659428</td>\n",
       "      <td>1.093494</td>\n",
       "      <td>fla_0786-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0786-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114111</th>\n",
       "      <td>1.228357</td>\n",
       "      <td>-1.324539</td>\n",
       "      <td>-1.271025</td>\n",
       "      <td>-0.032061</td>\n",
       "      <td>0.263037</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.244024</td>\n",
       "      <td>-0.504892</td>\n",
       "      <td>0.598826</td>\n",
       "      <td>1.263741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813546</td>\n",
       "      <td>-0.922323</td>\n",
       "      <td>0.898126</td>\n",
       "      <td>1.368238</td>\n",
       "      <td>0.335785</td>\n",
       "      <td>fla_0786-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0786-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114113</th>\n",
       "      <td>-0.206923</td>\n",
       "      <td>-1.441653</td>\n",
       "      <td>-1.038929</td>\n",
       "      <td>-1.146662</td>\n",
       "      <td>0.626272</td>\n",
       "      <td>0.697273</td>\n",
       "      <td>0.949951</td>\n",
       "      <td>0.955747</td>\n",
       "      <td>-1.017709</td>\n",
       "      <td>-0.313104</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.651202</td>\n",
       "      <td>0.879983</td>\n",
       "      <td>2.037314</td>\n",
       "      <td>-0.447420</td>\n",
       "      <td>-0.080577</td>\n",
       "      <td>fla_0868-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0868-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114114</th>\n",
       "      <td>-0.723097</td>\n",
       "      <td>-1.247955</td>\n",
       "      <td>-1.124376</td>\n",
       "      <td>-0.859342</td>\n",
       "      <td>0.115284</td>\n",
       "      <td>1.104113</td>\n",
       "      <td>-0.261734</td>\n",
       "      <td>0.871132</td>\n",
       "      <td>0.061863</td>\n",
       "      <td>-0.079344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718166</td>\n",
       "      <td>0.385470</td>\n",
       "      <td>0.336170</td>\n",
       "      <td>-0.328239</td>\n",
       "      <td>-0.425130</td>\n",
       "      <td>fla_0868-a</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0868-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114127</th>\n",
       "      <td>2.329828</td>\n",
       "      <td>-0.582908</td>\n",
       "      <td>-0.881897</td>\n",
       "      <td>-0.368329</td>\n",
       "      <td>0.557947</td>\n",
       "      <td>1.172652</td>\n",
       "      <td>-0.795926</td>\n",
       "      <td>0.368870</td>\n",
       "      <td>0.485527</td>\n",
       "      <td>1.271925</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.071885</td>\n",
       "      <td>1.406576</td>\n",
       "      <td>0.173551</td>\n",
       "      <td>0.355212</td>\n",
       "      <td>-0.332920</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114128</th>\n",
       "      <td>2.187123</td>\n",
       "      <td>-0.781658</td>\n",
       "      <td>-0.865484</td>\n",
       "      <td>-0.128599</td>\n",
       "      <td>0.319215</td>\n",
       "      <td>1.126453</td>\n",
       "      <td>-0.386829</td>\n",
       "      <td>-0.286814</td>\n",
       "      <td>0.566726</td>\n",
       "      <td>0.780479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.931791</td>\n",
       "      <td>-1.128283</td>\n",
       "      <td>-0.095879</td>\n",
       "      <td>-0.977967</td>\n",
       "      <td>-2.761666</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114129</th>\n",
       "      <td>2.064498</td>\n",
       "      <td>-0.758362</td>\n",
       "      <td>-0.719901</td>\n",
       "      <td>0.329034</td>\n",
       "      <td>0.114865</td>\n",
       "      <td>1.144897</td>\n",
       "      <td>-0.792268</td>\n",
       "      <td>-0.298201</td>\n",
       "      <td>1.161183</td>\n",
       "      <td>0.263045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470617</td>\n",
       "      <td>-0.233268</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.645763</td>\n",
       "      <td>-0.209217</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114130</th>\n",
       "      <td>2.117191</td>\n",
       "      <td>-0.475151</td>\n",
       "      <td>-0.820335</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>0.090696</td>\n",
       "      <td>1.366634</td>\n",
       "      <td>-0.353053</td>\n",
       "      <td>0.300699</td>\n",
       "      <td>1.197361</td>\n",
       "      <td>0.407148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.613271</td>\n",
       "      <td>1.007550</td>\n",
       "      <td>0.793516</td>\n",
       "      <td>-1.416191</td>\n",
       "      <td>-0.371291</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114131</th>\n",
       "      <td>2.052065</td>\n",
       "      <td>-0.782115</td>\n",
       "      <td>-0.802080</td>\n",
       "      <td>0.206398</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>1.054518</td>\n",
       "      <td>-0.318913</td>\n",
       "      <td>-0.134992</td>\n",
       "      <td>0.868932</td>\n",
       "      <td>0.115893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544051</td>\n",
       "      <td>-0.390625</td>\n",
       "      <td>-0.186260</td>\n",
       "      <td>-1.539336</td>\n",
       "      <td>-1.829452</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114132</th>\n",
       "      <td>2.079292</td>\n",
       "      <td>-0.468101</td>\n",
       "      <td>-0.910173</td>\n",
       "      <td>0.218450</td>\n",
       "      <td>0.595208</td>\n",
       "      <td>1.517526</td>\n",
       "      <td>-0.102799</td>\n",
       "      <td>-0.272696</td>\n",
       "      <td>1.317379</td>\n",
       "      <td>0.364536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120173</td>\n",
       "      <td>0.435237</td>\n",
       "      <td>-0.106165</td>\n",
       "      <td>0.778941</td>\n",
       "      <td>-0.657810</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114133</th>\n",
       "      <td>2.267603</td>\n",
       "      <td>-0.530195</td>\n",
       "      <td>-0.754170</td>\n",
       "      <td>0.419924</td>\n",
       "      <td>0.551442</td>\n",
       "      <td>1.265312</td>\n",
       "      <td>-0.234200</td>\n",
       "      <td>0.229783</td>\n",
       "      <td>1.422197</td>\n",
       "      <td>0.832166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511093</td>\n",
       "      <td>-1.728686</td>\n",
       "      <td>0.023745</td>\n",
       "      <td>1.149382</td>\n",
       "      <td>-2.161079</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114134</th>\n",
       "      <td>2.133139</td>\n",
       "      <td>-0.292450</td>\n",
       "      <td>-0.775466</td>\n",
       "      <td>0.399480</td>\n",
       "      <td>0.556767</td>\n",
       "      <td>1.460351</td>\n",
       "      <td>-0.662003</td>\n",
       "      <td>0.478674</td>\n",
       "      <td>1.991766</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534662</td>\n",
       "      <td>-0.011441</td>\n",
       "      <td>0.781475</td>\n",
       "      <td>-0.919665</td>\n",
       "      <td>1.012042</td>\n",
       "      <td>fla_0575-b</td>\n",
       "      <td>LDC2017E22</td>\n",
       "      <td>data</td>\n",
       "      <td>ara-apc</td>\n",
       "      <td>fla_0575-b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22229 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "2       2.061305 -0.140605 -0.627104 -0.682908  1.337613  1.261543 -0.651291   \n",
       "3       1.958049 -0.608315 -0.113532  0.167272  0.911100  1.116094 -0.847214   \n",
       "4       1.842436 -0.417598 -0.427428 -0.485898  1.204330  1.439188 -1.056642   \n",
       "5       1.953034 -0.596223 -0.535057 -0.371161  0.592551  1.366468 -1.361346   \n",
       "6       1.990754 -0.430986 -0.371372  0.035754  1.007754  1.237592 -0.698143   \n",
       "7      -0.466107 -0.884151 -0.357131 -0.577324 -0.220201  1.070911  0.060307   \n",
       "8      -0.516523 -0.627713 -0.105311 -0.322296 -0.553714  1.011796  0.757867   \n",
       "9      -0.725326 -0.702411 -0.345675 -0.701975  0.130706  1.177092  0.764530   \n",
       "10     -0.641780 -0.386634 -0.121360 -0.916782  0.394523  1.070178  1.007852   \n",
       "11     -0.578299 -0.598327 -0.278434 -0.740745 -0.022859  1.239188  0.778646   \n",
       "12     -0.672127 -0.861736 -0.339636 -0.154396 -0.494528  1.110264  0.027725   \n",
       "13     -0.743000 -0.311738 -0.258337 -0.386368 -0.423488  0.935386  0.303447   \n",
       "14     -0.640753 -0.459186 -0.401053 -0.540534  0.293700  1.096528  0.976757   \n",
       "15     -0.613265 -0.541661 -0.056440  0.017932 -0.516878  0.855285  0.820343   \n",
       "16     -0.597133 -0.405047 -0.210105 -0.312792 -0.244037  0.881740  0.549423   \n",
       "17     -0.669301 -0.457039 -0.070314 -0.546059 -0.462010  1.072645  0.876348   \n",
       "18     -0.787922 -0.172470 -0.123671 -0.219415 -0.139755  1.013841  0.897521   \n",
       "20      2.124002 -0.415169 -0.087091 -0.039534 -1.062281  1.317047 -1.045120   \n",
       "21      1.992214 -0.628076 -0.051747  0.235573 -1.335204  1.179586 -0.787103   \n",
       "22      1.741754 -0.881252 -0.384082  0.414502 -1.561445  1.122099 -1.473811   \n",
       "23      1.973936 -0.256964  0.063980 -0.077695 -0.445644  1.174091 -0.774289   \n",
       "24      2.144487 -0.318105 -0.133646  0.304825 -1.416811  0.907990 -0.275413   \n",
       "25      1.916529 -0.556581 -0.483805  0.245689 -0.958924  0.615802 -0.631454   \n",
       "26      1.703802 -0.145396 -0.768933 -0.373392  1.012472  0.822703  0.366804   \n",
       "27      1.995850 -0.235449 -0.757233 -0.757882  1.237943  0.997631 -0.410037   \n",
       "28      1.737229 -0.506690 -0.506285 -0.694673  1.226271  0.873060 -0.887662   \n",
       "29      1.650092 -1.009415 -0.651374 -0.412005  0.890934  0.898319 -1.087856   \n",
       "30      1.965209 -1.026842 -0.350471  0.047727 -0.238627  0.768176 -1.289087   \n",
       "31      1.729052 -1.036946 -0.450903 -0.024285 -0.032510  0.661935 -1.819420   \n",
       "32      1.430638 -1.224465  0.006190 -0.181670  0.773702  1.169713 -1.058302   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "114056 -0.229178 -0.913385 -0.755061 -0.695674  1.387606  0.642274  2.044730   \n",
       "114057 -0.159421 -1.020609 -1.098956 -0.633035  1.480968  1.137694  1.311504   \n",
       "114058 -0.145513 -0.936435 -0.649152 -0.594719  0.060615  0.836248  1.439989   \n",
       "114059 -0.357385 -1.204487 -0.410454 -0.887540  0.361747  1.082004  1.291034   \n",
       "114060 -0.372510 -1.003788 -1.202037 -0.808508  1.698299  1.201384  0.763676   \n",
       "114061 -0.335305 -1.440970 -0.755028 -0.487089  0.612376  1.279199  0.048583   \n",
       "114062  0.062634 -0.923104 -0.908086 -0.843761  0.793762  0.920865  1.132383   \n",
       "114063 -0.352595 -0.722480 -0.785234 -0.335814 -0.093107  0.649774  2.258405   \n",
       "114064 -0.246982 -0.681214 -1.088126 -0.957096  0.734624  0.993973  2.152219   \n",
       "114065 -0.375504 -0.677951 -0.925499 -0.865084  0.104830  1.138165  2.081274   \n",
       "114101 -0.165550 -1.335665 -0.612321 -0.950682  0.857310  1.756027  1.306508   \n",
       "114102  0.111276 -1.401036 -0.540194 -0.071190  0.026178  1.287818  0.295184   \n",
       "114103  0.316114 -1.561466 -0.092197 -0.065841 -0.372313  1.049140  0.753803   \n",
       "114104 -0.324709 -1.262340 -0.265428 -0.349591  0.410519  1.237145  1.155542   \n",
       "114105 -0.078987 -1.252219 -0.523565 -0.417998  0.663165  1.365662  0.751715   \n",
       "114107  1.020952 -1.715842 -1.427847 -0.394863  0.412751  1.281553  0.289828   \n",
       "114108  0.507351 -0.884475 -1.557367 -0.775401  1.452330  1.321919  1.215645   \n",
       "114109 -0.058735 -0.894696 -1.390107 -0.480259  0.606163  1.296501  1.110812   \n",
       "114110  0.689914 -0.691733 -1.962290 -1.341922  1.944916  1.142389  1.416393   \n",
       "114111  1.228357 -1.324539 -1.271025 -0.032061  0.263037  0.937792  0.244024   \n",
       "114113 -0.206923 -1.441653 -1.038929 -1.146662  0.626272  0.697273  0.949951   \n",
       "114114 -0.723097 -1.247955 -1.124376 -0.859342  0.115284  1.104113 -0.261734   \n",
       "114127  2.329828 -0.582908 -0.881897 -0.368329  0.557947  1.172652 -0.795926   \n",
       "114128  2.187123 -0.781658 -0.865484 -0.128599  0.319215  1.126453 -0.386829   \n",
       "114129  2.064498 -0.758362 -0.719901  0.329034  0.114865  1.144897 -0.792268   \n",
       "114130  2.117191 -0.475151 -0.820335  0.169723  0.090696  1.366634 -0.353053   \n",
       "114131  2.052065 -0.782115 -0.802080  0.206398  0.577997  1.054518 -0.318913   \n",
       "114132  2.079292 -0.468101 -0.910173  0.218450  0.595208  1.517526 -0.102799   \n",
       "114133  2.267603 -0.530195 -0.754170  0.419924  0.551442  1.265312 -0.234200   \n",
       "114134  2.133139 -0.292450 -0.775466  0.399480  0.556767  1.460351 -0.662003   \n",
       "\n",
       "               7         8         9            ...                   495  \\\n",
       "2       0.307040 -0.980717  0.335530            ...             -3.018432   \n",
       "3      -0.041357 -0.542335  0.199105            ...             -0.548767   \n",
       "4       0.111603 -0.404313 -0.307075            ...              0.297026   \n",
       "5       0.299445 -0.305046 -0.537489            ...              0.723645   \n",
       "6      -0.091484 -0.547547 -0.248731            ...             -1.230570   \n",
       "7       0.799765 -0.783718 -0.765216            ...             -1.309657   \n",
       "8       1.013907 -1.236593 -0.911828            ...              0.579321   \n",
       "9       1.189533 -1.282171 -1.082967            ...             -1.098577   \n",
       "10      1.380711 -1.707224 -1.127637            ...              0.284225   \n",
       "11      1.184929 -1.456520 -0.931703            ...             -0.582347   \n",
       "12      0.924851 -1.020621 -1.233343            ...              1.052173   \n",
       "13      1.620867 -0.658274 -1.129338            ...             -1.745930   \n",
       "14      1.216016 -1.263392 -0.517696            ...             -1.816436   \n",
       "15      0.775795 -1.310810 -0.871257            ...             -0.611984   \n",
       "16      0.744476 -1.494494 -1.283757            ...              0.230264   \n",
       "17      1.564480 -1.407819 -1.305188            ...              0.901232   \n",
       "18      1.747409 -1.457581 -1.142132            ...             -0.124980   \n",
       "20      0.859918  1.494985 -0.072534            ...              1.724883   \n",
       "21      0.320739  1.335336 -0.923129            ...              0.474895   \n",
       "22      0.434503  2.450365 -0.348727            ...              0.807964   \n",
       "23      1.137862  1.770896  0.571725            ...             -1.957440   \n",
       "24      0.272377  1.869421  0.527438            ...              1.265860   \n",
       "25      0.150644  1.678537 -0.183832            ...             -0.546908   \n",
       "26      0.104449 -0.400537 -0.783406            ...             -0.797966   \n",
       "27      0.375156 -0.597088 -0.423280            ...             -1.054940   \n",
       "28     -0.190512 -0.159313 -0.883016            ...              0.648937   \n",
       "29     -0.213814  0.582044 -1.416173            ...              0.743525   \n",
       "30     -0.121159  0.539304 -0.828773            ...              0.361229   \n",
       "31     -0.259435  0.505672 -2.055947            ...             -0.627368   \n",
       "32     -0.194160 -0.259012 -1.926087            ...             -2.104187   \n",
       "...          ...       ...       ...            ...                   ...   \n",
       "114056 -0.202899 -2.309105 -0.397000            ...              0.879567   \n",
       "114057  0.146375 -1.473602  0.647600            ...             -0.914937   \n",
       "114058  0.263596 -1.374492  0.842728            ...             -1.243502   \n",
       "114059  0.687474 -1.446936 -0.123939            ...              0.608805   \n",
       "114060 -0.231405 -1.883240 -0.222860            ...             -1.589069   \n",
       "114061  0.393183 -1.191804 -0.165754            ...              1.312090   \n",
       "114062 -0.113445 -1.260730  0.528146            ...             -2.417656   \n",
       "114063 -1.211995 -1.165709 -0.461915            ...             -1.184612   \n",
       "114064 -2.277984 -2.082111 -0.129629            ...             -0.137722   \n",
       "114065 -1.538388 -1.018548 -0.031013            ...             -1.133016   \n",
       "114101  0.403337  1.613370 -1.736646            ...              0.544383   \n",
       "114102  0.412685  0.982379 -1.970097            ...              1.343128   \n",
       "114103  0.504618  0.475146 -1.573862            ...              1.022234   \n",
       "114104  0.326916  0.411005 -2.171659            ...              2.128831   \n",
       "114105  0.631796  0.657306 -1.980506            ...              0.209246   \n",
       "114107 -0.857346  0.230864  1.584924            ...              0.967137   \n",
       "114108 -1.062804  0.651167  1.275090            ...             -0.141686   \n",
       "114109 -1.381067  1.805082 -0.077510            ...              1.318819   \n",
       "114110 -0.444577 -0.377445  1.309382            ...             -0.457459   \n",
       "114111 -0.504892  0.598826  1.263741            ...              0.813546   \n",
       "114113  0.955747 -1.017709 -0.313104            ...             -2.651202   \n",
       "114114  0.871132  0.061863 -0.079344            ...             -0.718166   \n",
       "114127  0.368870  0.485527  1.271925            ...             -3.071885   \n",
       "114128 -0.286814  0.566726  0.780479            ...              1.931791   \n",
       "114129 -0.298201  1.161183  0.263045            ...              0.470617   \n",
       "114130  0.300699  1.197361  0.407148            ...              1.613271   \n",
       "114131 -0.134992  0.868932  0.115893            ...             -0.544051   \n",
       "114132 -0.272696  1.317379  0.364536            ...             -0.120173   \n",
       "114133  0.229783  1.422197  0.832166            ...              0.511093   \n",
       "114134  0.478674  1.991766  0.227856            ...              0.534662   \n",
       "\n",
       "             496       497       498       499                        ids  \\\n",
       "2      -1.841361  0.814595 -0.200731  1.180795                 fla_0240-a   \n",
       "3      -1.254447  1.644647  0.555434  1.159140                 fla_0240-a   \n",
       "4      -1.858708  0.209784  0.583939  1.398789                 fla_0240-a   \n",
       "5      -2.709617  1.595344 -0.267579  0.810446                 fla_0240-a   \n",
       "6      -3.184290  2.312549 -1.312105  1.701691                 fla_0240-a   \n",
       "7       0.627864 -1.560878 -1.698744 -0.523284                 fla_0073-b   \n",
       "8      -0.415504 -0.113781 -0.765633 -0.917091                 fla_0073-b   \n",
       "9       0.045006 -0.065501 -0.826183  0.936744                 fla_0073-b   \n",
       "10     -0.634877  1.705949  0.416899 -1.166602                 fla_0073-b   \n",
       "11     -0.709782 -1.074811  0.894534  1.752265                 fla_0073-b   \n",
       "12     -0.343894 -0.670818  1.035438 -0.093136                 fla_0073-b   \n",
       "13     -1.223212  0.818071 -0.038440  2.021269                 fla_0073-b   \n",
       "14      1.020891  2.095646 -0.707287  0.815685                 fla_0073-b   \n",
       "15      1.610372 -0.755875 -0.155681 -0.354927                 fla_0073-b   \n",
       "16      0.149276  0.371657 -0.421422 -0.564310                 fla_0073-b   \n",
       "17     -0.325534 -1.193512 -0.697982  0.672191                 fla_0073-b   \n",
       "18      1.850188  1.517514  0.647885 -0.320918                 fla_0073-b   \n",
       "20      1.997481  1.341118  1.944841  0.744884     20110526_170002_2773-b   \n",
       "21     -1.573427  0.032893  0.912990 -0.282434     20110526_170002_2773-b   \n",
       "22      1.480003  0.440999  2.856270  2.313866     20110526_170002_2773-b   \n",
       "23     -0.985129 -0.201866 -0.053799  2.761286     20110526_170002_2773-b   \n",
       "24      0.890102  0.261814 -1.096945  3.023777     20110526_170002_2773-b   \n",
       "25      1.278314 -0.149836  1.055085  0.666255     20110526_170002_2773-b   \n",
       "26     -1.614497  0.922434 -0.415321 -1.492054                 fla_0412-a   \n",
       "27     -0.400615  1.116520 -1.325486  0.230663                 fla_0412-a   \n",
       "28     -1.301743  0.267589 -1.427142  0.060583                 fla_0412-a   \n",
       "29      0.925039  0.096341  0.317783 -1.042110                 fla_0412-a   \n",
       "30     -1.110531  1.327542  0.129050 -1.899957                 fla_0412-a   \n",
       "31     -3.575395  2.449936  0.215291 -0.964219                 fla_0412-a   \n",
       "32      0.141284  0.973415 -0.269860 -0.276838                 fla_0412-a   \n",
       "...          ...       ...       ...       ...                        ...   \n",
       "114056  1.591590 -1.044593  1.310788 -1.565817                 fla_0016-a   \n",
       "114057  2.217254  1.480719 -0.166892  1.536081                 fla_0016-a   \n",
       "114058 -0.031903  0.084165 -0.505835 -1.012982                 fla_0016-a   \n",
       "114059  1.065250  0.727062 -0.517500  0.577944                 fla_0016-a   \n",
       "114060 -0.030032  0.420181 -0.676640 -1.406064                 fla_0016-a   \n",
       "114061 -0.551421  1.184907  0.766099 -1.531552                 fla_0016-a   \n",
       "114062  1.377984  0.689489 -1.266281 -0.262855                 fla_0016-a   \n",
       "114063 -0.350654 -1.306569 -0.758846 -2.350675  arb_lev-20040714_185618-a   \n",
       "114064 -1.780250  0.499615 -0.576582 -0.352981  arb_lev-20040714_185618-a   \n",
       "114065 -0.535820  0.531809 -1.900438 -1.103557  arb_lev-20040714_185618-a   \n",
       "114101  1.700441  1.133453 -0.235122 -3.576728  arb_lev-20040730_193237-b   \n",
       "114102  1.131743  0.119886  0.137870 -3.398634  arb_lev-20040730_193237-b   \n",
       "114103  0.619942 -0.516427  2.204022 -3.818742  arb_lev-20040730_193237-b   \n",
       "114104  1.049835 -1.546098 -0.392640 -1.700719  arb_lev-20040730_193237-b   \n",
       "114105  1.572448 -2.584998  0.349739 -1.340583  arb_lev-20040730_193237-b   \n",
       "114107 -2.712715 -0.244639  0.799438  0.556199                 fla_0786-b   \n",
       "114108 -1.543074  0.014137  1.319193  1.154548                 fla_0786-b   \n",
       "114109 -0.739370  0.560318  1.527020  1.036619                 fla_0786-b   \n",
       "114110 -0.844212  0.142029  1.659428  1.093494                 fla_0786-b   \n",
       "114111 -0.922323  0.898126  1.368238  0.335785                 fla_0786-b   \n",
       "114113  0.879983  2.037314 -0.447420 -0.080577                 fla_0868-a   \n",
       "114114  0.385470  0.336170 -0.328239 -0.425130                 fla_0868-a   \n",
       "114127  1.406576  0.173551  0.355212 -0.332920                 fla_0575-b   \n",
       "114128 -1.128283 -0.095879 -0.977967 -2.761666                 fla_0575-b   \n",
       "114129 -0.233268  0.012689  0.645763 -0.209217                 fla_0575-b   \n",
       "114130  1.007550  0.793516 -1.416191 -0.371291                 fla_0575-b   \n",
       "114131 -0.390625 -0.186260 -1.539336 -1.829452                 fla_0575-b   \n",
       "114132  0.435237 -0.106165  0.778941 -0.657810                 fla_0575-b   \n",
       "114133 -1.728686  0.023745  1.149382 -2.161079                 fla_0575-b   \n",
       "114134 -0.011441  0.781475 -0.919665  1.012042                 fla_0575-b   \n",
       "\n",
       "              year  data     lang                    lang_id  \n",
       "2       LDC2017E22  data  ara-apc                 fla_0240-a  \n",
       "3       LDC2017E22  data  ara-apc                 fla_0240-a  \n",
       "4       LDC2017E22  data  ara-apc                 fla_0240-a  \n",
       "5       LDC2017E22  data  ara-apc                 fla_0240-a  \n",
       "6       LDC2017E22  data  ara-apc                 fla_0240-a  \n",
       "7       LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "8       LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "9       LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "10      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "11      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "12      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "13      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "14      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "15      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "16      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "17      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "18      LDC2017E22  data  ara-apc                 fla_0073-b  \n",
       "20      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "21      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "22      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "23      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "24      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "25      LDC2017E22  data  ara-apc     20110526_170002_2773-b  \n",
       "26      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "27      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "28      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "29      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "30      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "31      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "32      LDC2017E22  data  ara-apc                 fla_0412-a  \n",
       "...            ...   ...      ...                        ...  \n",
       "114056  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114057  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114058  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114059  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114060  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114061  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114062  LDC2017E22  data  ara-apc                 fla_0016-a  \n",
       "114063  LDC2017E22  data  ara-apc  arb_lev-20040714_185618-a  \n",
       "114064  LDC2017E22  data  ara-apc  arb_lev-20040714_185618-a  \n",
       "114065  LDC2017E22  data  ara-apc  arb_lev-20040714_185618-a  \n",
       "114101  LDC2017E22  data  ara-apc  arb_lev-20040730_193237-b  \n",
       "114102  LDC2017E22  data  ara-apc  arb_lev-20040730_193237-b  \n",
       "114103  LDC2017E22  data  ara-apc  arb_lev-20040730_193237-b  \n",
       "114104  LDC2017E22  data  ara-apc  arb_lev-20040730_193237-b  \n",
       "114105  LDC2017E22  data  ara-apc  arb_lev-20040730_193237-b  \n",
       "114107  LDC2017E22  data  ara-apc                 fla_0786-b  \n",
       "114108  LDC2017E22  data  ara-apc                 fla_0786-b  \n",
       "114109  LDC2017E22  data  ara-apc                 fla_0786-b  \n",
       "114110  LDC2017E22  data  ara-apc                 fla_0786-b  \n",
       "114111  LDC2017E22  data  ara-apc                 fla_0786-b  \n",
       "114113  LDC2017E22  data  ara-apc                 fla_0868-a  \n",
       "114114  LDC2017E22  data  ara-apc                 fla_0868-a  \n",
       "114127  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114128  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114129  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114130  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114131  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114132  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114133  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "114134  LDC2017E22  data  ara-apc                 fla_0575-b  \n",
       "\n",
       "[22229 rows x 505 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lre = pd.read_csv('/home/satishk/lre2.0/ivectors_csv_revised/dev_feat_BNF_h5_07Nov_Shreyas.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_lre = val_lre.iloc[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentid</th>\n",
       "      <th>language_code</th>\n",
       "      <th>data_source</th>\n",
       "      <th>speech_duration</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>uttid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lre17_ntrlosgu.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>30</td>\n",
       "      <td>1.697234</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>-0.400756</td>\n",
       "      <td>0.513963</td>\n",
       "      <td>-0.939232</td>\n",
       "      <td>1.500797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.314428</td>\n",
       "      <td>-0.927694</td>\n",
       "      <td>-0.370424</td>\n",
       "      <td>-0.514735</td>\n",
       "      <td>1.290885</td>\n",
       "      <td>0.688205</td>\n",
       "      <td>-0.494330</td>\n",
       "      <td>-0.053206</td>\n",
       "      <td>-1.330860</td>\n",
       "      <td>lre17_ntrlosgu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lre17_moxnwuqe.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>10</td>\n",
       "      <td>1.648232</td>\n",
       "      <td>-0.053318</td>\n",
       "      <td>-0.562867</td>\n",
       "      <td>1.035870</td>\n",
       "      <td>-1.577741</td>\n",
       "      <td>1.593584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929262</td>\n",
       "      <td>-1.301574</td>\n",
       "      <td>2.034934</td>\n",
       "      <td>-0.226545</td>\n",
       "      <td>-0.198926</td>\n",
       "      <td>-0.116174</td>\n",
       "      <td>0.347923</td>\n",
       "      <td>-0.870801</td>\n",
       "      <td>-2.599601</td>\n",
       "      <td>lre17_moxnwuqe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lre17_meesvkxz.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>3</td>\n",
       "      <td>1.242829</td>\n",
       "      <td>0.675515</td>\n",
       "      <td>-0.371491</td>\n",
       "      <td>0.534970</td>\n",
       "      <td>-0.246783</td>\n",
       "      <td>0.806262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>1.058771</td>\n",
       "      <td>1.018635</td>\n",
       "      <td>-1.929319</td>\n",
       "      <td>-0.307404</td>\n",
       "      <td>-0.486431</td>\n",
       "      <td>-2.839053</td>\n",
       "      <td>-2.704527</td>\n",
       "      <td>lre17_meesvkxz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lre17_rqmsmzui.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>30</td>\n",
       "      <td>1.226681</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>-0.396915</td>\n",
       "      <td>-0.097507</td>\n",
       "      <td>-0.013574</td>\n",
       "      <td>1.087025</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049862</td>\n",
       "      <td>0.285627</td>\n",
       "      <td>2.385587</td>\n",
       "      <td>0.680073</td>\n",
       "      <td>1.500978</td>\n",
       "      <td>1.660566</td>\n",
       "      <td>-0.370672</td>\n",
       "      <td>-0.924109</td>\n",
       "      <td>0.096676</td>\n",
       "      <td>lre17_rqmsmzui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lre17_qgszpuyw.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>10</td>\n",
       "      <td>1.411728</td>\n",
       "      <td>-0.119300</td>\n",
       "      <td>0.136256</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>-1.029447</td>\n",
       "      <td>1.227100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>-1.030222</td>\n",
       "      <td>2.933880</td>\n",
       "      <td>-1.417872</td>\n",
       "      <td>-0.227513</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>-0.671044</td>\n",
       "      <td>0.595977</td>\n",
       "      <td>1.722917</td>\n",
       "      <td>lre17_qgszpuyw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            segmentid language_code data_source  speech_duration         0  \\\n",
       "0  lre17_ntrlosgu.sph       ara-acm       mls14               30  1.697234   \n",
       "1  lre17_moxnwuqe.sph       ara-acm       mls14               10  1.648232   \n",
       "2  lre17_meesvkxz.sph       ara-acm       mls14                3  1.242829   \n",
       "3  lre17_rqmsmzui.sph       ara-acm       mls14               30  1.226681   \n",
       "4  lre17_qgszpuyw.sph       ara-acm       mls14               10  1.411728   \n",
       "\n",
       "          1         2         3         4         5       ...             491  \\\n",
       "0  0.029428 -0.400756  0.513963 -0.939232  1.500797       ...       -1.314428   \n",
       "1 -0.053318 -0.562867  1.035870 -1.577741  1.593584       ...       -0.929262   \n",
       "2  0.675515 -0.371491  0.534970 -0.246783  0.806262       ...        0.691336   \n",
       "3  0.014810 -0.396915 -0.097507 -0.013574  1.087025       ...        1.049862   \n",
       "4 -0.119300  0.136256  0.030535 -1.029447  1.227100       ...        0.155196   \n",
       "\n",
       "        492       493       494       495       496       497       498  \\\n",
       "0 -0.927694 -0.370424 -0.514735  1.290885  0.688205 -0.494330 -0.053206   \n",
       "1 -1.301574  2.034934 -0.226545 -0.198926 -0.116174  0.347923 -0.870801   \n",
       "2  0.257988  1.058771  1.018635 -1.929319 -0.307404 -0.486431 -2.839053   \n",
       "3  0.285627  2.385587  0.680073  1.500978  1.660566 -0.370672 -0.924109   \n",
       "4 -1.030222  2.933880 -1.417872 -0.227513  0.748810 -0.671044  0.595977   \n",
       "\n",
       "        499           uttid  \n",
       "0 -1.330860  lre17_ntrlosgu  \n",
       "1 -2.599601  lre17_moxnwuqe  \n",
       "2 -2.704527  lre17_meesvkxz  \n",
       "3  0.096676  lre17_rqmsmzui  \n",
       "4  1.722917  lre17_qgszpuyw  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_lre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_lre.drop([\"lang\",'ids','year','data','lang_id'],axis=1)\n",
    "y_train = train_lre[\"lang\"]\n",
    "#y_train_uttid = train_lre[\"uttid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_lre.drop([\"language_code\",\"uttid\",\"segmentid\",\"data_source\",\"speech_duration\"],axis=1)\n",
    "y_val = val_lre[\"language_code\"]\n",
    "y_val_segmentid = val_lre[\"segmentid\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = X_train.append(X_val, ignore_index=True)\n",
    "y_train = y_train.append(y_val, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ara-apc'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ara-acm', 'ara-apc', 'ara-ary', 'ara-arz', 'eng-gbr', 'eng-usg',\n",
       "       'por-brz', 'qsl-pol', 'qsl-rus', 'spa-car', 'spa-eur', 'spa-lac',\n",
       "       'zho-cmn', 'zho-nan'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "X_val=X_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 14"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val_labels, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = indices_to_one_hot(y_train, 14)\n",
    "#Y_test = indices_to_one_hot(y_test, 14)\n",
    "Y_val = indices_to_one_hot(y_val_labels, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3661, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22229, 14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22229, 500)\n",
      "22229 train samples\n",
      "3661 val samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "#X_val /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "print(X_val.shape[0], 'val samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22229, 500)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22229,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,  y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3661, 500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 13, 13, 13])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Shuffling the dataset\n",
    "#from sklearn.utils import shuffle\n",
    "#X_train,  y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val,  y_val_labels = shuffle(X_val, y_val_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = input_data.read_data_sets('/home/satishk/depy_04_AUG/MNIST_data', one_hot=True)\n",
    "mb_size = 256\n",
    "Z_dim = 100\n",
    "X_dim = 500 #mnist.train.images.shape[1]\n",
    "y_dim = 14 #mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dim, y_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Generator as Encoder-Decoder pair inspired from DAGAN base paper\n",
    "\n",
    "#Encoder should be able to take a batch of input(of dim 500-ivector) and be able to produce its representation r\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(500, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "    \n",
    "#We will use Decoder as Generator    \n",
    "#Decoder should be able to take input of dim 128(r) and 100(z),concatenate r and z and \n",
    "#produce an output od dim 500-ivector    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(228,256 )\n",
    "        self.fc2 = nn.Linear(256, 500)\n",
    "        #self.fc3 = nn.Linear(512,500)\n",
    "    def forward(self, x):\n",
    "        #inputs = torch.cat([z, x], 1)\n",
    "        return F.sigmoid(self.fc2(F.relu(self.fc1(x))))\n",
    "\n",
    "#input to Generator alias Decoder is inputs = torch.cat([z, r], 1)\n",
    "#Where r is the output of encoder given x i.e., representation of x encoded by encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we defined the Encoder and Decoder we now Implement the Generator \n",
    "\n",
    "class Generator(nn.Module):\n",
    "                                \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = Encoder()\n",
    "        self.fc2 = Decoder()\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        #inputs = torch.cat([z, x], 1)\n",
    "        return self.fc2(torch.cat([z, self.fc1(x)],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #self.inputs = torch.cat([z, c], 1)\n",
    "        self.fc1 = torch.nn.Linear(500, 128)\n",
    "        self.fc2 = torch.nn.Linear(128,1)\n",
    "        #self.fc3 = torch.nn.Linear(512,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #inputs = torch.cat([X, c], 1)\n",
    "        return F.sigmoid(self.fc2(F.relu(self.fc1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Generator(\n",
       "  (fc1): Encoder(\n",
       "    (fc1): Linear(in_features=500, out_features=256)\n",
       "    (fc2): Linear(in_features=256, out_features=128)\n",
       "  )\n",
       "  (fc2): Decoder(\n",
       "    (fc1): Linear(in_features=228, out_features=256)\n",
       "    (fc2): Linear(in_features=256, out_features=500)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.modules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G = G.cuda()\n",
    "D = D.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))\n",
    "ones_label_fake = Variable(torch.ones(mb_size*2))\n",
    "#ones_label = ones_label.cuda()\n",
    "#zeros_label = zeros_label.cuda()\n",
    "#ones_label_fake = ones_label_fake.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "#criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "G_solver = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_solver = torch.optim.Adam(D.parameters(), lr=learning_rate/2, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22229"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishk/miniconda3/envs/lre17/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/satishk/miniconda3/envs/lre17/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: [ 1.39441705]; G_loss: [ 0.70605069]\n",
      "Iter-256; D_loss: [ 1.33229136]; G_loss: [ 0.71103138]\n",
      "Iter-512; D_loss: [ 1.30183434]; G_loss: [ 0.72898847]\n",
      "Iter-768; D_loss: [ 1.28419924]; G_loss: [ 0.75008571]\n",
      "Iter-1024; D_loss: [ 1.21312332]; G_loss: [ 0.74295187]\n",
      "Iter-1280; D_loss: [ 1.23297703]; G_loss: [ 0.78058445]\n",
      "Iter-1536; D_loss: [ 1.18391418]; G_loss: [ 0.78383082]\n",
      "Iter-1792; D_loss: [ 1.15817332]; G_loss: [ 0.79684937]\n",
      "Iter-2048; D_loss: [ 1.15773106]; G_loss: [ 0.82139826]\n",
      "Iter-2304; D_loss: [ 1.09903848]; G_loss: [ 0.8152433]\n",
      "Iter-2560; D_loss: [ 1.11218691]; G_loss: [ 0.84535325]\n",
      "Iter-2816; D_loss: [ 1.09432256]; G_loss: [ 0.8587352]\n",
      "Iter-3072; D_loss: [ 1.06049848]; G_loss: [ 0.86463016]\n",
      "Iter-3328; D_loss: [ 1.04447532]; G_loss: [ 0.87792832]\n",
      "Iter-3584; D_loss: [ 1.05843234]; G_loss: [ 0.9064737]\n",
      "Iter-3840; D_loss: [ 1.04101443]; G_loss: [ 0.91912866]\n",
      "Iter-4096; D_loss: [ 0.99981117]; G_loss: [ 0.92047578]\n",
      "Iter-4352; D_loss: [ 0.99126458]; G_loss: [ 0.93622297]\n",
      "Iter-4608; D_loss: [ 0.97399557]; G_loss: [ 0.9480201]\n",
      "Iter-4864; D_loss: [ 0.95703518]; G_loss: [ 0.95635867]\n",
      "Iter-5120; D_loss: [ 0.93508244]; G_loss: [ 0.96179998]\n",
      "Iter-5376; D_loss: [ 0.93106365]; G_loss: [ 0.97150254]\n",
      "Iter-5632; D_loss: [ 0.91449499]; G_loss: [ 0.97697216]\n",
      "Iter-5888; D_loss: [ 0.91440964]; G_loss: [ 0.9857502]\n",
      "Iter-6144; D_loss: [ 0.91368234]; G_loss: [ 0.99220681]\n",
      "Iter-6400; D_loss: [ 0.90143228]; G_loss: [ 0.996539]\n",
      "Iter-6656; D_loss: [ 0.8927424]; G_loss: [ 0.99784738]\n",
      "Iter-6912; D_loss: [ 0.88368016]; G_loss: [ 1.00010169]\n",
      "Iter-7168; D_loss: [ 0.87508553]; G_loss: [ 1.00350308]\n",
      "Iter-7424; D_loss: [ 0.88320827]; G_loss: [ 1.01444066]\n",
      "Iter-7680; D_loss: [ 0.87706459]; G_loss: [ 1.01899588]\n",
      "Iter-7936; D_loss: [ 0.89363766]; G_loss: [ 1.03476524]\n",
      "Iter-8192; D_loss: [ 0.86003578]; G_loss: [ 1.024822]\n",
      "Iter-8448; D_loss: [ 0.86187136]; G_loss: [ 1.03363049]\n",
      "Iter-8704; D_loss: [ 0.84649545]; G_loss: [ 1.03364575]\n",
      "Iter-8960; D_loss: [ 0.85026097]; G_loss: [ 1.04256582]\n",
      "Iter-9216; D_loss: [ 0.82595265]; G_loss: [ 1.03761625]\n",
      "Iter-9472; D_loss: [ 0.8171283]; G_loss: [ 1.04112756]\n",
      "Iter-9728; D_loss: [ 0.82806885]; G_loss: [ 1.05172336]\n",
      "Iter-9984; D_loss: [ 0.82662725]; G_loss: [ 1.05797708]\n",
      "Iter-10240; D_loss: [ 0.81784296]; G_loss: [ 1.057374]\n",
      "Iter-10496; D_loss: [ 0.80390304]; G_loss: [ 1.05654275]\n",
      "Iter-10752; D_loss: [ 0.80921829]; G_loss: [ 1.05997789]\n",
      "Iter-11008; D_loss: [ 0.7950846]; G_loss: [ 1.05067134]\n",
      "Iter-11264; D_loss: [ 0.81475836]; G_loss: [ 1.06129324]\n",
      "Iter-11520; D_loss: [ 0.82309794]; G_loss: [ 1.06114757]\n",
      "Iter-11776; D_loss: [ 0.81941497]; G_loss: [ 1.04961014]\n",
      "Iter-12032; D_loss: [ 0.85267389]; G_loss: [ 1.05512416]\n",
      "Iter-12288; D_loss: [ 0.8159411]; G_loss: [ 1.01415873]\n",
      "Iter-12544; D_loss: [ 0.83118701]; G_loss: [ 1.00557983]\n",
      "Iter-12800; D_loss: [ 0.82834959]; G_loss: [ 0.98539126]\n",
      "Iter-13056; D_loss: [ 0.86256373]; G_loss: [ 0.97779387]\n",
      "Iter-13312; D_loss: [ 0.85561472]; G_loss: [ 0.9438144]\n",
      "Iter-13568; D_loss: [ 0.8629697]; G_loss: [ 0.90015137]\n",
      "Iter-13824; D_loss: [ 0.89284241]; G_loss: [ 0.88640332]\n",
      "Iter-14080; D_loss: [ 0.90892136]; G_loss: [ 0.8616544]\n",
      "Iter-14336; D_loss: [ 0.92772084]; G_loss: [ 0.84389704]\n",
      "Iter-14592; D_loss: [ 0.94402361]; G_loss: [ 0.8284899]\n",
      "Iter-14848; D_loss: [ 0.94141984]; G_loss: [ 0.79610014]\n",
      "Iter-15104; D_loss: [ 0.97379684]; G_loss: [ 0.79436719]\n",
      "Iter-15360; D_loss: [ 0.98136538]; G_loss: [ 0.7771697]\n",
      "Iter-15616; D_loss: [ 1.00623679]; G_loss: [ 0.78759551]\n",
      "Iter-15872; D_loss: [ 0.9826023]; G_loss: [ 0.75208837]\n",
      "Iter-16128; D_loss: [ 1.00475943]; G_loss: [ 0.75957769]\n",
      "Iter-16384; D_loss: [ 1.00586677]; G_loss: [ 0.74830222]\n",
      "Iter-16640; D_loss: [ 1.00348842]; G_loss: [ 0.75259376]\n",
      "Iter-16896; D_loss: [ 0.98675025]; G_loss: [ 0.74941611]\n",
      "Iter-17152; D_loss: [ 0.99333251]; G_loss: [ 0.75551754]\n",
      "Iter-17408; D_loss: [ 0.98569334]; G_loss: [ 0.76439327]\n",
      "Iter-17664; D_loss: [ 0.95936793]; G_loss: [ 0.75857866]\n",
      "Iter-17920; D_loss: [ 0.98515183]; G_loss: [ 0.78518033]\n",
      "Iter-18176; D_loss: [ 0.95782447]; G_loss: [ 0.78553474]\n",
      "Iter-18432; D_loss: [ 0.92041171]; G_loss: [ 0.78611821]\n",
      "Iter-18688; D_loss: [ 0.92131412]; G_loss: [ 0.80294538]\n",
      "Iter-18944; D_loss: [ 0.89754307]; G_loss: [ 0.80898923]\n",
      "Iter-19200; D_loss: [ 0.90808457]; G_loss: [ 0.83405232]\n",
      "Iter-19456; D_loss: [ 0.85617483]; G_loss: [ 0.82816935]\n",
      "Iter-19712; D_loss: [ 0.85299027]; G_loss: [ 0.84476078]\n",
      "Iter-19968; D_loss: [ 0.8430624]; G_loss: [ 0.85588443]\n",
      "Iter-20224; D_loss: [ 0.83531052]; G_loss: [ 0.86529064]\n",
      "Iter-20480; D_loss: [ 0.83004022]; G_loss: [ 0.87360358]\n",
      "Iter-20736; D_loss: [ 0.83142561]; G_loss: [ 0.88183314]\n",
      "Iter-20992; D_loss: [ 0.8207736]; G_loss: [ 0.88169134]\n",
      "Iter-21248; D_loss: [ 0.80144787]; G_loss: [ 0.87596822]\n",
      "Iter-21504; D_loss: [ 0.82333076]; G_loss: [ 0.88924843]\n",
      "Iter-21760; D_loss: [ 0.80929077]; G_loss: [ 0.88285714]\n",
      "epoch: 1\n",
      "Iter-0; D_loss: [ 0.81449604]; G_loss: [ 0.88609719]\n",
      "Iter-256; D_loss: [ 0.79062182]; G_loss: [ 0.87424165]\n",
      "Iter-512; D_loss: [ 0.80491412]; G_loss: [ 0.88042963]\n",
      "Iter-768; D_loss: [ 0.80171144]; G_loss: [ 0.8776412]\n",
      "Iter-1024; D_loss: [ 0.78086776]; G_loss: [ 0.86530626]\n",
      "Iter-1280; D_loss: [ 0.80140984]; G_loss: [ 0.87345117]\n",
      "Iter-1536; D_loss: [ 0.78499424]; G_loss: [ 0.8627072]\n",
      "Iter-1792; D_loss: [ 0.79267764]; G_loss: [ 0.86281168]\n",
      "Iter-2048; D_loss: [ 0.82699424]; G_loss: [ 0.87485743]\n",
      "Iter-2304; D_loss: [ 0.77481717]; G_loss: [ 0.84625924]\n",
      "Iter-2560; D_loss: [ 0.79817569]; G_loss: [ 0.85484344]\n",
      "Iter-2816; D_loss: [ 0.79997587]; G_loss: [ 0.85560876]\n",
      "Iter-3072; D_loss: [ 0.80245149]; G_loss: [ 0.85644418]\n",
      "Iter-3328; D_loss: [ 0.7965312]; G_loss: [ 0.85306263]\n",
      "Iter-3584; D_loss: [ 0.79795069]; G_loss: [ 0.85402334]\n",
      "Iter-3840; D_loss: [ 0.79364377]; G_loss: [ 0.85181564]\n",
      "Iter-4096; D_loss: [ 0.79225838]; G_loss: [ 0.85273415]\n",
      "Iter-4352; D_loss: [ 0.78271788]; G_loss: [ 0.84738028]\n",
      "Iter-4608; D_loss: [ 0.7632829]; G_loss: [ 0.83756799]\n",
      "Iter-4864; D_loss: [ 0.77172124]; G_loss: [ 0.84392887]\n",
      "Iter-5120; D_loss: [ 0.75355709]; G_loss: [ 0.83744788]\n",
      "Iter-5376; D_loss: [ 0.75407958]; G_loss: [ 0.84124005]\n",
      "Iter-5632; D_loss: [ 0.74693441]; G_loss: [ 0.84117132]\n",
      "Iter-5888; D_loss: [ 0.73855972]; G_loss: [ 0.84142017]\n",
      "Iter-6144; D_loss: [ 0.74065226]; G_loss: [ 0.84718877]\n",
      "Iter-6400; D_loss: [ 0.75521028]; G_loss: [ 0.8597542]\n",
      "Iter-6656; D_loss: [ 0.73609984]; G_loss: [ 0.8552447]\n",
      "Iter-6912; D_loss: [ 0.71253908]; G_loss: [ 0.84806299]\n",
      "Iter-7168; D_loss: [ 0.72223079]; G_loss: [ 0.85801554]\n",
      "Iter-7424; D_loss: [ 0.72299755]; G_loss: [ 0.86487699]\n",
      "Iter-7680; D_loss: [ 0.72824585]; G_loss: [ 0.8737151]\n",
      "Iter-7936; D_loss: [ 0.73744738]; G_loss: [ 0.8853423]\n",
      "Iter-8192; D_loss: [ 0.71123171]; G_loss: [ 0.87719226]\n",
      "Iter-8448; D_loss: [ 0.71725845]; G_loss: [ 0.88323098]\n",
      "Iter-8704; D_loss: [ 0.7043376]; G_loss: [ 0.87830067]\n",
      "Iter-8960; D_loss: [ 0.72001851]; G_loss: [ 0.88788998]\n",
      "Iter-9216; D_loss: [ 0.68743181]; G_loss: [ 0.87456691]\n",
      "Iter-9472; D_loss: [ 0.66288412]; G_loss: [ 0.86615133]\n",
      "Iter-9728; D_loss: [ 0.66274405]; G_loss: [ 0.87195504]\n",
      "Iter-9984; D_loss: [ 0.67714167]; G_loss: [ 0.88722694]\n",
      "Iter-10240; D_loss: [ 0.66902703]; G_loss: [ 0.89068264]\n",
      "Iter-10496; D_loss: [ 0.6659351]; G_loss: [ 0.89654237]\n",
      "Iter-10752; D_loss: [ 0.65544713]; G_loss: [ 0.89865625]\n",
      "Iter-11008; D_loss: [ 0.64070714]; G_loss: [ 0.8984614]\n",
      "Iter-11264; D_loss: [ 0.65252209]; G_loss: [ 0.91041034]\n",
      "Iter-11520; D_loss: [ 0.68407977]; G_loss: [ 0.9302969]\n",
      "Iter-11776; D_loss: [ 0.66286653]; G_loss: [ 0.92297727]\n",
      "Iter-12032; D_loss: [ 0.6791364]; G_loss: [ 0.9364984]\n",
      "Iter-12288; D_loss: [ 0.63380849]; G_loss: [ 0.92122197]\n",
      "Iter-12544; D_loss: [ 0.64712369]; G_loss: [ 0.93476814]\n",
      "Iter-12800; D_loss: [ 0.62184846]; G_loss: [ 0.92902333]\n",
      "Iter-13056; D_loss: [ 0.63699919]; G_loss: [ 0.943802]\n",
      "Iter-13312; D_loss: [ 0.62855518]; G_loss: [ 0.94651693]\n",
      "Iter-13568; D_loss: [ 0.60413051]; G_loss: [ 0.94120753]\n",
      "Iter-13824; D_loss: [ 0.60666275]; G_loss: [ 0.94901383]\n",
      "Iter-14080; D_loss: [ 0.59470725]; G_loss: [ 0.95018053]\n",
      "Iter-14336; D_loss: [ 0.596614]; G_loss: [ 0.95771098]\n",
      "Iter-14592; D_loss: [ 0.606619]; G_loss: [ 0.96950859]\n",
      "Iter-14848; D_loss: [ 0.57901132]; G_loss: [ 0.96245569]\n",
      "Iter-15104; D_loss: [ 0.58481634]; G_loss: [ 0.97174531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-15360; D_loss: [ 0.57255465]; G_loss: [ 0.97216451]\n",
      "Iter-15616; D_loss: [ 0.60807288]; G_loss: [ 0.99630016]\n",
      "Iter-15872; D_loss: [ 0.55788362]; G_loss: [ 0.97834599]\n",
      "Iter-16128; D_loss: [ 0.59136403]; G_loss: [ 1.00114596]\n",
      "Iter-16384; D_loss: [ 0.57650805]; G_loss: [ 1.00021744]\n",
      "Iter-16640; D_loss: [ 0.55404449]; G_loss: [ 0.99527764]\n",
      "Iter-16896; D_loss: [ 0.55322886]; G_loss: [ 1.00134122]\n",
      "Iter-17152; D_loss: [ 0.5547716]; G_loss: [ 1.00856042]\n",
      "Iter-17408; D_loss: [ 0.54886848]; G_loss: [ 1.01181173]\n",
      "Iter-17664; D_loss: [ 0.53245211]; G_loss: [ 1.01027083]\n",
      "Iter-17920; D_loss: [ 0.56657588]; G_loss: [ 1.03314185]\n",
      "Iter-18176; D_loss: [ 0.55048335]; G_loss: [ 1.03137875]\n",
      "Iter-18432; D_loss: [ 0.53099513]; G_loss: [ 1.02802074]\n",
      "Iter-18688; D_loss: [ 0.5475347]; G_loss: [ 1.04169559]\n",
      "Iter-18944; D_loss: [ 0.51789707]; G_loss: [ 1.03349006]\n",
      "Iter-19200; D_loss: [ 0.53758478]; G_loss: [ 1.04894757]\n",
      "Iter-19456; D_loss: [ 0.50785607]; G_loss: [ 1.04043686]\n",
      "Iter-19712; D_loss: [ 0.51469183]; G_loss: [ 1.04992437]\n",
      "Iter-19968; D_loss: [ 0.51920545]; G_loss: [ 1.0580591]\n",
      "Iter-20224; D_loss: [ 0.50941145]; G_loss: [ 1.05924547]\n",
      "Iter-20480; D_loss: [ 0.50693464]; G_loss: [ 1.06379831]\n",
      "Iter-20736; D_loss: [ 0.51429784]; G_loss: [ 1.07323122]\n",
      "Iter-20992; D_loss: [ 0.50390953]; G_loss: [ 1.073717]\n",
      "Iter-21248; D_loss: [ 0.48199385]; G_loss: [ 1.06878424]\n",
      "Iter-21504; D_loss: [ 0.50472939]; G_loss: [ 1.08579016]\n",
      "Iter-21760; D_loss: [ 0.4957273]; G_loss: [ 1.08687794]\n",
      "epoch: 2\n",
      "Iter-0; D_loss: [ 0.49764416]; G_loss: [ 1.09238482]\n",
      "Iter-256; D_loss: [ 0.48245561]; G_loss: [ 1.08079672]\n",
      "Iter-512; D_loss: [ 0.48348254]; G_loss: [ 1.08377957]\n",
      "Iter-768; D_loss: [ 0.4770115]; G_loss: [ 1.08719063]\n",
      "Iter-1024; D_loss: [ 0.46425372]; G_loss: [ 1.08659768]\n",
      "Iter-1280; D_loss: [ 0.47604084]; G_loss: [ 1.09705508]\n",
      "Iter-1536; D_loss: [ 0.46479857]; G_loss: [ 1.08608103]\n",
      "Iter-1792; D_loss: [ 0.46715528]; G_loss: [ 1.09120524]\n",
      "Iter-2048; D_loss: [ 0.49894062]; G_loss: [ 1.11234665]\n",
      "Iter-2304; D_loss: [ 0.45131314]; G_loss: [ 1.09449065]\n",
      "Iter-2560; D_loss: [ 0.46791625]; G_loss: [ 1.10852587]\n",
      "Iter-2816; D_loss: [ 0.46341515]; G_loss: [ 1.11194384]\n",
      "Iter-3072; D_loss: [ 0.47333574]; G_loss: [ 1.12203479]\n",
      "Iter-3328; D_loss: [ 0.4573068]; G_loss: [ 1.11945474]\n",
      "Iter-3584; D_loss: [ 0.45088279]; G_loss: [ 1.12149906]\n",
      "Iter-3840; D_loss: [ 0.44969299]; G_loss: [ 1.12640083]\n",
      "Iter-4096; D_loss: [ 0.46023747]; G_loss: [ 1.13712609]\n",
      "Iter-4352; D_loss: [ 0.44055682]; G_loss: [ 1.1324501]\n",
      "Iter-4608; D_loss: [ 0.42143446]; G_loss: [ 1.12868547]\n",
      "Iter-4864; D_loss: [ 0.43439692]; G_loss: [ 1.14059269]\n",
      "Iter-5120; D_loss: [ 0.41826677]; G_loss: [ 1.13801479]\n",
      "Iter-5376; D_loss: [ 0.41743672]; G_loss: [ 1.14316487]\n",
      "Iter-5632; D_loss: [ 0.41167381]; G_loss: [ 1.14552438]\n",
      "Iter-5888; D_loss: [ 0.39925224]; G_loss: [ 1.14507282]\n",
      "Iter-6144; D_loss: [ 0.40935093]; G_loss: [ 1.15515935]\n",
      "Iter-6400; D_loss: [ 0.42820257]; G_loss: [ 1.1694752]\n",
      "Iter-6656; D_loss: [ 0.40779096]; G_loss: [ 1.16471279]\n",
      "Iter-6912; D_loss: [ 0.38918623]; G_loss: [ 1.160882]\n",
      "Iter-7168; D_loss: [ 0.40336305]; G_loss: [ 1.17288053]\n",
      "Iter-7424; D_loss: [ 0.39687029]; G_loss: [ 1.17506444]\n",
      "Iter-7680; D_loss: [ 0.40844643]; G_loss: [ 1.18552113]\n",
      "Iter-7936; D_loss: [ 0.41993168]; G_loss: [ 1.19641125]\n",
      "Iter-8192; D_loss: [ 0.39769498]; G_loss: [ 1.19015229]\n",
      "Iter-8448; D_loss: [ 0.40637404]; G_loss: [ 1.19927752]\n",
      "Iter-8704; D_loss: [ 0.39545968]; G_loss: [ 1.19866526]\n",
      "Iter-8960; D_loss: [ 0.40723115]; G_loss: [ 1.20900798]\n",
      "Iter-9216; D_loss: [ 0.37970936]; G_loss: [ 1.20015001]\n",
      "Iter-9472; D_loss: [ 0.35319591]; G_loss: [ 1.19251215]\n",
      "Iter-9728; D_loss: [ 0.35472646]; G_loss: [ 1.19811594]\n",
      "Iter-9984; D_loss: [ 0.37621927]; G_loss: [ 1.21431446]\n",
      "Iter-10240; D_loss: [ 0.36843243]; G_loss: [ 1.21556175]\n",
      "Iter-10496; D_loss: [ 0.37054688]; G_loss: [ 1.22142029]\n",
      "Iter-10752; D_loss: [ 0.36153606]; G_loss: [ 1.22197223]\n",
      "Iter-11008; D_loss: [ 0.35124725]; G_loss: [ 1.22163069]\n",
      "Iter-11264; D_loss: [ 0.36242515]; G_loss: [ 1.23182011]\n",
      "Iter-11520; D_loss: [ 0.39112219]; G_loss: [ 1.25066125]\n",
      "Iter-11776; D_loss: [ 0.37317657]; G_loss: [ 1.24661732]\n",
      "Iter-12032; D_loss: [ 0.38574943]; G_loss: [ 1.25707769]\n",
      "Iter-12288; D_loss: [ 0.35208774]; G_loss: [ 1.24538434]\n",
      "Iter-12544; D_loss: [ 0.36797383]; G_loss: [ 1.25793624]\n",
      "Iter-12800; D_loss: [ 0.34552145]; G_loss: [ 1.25144315]\n",
      "Iter-13056; D_loss: [ 0.35723385]; G_loss: [ 1.26221359]\n",
      "Iter-13312; D_loss: [ 0.36354077]; G_loss: [ 1.26997447]\n",
      "Iter-13568; D_loss: [ 0.33835796]; G_loss: [ 1.26221263]\n",
      "Iter-13824; D_loss: [ 0.34115505]; G_loss: [ 1.26807678]\n",
      "Iter-14080; D_loss: [ 0.32670683]; G_loss: [ 1.26589811]\n",
      "Iter-14336; D_loss: [ 0.33758649]; G_loss: [ 1.27584922]\n",
      "Iter-14592; D_loss: [ 0.34562579]; G_loss: [ 1.28437912]\n",
      "Iter-14848; D_loss: [ 0.32815543]; G_loss: [ 1.2803756]\n",
      "Iter-15104; D_loss: [ 0.32703474]; G_loss: [ 1.28433359]\n",
      "Iter-15360; D_loss: [ 0.31893846]; G_loss: [ 1.28485847]\n",
      "Iter-15616; D_loss: [ 0.35337016]; G_loss: [ 1.30628395]\n",
      "Iter-15872; D_loss: [ 0.31513995]; G_loss: [ 1.29239428]\n",
      "Iter-16128; D_loss: [ 0.34574026]; G_loss: [ 1.31169403]\n",
      "Iter-16384; D_loss: [ 0.33334017]; G_loss: [ 1.31000435]\n",
      "Iter-16640; D_loss: [ 0.30936578]; G_loss: [ 1.30260825]\n",
      "Iter-16896; D_loss: [ 0.31357738]; G_loss: [ 1.30922151]\n",
      "Iter-17152; D_loss: [ 0.3147139]; G_loss: [ 1.31437349]\n",
      "Iter-17408; D_loss: [ 0.31287426]; G_loss: [ 1.31789315]\n",
      "Iter-17664; D_loss: [ 0.29843223]; G_loss: [ 1.31551039]\n",
      "Iter-17920; D_loss: [ 0.3280116]; G_loss: [ 1.3342725]\n",
      "Iter-18176; D_loss: [ 0.31919745]; G_loss: [ 1.33422482]\n",
      "Iter-18432; D_loss: [ 0.30556273]; G_loss: [ 1.33192325]\n",
      "Iter-18688; D_loss: [ 0.32572624]; G_loss: [ 1.34551966]\n",
      "Iter-18944; D_loss: [ 0.29483777]; G_loss: [ 1.33490694]\n",
      "Iter-19200; D_loss: [ 0.31194115]; G_loss: [ 1.34724689]\n",
      "Iter-19456; D_loss: [ 0.292622]; G_loss: [ 1.3412987]\n",
      "Iter-19712; D_loss: [ 0.30765349]; G_loss: [ 1.32007158]\n",
      "Iter-19968; D_loss: [ 0.31378967]; G_loss: [ 1.3283155]\n",
      "Iter-20224; D_loss: [ 0.30385101]; G_loss: [ 1.32862711]\n",
      "Iter-20480; D_loss: [ 0.30060819]; G_loss: [ 1.33187222]\n",
      "Iter-20736; D_loss: [ 0.3087976]; G_loss: [ 1.34034884]\n",
      "Iter-20992; D_loss: [ 0.30129749]; G_loss: [ 1.34079909]\n",
      "Iter-21248; D_loss: [ 0.28333628]; G_loss: [ 1.33634043]\n",
      "Iter-21504; D_loss: [ 0.30149245]; G_loss: [ 1.34952343]\n",
      "Iter-21760; D_loss: [ 0.29827026]; G_loss: [ 1.35217726]\n",
      "epoch: 3\n",
      "Iter-0; D_loss: [ 0.3012284]; G_loss: [ 1.35777402]\n",
      "Iter-256; D_loss: [ 0.29172671]; G_loss: [ 1.35751534]\n",
      "Iter-512; D_loss: [ 0.28429347]; G_loss: [ 1.35825253]\n",
      "Iter-768; D_loss: [ 0.28017458]; G_loss: [ 1.36067724]\n",
      "Iter-1024; D_loss: [ 0.27239287]; G_loss: [ 1.36082196]\n",
      "Iter-1280; D_loss: [ 0.28435463]; G_loss: [ 1.37119126]\n",
      "Iter-1536; D_loss: [ 0.27105781]; G_loss: [ 1.36888158]\n",
      "Iter-1792; D_loss: [ 0.27376348]; G_loss: [ 1.37441754]\n",
      "Iter-2048; D_loss: [ 0.2982775]; G_loss: [ 1.39016008]\n",
      "Iter-2304; D_loss: [ 0.26637605]; G_loss: [ 1.37849653]\n",
      "Iter-2560; D_loss: [ 0.28101]; G_loss: [ 1.38984323]\n",
      "Iter-2816; D_loss: [ 0.27639118]; G_loss: [ 1.39177144]\n",
      "Iter-3072; D_loss: [ 0.28503597]; G_loss: [ 1.39971507]\n",
      "Iter-3328; D_loss: [ 0.26982066]; G_loss: [ 1.39623272]\n",
      "Iter-3584; D_loss: [ 0.26675043]; G_loss: [ 1.39871442]\n",
      "Iter-3840; D_loss: [ 0.26774439]; G_loss: [ 1.40335298]\n",
      "Iter-4096; D_loss: [ 0.27921766]; G_loss: [ 1.41300941]\n",
      "Iter-4352; D_loss: [ 0.26039934]; G_loss: [ 1.40745997]\n",
      "Iter-4608; D_loss: [ 0.24949092]; G_loss: [ 1.40640056]\n",
      "Iter-4864; D_loss: [ 0.26189187]; G_loss: [ 1.41666365]\n",
      "Iter-5120; D_loss: [ 0.2496196]; G_loss: [ 1.41465402]\n",
      "Iter-5376; D_loss: [ 0.2476515]; G_loss: [ 1.4178468]\n",
      "Iter-5632; D_loss: [ 0.24317378]; G_loss: [ 1.41958463]\n",
      "Iter-5888; D_loss: [ 0.23477799]; G_loss: [ 1.41980863]\n",
      "Iter-6144; D_loss: [ 0.24508968]; G_loss: [ 1.42876291]\n",
      "Iter-6400; D_loss: [ 0.2599349]; G_loss: [ 1.43970466]\n",
      "Iter-6656; D_loss: [ 0.24336189]; G_loss: [ 1.43555045]\n",
      "Iter-6912; D_loss: [ 0.23212963]; G_loss: [ 1.43406391]\n",
      "Iter-7168; D_loss: [ 0.24409734]; G_loss: [ 1.44375014]\n",
      "Iter-7424; D_loss: [ 0.2352152]; G_loss: [ 1.44347334]\n",
      "Iter-7680; D_loss: [ 0.24653265]; G_loss: [ 1.45257998]\n",
      "Iter-7936; D_loss: [ 0.25647256]; G_loss: [ 1.46128213]\n",
      "Iter-8192; D_loss: [ 0.24013624]; G_loss: [ 1.45677447]\n",
      "Iter-8448; D_loss: [ 0.25144055]; G_loss: [ 1.46597266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8704; D_loss: [ 0.24179605]; G_loss: [ 1.46466231]\n",
      "Iter-8960; D_loss: [ 0.24977393]; G_loss: [ 1.47192049]\n",
      "Iter-9216; D_loss: [ 0.23023611]; G_loss: [ 1.46577239]\n",
      "Iter-9472; D_loss: [ 0.20916834]; G_loss: [ 1.4596324]\n",
      "Iter-9728; D_loss: [ 0.21027496]; G_loss: [ 1.46401823]\n",
      "Iter-9984; D_loss: [ 0.23017064]; G_loss: [ 1.47800791]\n",
      "Iter-10240; D_loss: [ 0.22307223]; G_loss: [ 1.47844315]\n",
      "Iter-10496; D_loss: [ 0.2250703]; G_loss: [ 1.48309243]\n",
      "Iter-10752; D_loss: [ 0.22001687]; G_loss: [ 1.4844209]\n",
      "Iter-11008; D_loss: [ 0.21207681]; G_loss: [ 1.48409748]\n",
      "Iter-11264; D_loss: [ 0.22197349]; G_loss: [ 1.49258995]\n",
      "Iter-11520; D_loss: [ 0.24205106]; G_loss: [ 1.50592101]\n",
      "Iter-11776; D_loss: [ 0.23030077]; G_loss: [ 1.50371993]\n",
      "Iter-12032; D_loss: [ 0.23834227]; G_loss: [ 1.51081169]\n",
      "Iter-12288; D_loss: [ 0.2144959]; G_loss: [ 1.5028075]\n",
      "Iter-12544; D_loss: [ 0.23052785]; G_loss: [ 1.51426208]\n",
      "Iter-12800; D_loss: [ 0.21032807]; G_loss: [ 1.50785792]\n",
      "Iter-13056; D_loss: [ 0.21850537]; G_loss: [ 1.51569176]\n",
      "Iter-13312; D_loss: [ 0.23047532]; G_loss: [ 1.52510607]\n",
      "Iter-13568; D_loss: [ 0.20710266]; G_loss: [ 1.51715767]\n",
      "Iter-13824; D_loss: [ 0.20975384]; G_loss: [ 1.5219152]\n",
      "Iter-14080; D_loss: [ 0.19628435]; G_loss: [ 1.51898384]\n",
      "Iter-14336; D_loss: [ 0.21411726]; G_loss: [ 1.4885447]\n",
      "Iter-14592; D_loss: [ 0.21836101]; G_loss: [ 1.49795926]\n",
      "Iter-14848; D_loss: [ 0.20797881]; G_loss: [ 1.49995875]\n",
      "Iter-15104; D_loss: [ 0.20301159]; G_loss: [ 1.50419414]\n",
      "Iter-15360; D_loss: [ 0.19748075]; G_loss: [ 1.50692761]\n",
      "Iter-15616; D_loss: [ 0.22455038]; G_loss: [ 1.52488947]\n",
      "Iter-15872; D_loss: [ 0.19843882]; G_loss: [ 1.51651132]\n",
      "Iter-16128; D_loss: [ 0.22201352]; G_loss: [ 1.53156936]\n",
      "Iter-16384; D_loss: [ 0.21215267]; G_loss: [ 1.5302577]\n",
      "Iter-16640; D_loss: [ 0.19172874]; G_loss: [ 1.52378297]\n",
      "Iter-16896; D_loss: [ 0.19604334]; G_loss: [ 1.52964246]\n",
      "Iter-17152; D_loss: [ 0.19645531]; G_loss: [ 1.53358018]\n",
      "Iter-17408; D_loss: [ 0.19678344]; G_loss: [ 1.53698003]\n",
      "Iter-17664; D_loss: [ 0.1912418]; G_loss: [ 1.48527992]\n",
      "Iter-17920; D_loss: [ 0.21351658]; G_loss: [ 1.50429368]\n",
      "Iter-18176; D_loss: [ 0.20840088]; G_loss: [ 1.51104403]\n",
      "Iter-18432; D_loss: [ 0.19808617]; G_loss: [ 1.51452923]\n",
      "Iter-18688; D_loss: [ 0.21721227]; G_loss: [ 1.53088343]\n",
      "Iter-18944; D_loss: [ 0.18874145]; G_loss: [ 1.5228672]\n",
      "Iter-19200; D_loss: [ 0.2018709]; G_loss: [ 1.53430355]\n",
      "Iter-19456; D_loss: [ 0.188685]; G_loss: [ 1.53247869]\n",
      "Iter-19712; D_loss: [ 0.19804296]; G_loss: [ 1.54149497]\n",
      "Iter-19968; D_loss: [ 0.20308013]; G_loss: [ 1.54797137]\n",
      "Iter-20224; D_loss: [ 0.19538838]; G_loss: [ 1.54825389]\n",
      "Iter-20480; D_loss: [ 0.19212759]; G_loss: [ 1.55050027]\n",
      "Iter-20736; D_loss: [ 0.19914193]; G_loss: [ 1.55754924]\n",
      "Iter-20992; D_loss: [ 0.19300498]; G_loss: [ 1.55805337]\n",
      "Iter-21248; D_loss: [ 0.1800213]; G_loss: [ 1.55536795]\n",
      "Iter-21504; D_loss: [ 0.19307433]; G_loss: [ 1.56541812]\n",
      "Iter-21760; D_loss: [ 0.19207861]; G_loss: [ 1.56837571]\n",
      "epoch: 4\n",
      "Iter-0; D_loss: [ 0.19567782]; G_loss: [ 1.57350707]\n",
      "Iter-256; D_loss: [ 0.19172487]; G_loss: [ 1.57514453]\n",
      "Iter-512; D_loss: [ 0.18177402]; G_loss: [ 1.57386029]\n",
      "Iter-768; D_loss: [ 0.17831659]; G_loss: [ 1.57587385]\n",
      "Iter-1024; D_loss: [ 0.17381158]; G_loss: [ 1.57703245]\n",
      "Iter-1280; D_loss: [ 0.18472233]; G_loss: [ 1.58608115]\n",
      "Iter-1536; D_loss: [ 0.17258288]; G_loss: [ 1.58358812]\n",
      "Iter-1792; D_loss: [ 0.17555937]; G_loss: [ 1.58852661]\n",
      "Iter-2048; D_loss: [ 0.19276109]; G_loss: [ 1.60009873]\n",
      "Iter-2304; D_loss: [ 0.17198166]; G_loss: [ 1.59319222]\n",
      "Iter-2560; D_loss: [ 0.18413374]; G_loss: [ 1.60251713]\n",
      "Iter-2816; D_loss: [ 0.1797322]; G_loss: [ 1.60377645]\n",
      "Iter-3072; D_loss: [ 0.18648751]; G_loss: [ 1.61007428]\n",
      "Iter-3328; D_loss: [ 0.17306913]; G_loss: [ 1.60672092]\n",
      "Iter-3584; D_loss: [ 0.17188384]; G_loss: [ 1.60958135]\n",
      "Iter-3840; D_loss: [ 0.1741498]; G_loss: [ 1.61407495]\n",
      "Iter-4096; D_loss: [ 0.18395542]; G_loss: [ 1.62210453]\n",
      "Iter-4352; D_loss: [ 0.16722423]; G_loss: [ 1.61699915]\n",
      "Iter-4608; D_loss: [ 0.16173115]; G_loss: [ 1.61781859]\n",
      "Iter-4864; D_loss: [ 0.17222378]; G_loss: [ 1.62639081]\n",
      "Iter-5120; D_loss: [ 0.16326842]; G_loss: [ 1.62529445]\n",
      "Iter-5376; D_loss: [ 0.16056226]; G_loss: [ 1.62728155]\n",
      "Iter-5632; D_loss: [ 0.15656614]; G_loss: [ 1.62865639]\n",
      "Iter-5888; D_loss: [ 0.15145209]; G_loss: [ 1.62978971]\n",
      "Iter-6144; D_loss: [ 0.15999125]; G_loss: [ 1.63721395]\n",
      "Iter-6400; D_loss: [ 0.17137742]; G_loss: [ 1.64572442]\n",
      "Iter-6656; D_loss: [ 0.15807283]; G_loss: [ 1.64247763]\n",
      "Iter-6912; D_loss: [ 0.15135778]; G_loss: [ 1.6425432]\n",
      "Iter-7168; D_loss: [ 0.16022226]; G_loss: [ 1.65006673]\n",
      "Iter-7424; D_loss: [ 0.15192474]; G_loss: [ 1.64934993]\n",
      "Iter-7680; D_loss: [ 0.16178849]; G_loss: [ 1.65711546]\n",
      "Iter-7936; D_loss: [ 0.16968596]; G_loss: [ 1.66408265]\n",
      "Iter-8192; D_loss: [ 0.1571973]; G_loss: [ 1.66084814]\n",
      "Iter-8448; D_loss: [ 0.16893104]; G_loss: [ 1.66960633]\n",
      "Iter-8704; D_loss: [ 0.16097051]; G_loss: [ 1.66855323]\n",
      "Iter-8960; D_loss: [ 0.16573261]; G_loss: [ 1.67355728]\n",
      "Iter-9216; D_loss: [ 0.15231474]; G_loss: [ 1.66984057]\n",
      "Iter-9472; D_loss: [ 0.13556015]; G_loss: [ 1.66498268]\n",
      "Iter-9728; D_loss: [ 0.13661326]; G_loss: [ 1.66877759]\n",
      "Iter-9984; D_loss: [ 0.15249781]; G_loss: [ 1.68003178]\n",
      "Iter-10240; D_loss: [ 0.14676282]; G_loss: [ 1.68049061]\n",
      "Iter-10496; D_loss: [ 0.14825931]; G_loss: [ 1.68429065]\n",
      "Iter-10752; D_loss: [ 0.14627805]; G_loss: [ 1.68644631]\n",
      "Iter-11008; D_loss: [ 0.13996229]; G_loss: [ 1.68631935]\n",
      "Iter-11264; D_loss: [ 0.14803545]; G_loss: [ 1.69335449]\n",
      "Iter-11520; D_loss: [ 0.16158147]; G_loss: [ 1.70287943]\n",
      "Iter-11776; D_loss: [ 0.15428978]; G_loss: [ 1.70200074]\n",
      "Iter-12032; D_loss: [ 0.15857951]; G_loss: [ 1.70675695]\n",
      "Iter-12288; D_loss: [ 0.1422599]; G_loss: [ 1.70176792]\n",
      "Iter-12544; D_loss: [ 0.15659043]; G_loss: [ 1.71179402]\n",
      "Iter-12800; D_loss: [ 0.13909498]; G_loss: [ 1.70617175]\n",
      "Iter-13056; D_loss: [ 0.14463481]; G_loss: [ 1.71206033]\n",
      "Iter-13312; D_loss: [ 0.15788092]; G_loss: [ 1.72142673]\n",
      "Iter-13568; D_loss: [ 0.1375878]; G_loss: [ 1.71445382]\n",
      "Iter-13824; D_loss: [ 0.1403311]; G_loss: [ 1.7186296]\n",
      "Iter-14080; D_loss: [ 0.12819934]; G_loss: [ 1.71584964]\n",
      "Iter-14336; D_loss: [ 0.13979894]; G_loss: [ 1.72460973]\n",
      "Iter-14592; D_loss: [ 0.14252211]; G_loss: [ 1.72877634]\n",
      "Iter-14848; D_loss: [ 0.13725919]; G_loss: [ 1.72900903]\n",
      "Iter-15104; D_loss: [ 0.13170463]; G_loss: [ 1.72923136]\n",
      "Iter-15360; D_loss: [ 0.12751365]; G_loss: [ 1.73024321]\n",
      "Iter-15616; D_loss: [ 0.14845206]; G_loss: [ 1.7434051]\n",
      "Iter-15872; D_loss: [ 0.13097565]; G_loss: [ 1.73802805]\n",
      "Iter-16128; D_loss: [ 0.14897162]; G_loss: [ 1.7495898]\n",
      "Iter-16384; D_loss: [ 0.1417968]; G_loss: [ 1.74880993]\n",
      "Iter-16640; D_loss: [ 0.12501733]; G_loss: [ 1.74349451]\n",
      "Iter-16896; D_loss: [ 0.12916397]; G_loss: [ 1.74842465]\n",
      "Iter-17152; D_loss: [ 0.12827347]; G_loss: [ 1.75104773]\n",
      "Iter-17408; D_loss: [ 0.12959576]; G_loss: [ 1.75471401]\n",
      "Iter-17664; D_loss: [ 0.12025054]; G_loss: [ 1.75327158]\n",
      "Iter-17920; D_loss: [ 0.13697371]; G_loss: [ 1.76435959]\n",
      "Iter-18176; D_loss: [ 0.13510342]; G_loss: [ 1.76620078]\n",
      "Iter-18432; D_loss: [ 0.12826358]; G_loss: [ 1.7656039]\n",
      "Iter-18688; D_loss: [ 0.14569296]; G_loss: [ 1.77631986]\n",
      "Iter-18944; D_loss: [ 0.12179513]; G_loss: [ 1.76765513]\n",
      "Iter-19200; D_loss: [ 0.13220666]; G_loss: [ 1.77535868]\n",
      "Iter-19456; D_loss: [ 0.12297888]; G_loss: [ 1.77370358]\n",
      "Iter-19712; D_loss: [ 0.13194883]; G_loss: [ 1.78093958]\n",
      "Iter-19968; D_loss: [ 0.13557853]; G_loss: [ 1.78541517]\n",
      "Iter-20224; D_loss: [ 0.12979302]; G_loss: [ 1.78524435]\n",
      "Iter-20480; D_loss: [ 0.1272178]; G_loss: [ 1.78679407]\n",
      "Iter-20736; D_loss: [ 0.13240632]; G_loss: [ 1.79177606]\n",
      "Iter-20992; D_loss: [ 0.12754689]; G_loss: [ 1.79185879]\n",
      "Iter-21248; D_loss: [ 0.11851948]; G_loss: [ 1.79008579]\n",
      "Iter-21504; D_loss: [ 0.12795484]; G_loss: [ 1.79751623]\n",
      "Iter-21760; D_loss: [ 0.12754767]; G_loss: [ 1.7999121]\n",
      "epoch: 5\n",
      "Iter-0; D_loss: [ 0.13132696]; G_loss: [ 1.8042053]\n",
      "Iter-256; D_loss: [ 0.13050693]; G_loss: [ 1.80648637]\n",
      "Iter-512; D_loss: [ 0.12024777]; G_loss: [ 1.80424237]\n",
      "Iter-768; D_loss: [ 0.11735919]; G_loss: [ 1.80568159]\n",
      "Iter-1024; D_loss: [ 0.11462422]; G_loss: [ 1.80689549]\n",
      "Iter-1280; D_loss: [ 0.12423587]; G_loss: [ 1.81451106]\n",
      "Iter-1536; D_loss: [ 0.11361231]; G_loss: [ 1.81182241]\n",
      "Iter-1792; D_loss: [ 0.11641207]; G_loss: [ 1.81592143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2048; D_loss: [ 0.12843327]; G_loss: [ 1.82415986]\n",
      "Iter-2304; D_loss: [ 0.11475236]; G_loss: [ 1.82007885]\n",
      "Iter-2560; D_loss: [ 0.12485467]; G_loss: [ 1.82752228]\n",
      "Iter-2816; D_loss: [ 0.12110968]; G_loss: [ 1.828269]\n",
      "Iter-3072; D_loss: [ 0.12606421]; G_loss: [ 1.83295631]\n",
      "Iter-3328; D_loss: [ 0.1149573]; G_loss: [ 1.8299067]\n",
      "Iter-3584; D_loss: [ 0.11483937]; G_loss: [ 1.83257747]\n",
      "Iter-3840; D_loss: [ 0.11733433]; G_loss: [ 1.83635175]\n",
      "Iter-4096; D_loss: [ 0.12509146]; G_loss: [ 1.84257102]\n",
      "Iter-4352; D_loss: [ 0.11097211]; G_loss: [ 1.83803582]\n",
      "Iter-4608; D_loss: [ 0.10868907]; G_loss: [ 1.83963072]\n",
      "Iter-4864; D_loss: [ 0.11722612]; G_loss: [ 1.84642351]\n",
      "Iter-5120; D_loss: [ 0.11058782]; G_loss: [ 1.8456794]\n",
      "Iter-5376; D_loss: [ 0.10779258]; G_loss: [ 1.84692109]\n",
      "Iter-5632; D_loss: [ 0.10426028]; G_loss: [ 1.84783924]\n",
      "Iter-5888; D_loss: [ 0.10148149]; G_loss: [ 1.84926975]\n",
      "Iter-6144; D_loss: [ 0.1080658]; G_loss: [ 1.85501754]\n",
      "Iter-6400; D_loss: [ 0.11663115]; G_loss: [ 1.86136413]\n",
      "Iter-6656; D_loss: [ 0.10603718]; G_loss: [ 1.85875154]\n",
      "Iter-6912; D_loss: [ 0.10205949]; G_loss: [ 1.85937405]\n",
      "Iter-7168; D_loss: [ 0.10850529]; G_loss: [ 1.86501801]\n",
      "Iter-7424; D_loss: [ 0.101592]; G_loss: [ 1.86436331]\n",
      "Iter-7680; D_loss: [ 0.10961516]; G_loss: [ 1.87056005]\n",
      "Iter-7936; D_loss: [ 0.11585774]; G_loss: [ 1.87589729]\n",
      "Iter-8192; D_loss: [ 0.10633223]; G_loss: [ 1.87357199]\n",
      "Iter-8448; D_loss: [ 0.11720443]; G_loss: [ 1.88116038]\n",
      "Iter-8704; D_loss: [ 0.11056431]; G_loss: [ 1.88000524]\n",
      "Iter-8960; D_loss: [ 0.113547]; G_loss: [ 1.88344646]\n",
      "Iter-9216; D_loss: [ 0.10408822]; G_loss: [ 1.88098466]\n",
      "Iter-9472; D_loss: [ 0.09120712]; G_loss: [ 1.8774991]\n",
      "Iter-9728; D_loss: [ 0.09211542]; G_loss: [ 1.8805815]\n",
      "Iter-9984; D_loss: [ 0.10426231]; G_loss: [ 1.88924134]\n",
      "Iter-10240; D_loss: [ 0.09991059]; G_loss: [ 1.88967669]\n",
      "Iter-10496; D_loss: [ 0.10068103]; G_loss: [ 1.89240324]\n"
     ]
    }
   ],
   "source": [
    "batch_size = mb_size\n",
    "# Start training\n",
    "for epoch in range(50):\n",
    "    \n",
    "    \n",
    "\n",
    "    print('epoch:',epoch)\n",
    "    #for i in range(XX_train):\n",
    "    # Build mini-batch dataset\n",
    "    #batch_size = images.size(0)\n",
    "    #images = to_var(images.view(batch_size, -1))\n",
    "\n",
    "    it=0\n",
    "    while it+batch_size < len(X_train) :\n",
    "        \n",
    "\n",
    "        start= it\n",
    "        end= it + batch_size\n",
    "\n",
    "\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        X = X_train[start:end]\n",
    "\n",
    "        #c = Y_train[start:end]\n",
    "        X = Variable(torch.from_numpy(X))\n",
    "        #c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "        #z = z.cuda()\n",
    "        #X = X.cuda()\n",
    "        # Dicriminator forward-loss-backward-update\n",
    "        G_sample = G(z, X)\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        D_loss_real = binary_cross_entropy(D_real, ones_label)\n",
    "        D_loss_fake = binary_cross_entropy(D_fake, zeros_label)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Generator forward-loss-backward-update\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        G_sample = G(z, X)\n",
    "        X_fake = torch.cat([G_sample, X], 0)\n",
    "        D_fake = D(X_fake) #Here we need to give Xi along with Xg(i.e. Xg+X or G_sample+X)\n",
    "\n",
    "        G_loss = binary_cross_entropy(D_fake, ones_label_fake)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        D.zero_grad()\n",
    "\n",
    "        #Print and plot every now and then\n",
    "        #if it % 2 == 0:\n",
    "\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "        it+= batch_size\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(D_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF=pd.DataFrame()\n",
    "samples_per_class = 1000\n",
    "#c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "#c[:, np.random.randint(0, 10)] = 1.\n",
    "for i in range(14):\n",
    "    #print(i)\n",
    "    c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "    c[:, i] = 1.\n",
    "    c_df=pd.DataFrame(c)\n",
    "    df_SF = df_SF.append(c_df,ignore_index = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = df_SF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen = Variable(torch.randn(df_SF.shape[0], Z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = Variable(torch.from_numpy(c_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = G(z_gen, c_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the generated iVectors we will try to check the acc by MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = samples.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1 = c_gen.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "Y_train = pd.DataFrame(Y_train)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = pd.DataFrame(train_X1)\n",
    "train_y1 = pd.DataFrame(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train.append(train_X1, ignore_index=True)\n",
    "train_y = Y_train.append(train_y1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X,  train_y = shuffle(train_X, train_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.values\n",
    "train_y = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_filepath = '/home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5'\n",
    "saveBestModel = ModelCheckpoint(best_weights_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epoch=30"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Checking Baseline Accuracy with only training data\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Baseline ERROR %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train_X1 and train_y1 are the augmented data alone to check accuracy only on augmented data \n",
    "#feed the model.fit only with these\n",
    "train_X1 = train_X1.values\n",
    "train_y1 = train_y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Accuracy with training+augmented data train_X and train_y are 'train + augmented' data\n",
    "history = model.fit(train_X, train_y, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame label accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('ERROR after Data Augmentation %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
