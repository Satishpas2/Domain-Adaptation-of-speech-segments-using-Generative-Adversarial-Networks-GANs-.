{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lre = pd.read_csv('/home/satishk/lre2.0/ivectors_csv_revised/train_feat_BNF_h5_07Nov_Shreyas.csv')\n",
    "#train_afds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lre = train_lre.iloc[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>langid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.141175</td>\n",
       "      <td>-1.160019</td>\n",
       "      <td>0.891814</td>\n",
       "      <td>1.898842</td>\n",
       "      <td>0.393065</td>\n",
       "      <td>0.983582</td>\n",
       "      <td>-0.559143</td>\n",
       "      <td>-0.761900</td>\n",
       "      <td>0.525598</td>\n",
       "      <td>0.344597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.587687</td>\n",
       "      <td>-0.609223</td>\n",
       "      <td>1.529694</td>\n",
       "      <td>1.677775</td>\n",
       "      <td>-0.388426</td>\n",
       "      <td>1.044584</td>\n",
       "      <td>-1.365691</td>\n",
       "      <td>-0.354752</td>\n",
       "      <td>-0.815562</td>\n",
       "      <td>spa-eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971834</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>0.785385</td>\n",
       "      <td>1.328749</td>\n",
       "      <td>-0.043342</td>\n",
       "      <td>1.219254</td>\n",
       "      <td>0.761090</td>\n",
       "      <td>1.069464</td>\n",
       "      <td>0.879541</td>\n",
       "      <td>0.898939</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.208031</td>\n",
       "      <td>0.527433</td>\n",
       "      <td>-0.709180</td>\n",
       "      <td>-1.117489</td>\n",
       "      <td>0.566520</td>\n",
       "      <td>1.642208</td>\n",
       "      <td>-0.703815</td>\n",
       "      <td>0.376027</td>\n",
       "      <td>-1.425985</td>\n",
       "      <td>spa-eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.315703</td>\n",
       "      <td>-0.868423</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>1.717784</td>\n",
       "      <td>-0.846024</td>\n",
       "      <td>1.177214</td>\n",
       "      <td>-0.191977</td>\n",
       "      <td>-0.658569</td>\n",
       "      <td>0.529625</td>\n",
       "      <td>0.313590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.896515</td>\n",
       "      <td>-0.128051</td>\n",
       "      <td>-0.075781</td>\n",
       "      <td>1.307904</td>\n",
       "      <td>-1.443047</td>\n",
       "      <td>2.551083</td>\n",
       "      <td>0.436309</td>\n",
       "      <td>-0.623076</td>\n",
       "      <td>0.194851</td>\n",
       "      <td>spa-eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.247949</td>\n",
       "      <td>-0.279720</td>\n",
       "      <td>1.320115</td>\n",
       "      <td>1.772152</td>\n",
       "      <td>-0.130394</td>\n",
       "      <td>0.517343</td>\n",
       "      <td>-0.290516</td>\n",
       "      <td>0.406225</td>\n",
       "      <td>-0.833366</td>\n",
       "      <td>1.567592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398762</td>\n",
       "      <td>-0.078148</td>\n",
       "      <td>0.168076</td>\n",
       "      <td>-0.272796</td>\n",
       "      <td>-1.102862</td>\n",
       "      <td>0.358274</td>\n",
       "      <td>0.090417</td>\n",
       "      <td>-0.490244</td>\n",
       "      <td>1.819196</td>\n",
       "      <td>spa-eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.986578</td>\n",
       "      <td>-0.357548</td>\n",
       "      <td>1.550832</td>\n",
       "      <td>1.400735</td>\n",
       "      <td>0.444350</td>\n",
       "      <td>0.465660</td>\n",
       "      <td>0.209248</td>\n",
       "      <td>1.175847</td>\n",
       "      <td>-1.079963</td>\n",
       "      <td>0.270915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947946</td>\n",
       "      <td>0.917957</td>\n",
       "      <td>-1.043352</td>\n",
       "      <td>0.414811</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>-0.485602</td>\n",
       "      <td>-0.433002</td>\n",
       "      <td>-0.432070</td>\n",
       "      <td>0.778250</td>\n",
       "      <td>spa-eur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.141175 -1.160019  0.891814  1.898842  0.393065  0.983582 -0.559143   \n",
       "1  0.971834  0.035743  0.785385  1.328749 -0.043342  1.219254  0.761090   \n",
       "2 -0.315703 -0.868423  0.619893  1.717784 -0.846024  1.177214 -0.191977   \n",
       "3  0.247949 -0.279720  1.320115  1.772152 -0.130394  0.517343 -0.290516   \n",
       "4 -0.986578 -0.357548  1.550832  1.400735  0.444350  0.465660  0.209248   \n",
       "\n",
       "          7         8         9   ...          491       492       493  \\\n",
       "0 -0.761900  0.525598  0.344597   ...     1.587687 -0.609223  1.529694   \n",
       "1  1.069464  0.879541  0.898939   ...    -1.208031  0.527433 -0.709180   \n",
       "2 -0.658569  0.529625  0.313590   ...    -0.896515 -0.128051 -0.075781   \n",
       "3  0.406225 -0.833366  1.567592   ...    -0.398762 -0.078148  0.168076   \n",
       "4  1.175847 -1.079963  0.270915   ...     0.947946  0.917957 -1.043352   \n",
       "\n",
       "        494       495       496       497       498       499   langid  \n",
       "0  1.677775 -0.388426  1.044584 -1.365691 -0.354752 -0.815562  spa-eur  \n",
       "1 -1.117489  0.566520  1.642208 -0.703815  0.376027 -1.425985  spa-eur  \n",
       "2  1.307904 -1.443047  2.551083  0.436309 -0.623076  0.194851  spa-eur  \n",
       "3 -0.272796 -1.102862  0.358274  0.090417 -0.490244  1.819196  spa-eur  \n",
       "4  0.414811  0.016396 -0.485602 -0.433002 -0.432070  0.778250  spa-eur  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langid\n",
       "ara-acm    1406\n",
       "ara-apc    3509\n",
       "ara-ary     919\n",
       "ara-arz     440\n",
       "eng-gbr      98\n",
       "eng-usg    2448\n",
       "por-brz     444\n",
       "qsl-pol     587\n",
       "qsl-rus    1221\n",
       "spa-car     688\n",
       "spa-eur     121\n",
       "spa-lac     898\n",
       "zho-cmn    3331\n",
       "zho-nan      95\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lre.groupby(['langid']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lre = pd.read_csv('/home/satishk/lre2.0/ivectors_csv_revised/dev_feat_BNF_h5_07Nov_Shreyas.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_lre = val_lre.iloc[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentid</th>\n",
       "      <th>language_code</th>\n",
       "      <th>data_source</th>\n",
       "      <th>speech_duration</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>uttid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lre17_ntrlosgu.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>30</td>\n",
       "      <td>1.697234</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>-0.400756</td>\n",
       "      <td>0.513963</td>\n",
       "      <td>-0.939232</td>\n",
       "      <td>1.500797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.314428</td>\n",
       "      <td>-0.927694</td>\n",
       "      <td>-0.370424</td>\n",
       "      <td>-0.514735</td>\n",
       "      <td>1.290885</td>\n",
       "      <td>0.688205</td>\n",
       "      <td>-0.494330</td>\n",
       "      <td>-0.053206</td>\n",
       "      <td>-1.330860</td>\n",
       "      <td>lre17_ntrlosgu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lre17_moxnwuqe.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>10</td>\n",
       "      <td>1.648232</td>\n",
       "      <td>-0.053318</td>\n",
       "      <td>-0.562867</td>\n",
       "      <td>1.035870</td>\n",
       "      <td>-1.577741</td>\n",
       "      <td>1.593584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929262</td>\n",
       "      <td>-1.301574</td>\n",
       "      <td>2.034934</td>\n",
       "      <td>-0.226545</td>\n",
       "      <td>-0.198926</td>\n",
       "      <td>-0.116174</td>\n",
       "      <td>0.347923</td>\n",
       "      <td>-0.870801</td>\n",
       "      <td>-2.599601</td>\n",
       "      <td>lre17_moxnwuqe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lre17_meesvkxz.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>3</td>\n",
       "      <td>1.242829</td>\n",
       "      <td>0.675515</td>\n",
       "      <td>-0.371491</td>\n",
       "      <td>0.534970</td>\n",
       "      <td>-0.246783</td>\n",
       "      <td>0.806262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>1.058771</td>\n",
       "      <td>1.018635</td>\n",
       "      <td>-1.929319</td>\n",
       "      <td>-0.307404</td>\n",
       "      <td>-0.486431</td>\n",
       "      <td>-2.839053</td>\n",
       "      <td>-2.704527</td>\n",
       "      <td>lre17_meesvkxz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lre17_rqmsmzui.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>30</td>\n",
       "      <td>1.226681</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>-0.396915</td>\n",
       "      <td>-0.097507</td>\n",
       "      <td>-0.013574</td>\n",
       "      <td>1.087025</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049862</td>\n",
       "      <td>0.285627</td>\n",
       "      <td>2.385587</td>\n",
       "      <td>0.680073</td>\n",
       "      <td>1.500978</td>\n",
       "      <td>1.660566</td>\n",
       "      <td>-0.370672</td>\n",
       "      <td>-0.924109</td>\n",
       "      <td>0.096676</td>\n",
       "      <td>lre17_rqmsmzui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lre17_qgszpuyw.sph</td>\n",
       "      <td>ara-acm</td>\n",
       "      <td>mls14</td>\n",
       "      <td>10</td>\n",
       "      <td>1.411728</td>\n",
       "      <td>-0.119300</td>\n",
       "      <td>0.136256</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>-1.029447</td>\n",
       "      <td>1.227100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>-1.030222</td>\n",
       "      <td>2.933880</td>\n",
       "      <td>-1.417872</td>\n",
       "      <td>-0.227513</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>-0.671044</td>\n",
       "      <td>0.595977</td>\n",
       "      <td>1.722917</td>\n",
       "      <td>lre17_qgszpuyw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            segmentid language_code data_source  speech_duration         0  \\\n",
       "0  lre17_ntrlosgu.sph       ara-acm       mls14               30  1.697234   \n",
       "1  lre17_moxnwuqe.sph       ara-acm       mls14               10  1.648232   \n",
       "2  lre17_meesvkxz.sph       ara-acm       mls14                3  1.242829   \n",
       "3  lre17_rqmsmzui.sph       ara-acm       mls14               30  1.226681   \n",
       "4  lre17_qgszpuyw.sph       ara-acm       mls14               10  1.411728   \n",
       "\n",
       "          1         2         3         4         5       ...             491  \\\n",
       "0  0.029428 -0.400756  0.513963 -0.939232  1.500797       ...       -1.314428   \n",
       "1 -0.053318 -0.562867  1.035870 -1.577741  1.593584       ...       -0.929262   \n",
       "2  0.675515 -0.371491  0.534970 -0.246783  0.806262       ...        0.691336   \n",
       "3  0.014810 -0.396915 -0.097507 -0.013574  1.087025       ...        1.049862   \n",
       "4 -0.119300  0.136256  0.030535 -1.029447  1.227100       ...        0.155196   \n",
       "\n",
       "        492       493       494       495       496       497       498  \\\n",
       "0 -0.927694 -0.370424 -0.514735  1.290885  0.688205 -0.494330 -0.053206   \n",
       "1 -1.301574  2.034934 -0.226545 -0.198926 -0.116174  0.347923 -0.870801   \n",
       "2  0.257988  1.058771  1.018635 -1.929319 -0.307404 -0.486431 -2.839053   \n",
       "3  0.285627  2.385587  0.680073  1.500978  1.660566 -0.370672 -0.924109   \n",
       "4 -1.030222  2.933880 -1.417872 -0.227513  0.748810 -0.671044  0.595977   \n",
       "\n",
       "        499           uttid  \n",
       "0 -1.330860  lre17_ntrlosgu  \n",
       "1 -2.599601  lre17_moxnwuqe  \n",
       "2 -2.704527  lre17_meesvkxz  \n",
       "3  0.096676  lre17_rqmsmzui  \n",
       "4  1.722917  lre17_qgszpuyw  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_lre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_lre.drop(\"langid\",axis=1)\n",
    "y_train = train_lre[\"langid\"]\n",
    "#y_train_uttid = train_lre[\"uttid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_lre.drop([\"language_code\",\"uttid\",\"segmentid\",\"data_source\",\"speech_duration\"],axis=1)\n",
    "y_val = val_lre[\"language_code\"]\n",
    "y_val_segmentid = val_lre[\"segmentid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_val, ignore_index=True)\n",
    "y_train = y_train.append(y_val, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ara-acm', 'ara-apc', 'ara-ary', 'ara-arz', 'eng-gbr', 'eng-usg',\n",
       "       'por-brz', 'qsl-pol', 'qsl-rus', 'spa-car', 'spa-eur', 'spa-lac',\n",
       "       'zho-cmn', 'zho-nan'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ara-acm', 'ara-apc', 'ara-ary', 'ara-arz', 'eng-gbr', 'eng-usg',\n",
       "       'por-brz', 'qsl-pol', 'qsl-rus', 'spa-car', 'spa-eur', 'spa-lac',\n",
       "       'zho-cmn', 'zho-nan'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "X_val=X_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 14"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val_labels, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = indices_to_one_hot(y_train, 14)\n",
    "#Y_test = indices_to_one_hot(y_test, 14)\n",
    "Y_val = indices_to_one_hot(y_val_labels, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3661, 14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19866, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (19866, 500)\n",
      "19866 train samples\n",
      "3661 val samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "#X_val /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "print(X_val.shape[0], 'val samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19866, 500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19866,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,  y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3661, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 13, 13, 13])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Shuffling the dataset\n",
    "#from sklearn.utils import shuffle\n",
    "#X_train,  y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val,  y_val_labels = shuffle(X_val, y_val_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = input_data.read_data_sets('/home/satishk/depy_04_AUG/MNIST_data', one_hot=True)\n",
    "mb_size = 256\n",
    "Z_dim = 100\n",
    "X_dim = 500 #mnist.train.images.shape[1]\n",
    "y_dim = 14 #mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dim, y_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def G(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X, c):\n",
    "    inputs = torch.cat([X, c], 1)\n",
    "    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "#ones_label = Variable(torch.ones(mb_size))\n",
    "#zeros_label = Variable(torch.zeros(mb_size))\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15892"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter-0; D_loss: [ 2.3023262]; G_loss: [ 0.65551734]\n",
      "Iter-256; D_loss: [ 1.25758111]; G_loss: [ 1.44586122]\n",
      "Iter-512; D_loss: [ 0.66614532]; G_loss: [ 2.40486073]\n",
      "Iter-768; D_loss: [ 0.51211506]; G_loss: [ 3.29984045]\n",
      "Iter-1024; D_loss: [ 0.38790327]; G_loss: [ 4.02416039]\n",
      "Iter-1280; D_loss: [ 0.3340939]; G_loss: [ 4.55242443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishk/miniconda3/envs/lre17/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1536; D_loss: [ 0.32332253]; G_loss: [ 4.98773336]\n",
      "Iter-1792; D_loss: [ 0.30346888]; G_loss: [ 5.33819342]\n",
      "Iter-2048; D_loss: [ 0.2884827]; G_loss: [ 5.46230412]\n",
      "Iter-2304; D_loss: [ 0.25673652]; G_loss: [ 5.57792759]\n",
      "Iter-2560; D_loss: [ 0.29571006]; G_loss: [ 5.64273214]\n",
      "Iter-2816; D_loss: [ 0.24197854]; G_loss: [ 5.65974188]\n",
      "Iter-3072; D_loss: [ 0.19153138]; G_loss: [ 5.63087082]\n",
      "Iter-3328; D_loss: [ 0.20759515]; G_loss: [ 5.51941299]\n",
      "Iter-3584; D_loss: [ 0.1678593]; G_loss: [ 5.46293974]\n",
      "Iter-3840; D_loss: [ 0.15125678]; G_loss: [ 5.22235489]\n",
      "Iter-4096; D_loss: [ 0.1410941]; G_loss: [ 5.11091948]\n",
      "Iter-4352; D_loss: [ 0.14233965]; G_loss: [ 4.91631126]\n",
      "Iter-4608; D_loss: [ 0.11724956]; G_loss: [ 4.77727318]\n",
      "Iter-4864; D_loss: [ 0.13049422]; G_loss: [ 4.56257248]\n",
      "Iter-5120; D_loss: [ 0.13021444]; G_loss: [ 4.36846733]\n",
      "Iter-5376; D_loss: [ 0.10716747]; G_loss: [ 4.24481344]\n",
      "Iter-5632; D_loss: [ 0.12209201]; G_loss: [ 4.01036072]\n",
      "Iter-5888; D_loss: [ 0.11139617]; G_loss: [ 3.83996725]\n",
      "Iter-6144; D_loss: [ 0.10587974]; G_loss: [ 3.79762912]\n",
      "Iter-6400; D_loss: [ 0.11144722]; G_loss: [ 3.57844853]\n",
      "Iter-6656; D_loss: [ 0.10202131]; G_loss: [ 3.54259086]\n",
      "Iter-6912; D_loss: [ 0.1136924]; G_loss: [ 3.45168877]\n",
      "Iter-7168; D_loss: [ 0.10765417]; G_loss: [ 3.43777728]\n",
      "Iter-7424; D_loss: [ 0.11113509]; G_loss: [ 3.3927474]\n",
      "Iter-7680; D_loss: [ 0.11357887]; G_loss: [ 3.33257318]\n",
      "Iter-7936; D_loss: [ 0.10600515]; G_loss: [ 3.39726138]\n",
      "Iter-8192; D_loss: [ 0.11475496]; G_loss: [ 3.42168593]\n",
      "Iter-8448; D_loss: [ 0.10979355]; G_loss: [ 3.35313272]\n",
      "Iter-8704; D_loss: [ 0.09203984]; G_loss: [ 3.38009214]\n",
      "Iter-8960; D_loss: [ 0.10597794]; G_loss: [ 3.51247644]\n",
      "Iter-9216; D_loss: [ 0.09252089]; G_loss: [ 3.51386786]\n",
      "Iter-9472; D_loss: [ 0.0812761]; G_loss: [ 3.49248457]\n",
      "Iter-9728; D_loss: [ 0.08876155]; G_loss: [ 3.63205123]\n",
      "Iter-9984; D_loss: [ 0.07283977]; G_loss: [ 3.67378163]\n",
      "Iter-10240; D_loss: [ 0.07541463]; G_loss: [ 3.76725912]\n",
      "Iter-10496; D_loss: [ 0.07285263]; G_loss: [ 3.79521751]\n",
      "Iter-10752; D_loss: [ 0.06871961]; G_loss: [ 3.81241632]\n",
      "Iter-11008; D_loss: [ 0.07463965]; G_loss: [ 3.84607053]\n",
      "Iter-11264; D_loss: [ 0.07813832]; G_loss: [ 3.89847088]\n",
      "Iter-11520; D_loss: [ 0.07053929]; G_loss: [ 3.94477844]\n",
      "Iter-11776; D_loss: [ 0.06116106]; G_loss: [ 3.99337077]\n",
      "Iter-12032; D_loss: [ 0.05949479]; G_loss: [ 4.02893209]\n",
      "Iter-12288; D_loss: [ 0.05721856]; G_loss: [ 4.06154013]\n",
      "Iter-12544; D_loss: [ 0.06998608]; G_loss: [ 4.01418352]\n",
      "Iter-12800; D_loss: [ 0.05300451]; G_loss: [ 4.07744408]\n",
      "Iter-13056; D_loss: [ 0.05201011]; G_loss: [ 4.06434441]\n",
      "Iter-13312; D_loss: [ 0.0521035]; G_loss: [ 4.06041718]\n",
      "Iter-13568; D_loss: [ 0.06570856]; G_loss: [ 4.08726549]\n",
      "Iter-13824; D_loss: [ 0.04967537]; G_loss: [ 4.0773201]\n",
      "Iter-14080; D_loss: [ 0.05480161]; G_loss: [ 4.07409668]\n",
      "Iter-14336; D_loss: [ 0.05840568]; G_loss: [ 4.07226276]\n",
      "Iter-14592; D_loss: [ 0.05096919]; G_loss: [ 4.03052759]\n",
      "Iter-14848; D_loss: [ 0.05639367]; G_loss: [ 4.07983065]\n",
      "Iter-15104; D_loss: [ 0.05792606]; G_loss: [ 4.01443958]\n",
      "Iter-15360; D_loss: [ 0.06212347]; G_loss: [ 4.01794481]\n",
      "Iter-15616; D_loss: [ 0.05348846]; G_loss: [ 3.96109796]\n",
      "epoch: 1\n",
      "Iter-0; D_loss: [ 0.0563744]; G_loss: [ 3.96426702]\n",
      "Iter-256; D_loss: [ 0.04532828]; G_loss: [ 3.92080593]\n",
      "Iter-512; D_loss: [ 0.04776665]; G_loss: [ 3.91356564]\n",
      "Iter-768; D_loss: [ 0.04680297]; G_loss: [ 3.90302992]\n",
      "Iter-1024; D_loss: [ 0.04448839]; G_loss: [ 3.90736842]\n",
      "Iter-1280; D_loss: [ 0.04856926]; G_loss: [ 3.88538766]\n",
      "Iter-1536; D_loss: [ 0.05285294]; G_loss: [ 3.83322072]\n",
      "Iter-1792; D_loss: [ 0.05107319]; G_loss: [ 3.88276839]\n",
      "Iter-2048; D_loss: [ 0.04967284]; G_loss: [ 3.85204101]\n",
      "Iter-2304; D_loss: [ 0.04857729]; G_loss: [ 3.83868527]\n",
      "Iter-2560; D_loss: [ 0.05406924]; G_loss: [ 3.83670259]\n",
      "Iter-2816; D_loss: [ 0.04836261]; G_loss: [ 3.81492352]\n",
      "Iter-3072; D_loss: [ 0.04535468]; G_loss: [ 3.8067615]\n",
      "Iter-3328; D_loss: [ 0.053216]; G_loss: [ 3.7684617]\n",
      "Iter-3584; D_loss: [ 0.04897732]; G_loss: [ 3.82052946]\n",
      "Iter-3840; D_loss: [ 0.04336928]; G_loss: [ 3.76509428]\n",
      "Iter-4096; D_loss: [ 0.04870344]; G_loss: [ 3.75805283]\n",
      "Iter-4352; D_loss: [ 0.04895042]; G_loss: [ 3.76074553]\n",
      "Iter-4608; D_loss: [ 0.04296269]; G_loss: [ 3.72662377]\n",
      "Iter-4864; D_loss: [ 0.04880069]; G_loss: [ 3.7284193]\n",
      "Iter-5120; D_loss: [ 0.0469263]; G_loss: [ 3.71299291]\n",
      "Iter-5376; D_loss: [ 0.04726819]; G_loss: [ 3.69657588]\n",
      "Iter-5632; D_loss: [ 0.052799]; G_loss: [ 3.70253706]\n",
      "Iter-5888; D_loss: [ 0.0495269]; G_loss: [ 3.67713571]\n",
      "Iter-6144; D_loss: [ 0.04920777]; G_loss: [ 3.71761036]\n",
      "Iter-6400; D_loss: [ 0.05854477]; G_loss: [ 3.65666509]\n",
      "Iter-6656; D_loss: [ 0.05486159]; G_loss: [ 3.62986422]\n",
      "Iter-6912; D_loss: [ 0.05693154]; G_loss: [ 3.60632181]\n",
      "Iter-7168; D_loss: [ 0.05934441]; G_loss: [ 3.62152648]\n",
      "Iter-7424; D_loss: [ 0.05546097]; G_loss: [ 3.55514407]\n",
      "Iter-7680; D_loss: [ 0.05984863]; G_loss: [ 3.54953003]\n",
      "Iter-7936; D_loss: [ 0.06018876]; G_loss: [ 3.50208354]\n",
      "Iter-8192; D_loss: [ 0.0731681]; G_loss: [ 3.48696351]\n",
      "Iter-8448; D_loss: [ 0.06277304]; G_loss: [ 3.46932769]\n",
      "Iter-8704; D_loss: [ 0.06758052]; G_loss: [ 3.45201659]\n",
      "Iter-8960; D_loss: [ 0.06043287]; G_loss: [ 3.42486382]\n",
      "Iter-9216; D_loss: [ 0.06347054]; G_loss: [ 3.41495132]\n",
      "Iter-9472; D_loss: [ 0.06243131]; G_loss: [ 3.38167191]\n",
      "Iter-9728; D_loss: [ 0.07041974]; G_loss: [ 3.38299155]\n",
      "Iter-9984; D_loss: [ 0.05675784]; G_loss: [ 3.35094905]\n",
      "Iter-10240; D_loss: [ 0.06419342]; G_loss: [ 3.35777736]\n",
      "Iter-10496; D_loss: [ 0.064867]; G_loss: [ 3.34398413]\n",
      "Iter-10752; D_loss: [ 0.0627827]; G_loss: [ 3.32979298]\n",
      "Iter-11008; D_loss: [ 0.07314789]; G_loss: [ 3.31722951]\n",
      "Iter-11264; D_loss: [ 0.08055239]; G_loss: [ 3.29398704]\n",
      "Iter-11520; D_loss: [ 0.08238164]; G_loss: [ 3.29265976]\n",
      "Iter-11776; D_loss: [ 0.07682665]; G_loss: [ 3.23985815]\n",
      "Iter-12032; D_loss: [ 0.07759641]; G_loss: [ 3.23044062]\n",
      "Iter-12288; D_loss: [ 0.07840031]; G_loss: [ 3.21749806]\n",
      "Iter-12544; D_loss: [ 0.07356535]; G_loss: [ 3.18389916]\n",
      "Iter-12800; D_loss: [ 0.06908342]; G_loss: [ 3.17479968]\n",
      "Iter-13056; D_loss: [ 0.07217922]; G_loss: [ 3.16961432]\n",
      "Iter-13312; D_loss: [ 0.07164053]; G_loss: [ 3.0893333]\n",
      "Iter-13568; D_loss: [ 0.08640324]; G_loss: [ 3.09562898]\n",
      "Iter-13824; D_loss: [ 0.0850611]; G_loss: [ 3.07577109]\n",
      "Iter-14080; D_loss: [ 0.07896054]; G_loss: [ 3.01895022]\n",
      "Iter-14336; D_loss: [ 0.08668848]; G_loss: [ 2.9869256]\n",
      "Iter-14592; D_loss: [ 0.09636912]; G_loss: [ 2.95926499]\n",
      "Iter-14848; D_loss: [ 0.09235477]; G_loss: [ 2.92246509]\n",
      "Iter-15104; D_loss: [ 0.10219742]; G_loss: [ 2.82313585]\n",
      "Iter-15360; D_loss: [ 0.10663879]; G_loss: [ 2.7901125]\n",
      "Iter-15616; D_loss: [ 0.11186324]; G_loss: [ 2.69694757]\n",
      "epoch: 2\n",
      "Iter-0; D_loss: [ 0.11323862]; G_loss: [ 2.65973282]\n",
      "Iter-256; D_loss: [ 0.10760698]; G_loss: [ 2.58464217]\n",
      "Iter-512; D_loss: [ 0.12309486]; G_loss: [ 2.49494314]\n",
      "Iter-768; D_loss: [ 0.13011989]; G_loss: [ 2.44798207]\n",
      "Iter-1024; D_loss: [ 0.14720243]; G_loss: [ 2.34093142]\n",
      "Iter-1280; D_loss: [ 0.15890953]; G_loss: [ 2.22560978]\n",
      "Iter-1536; D_loss: [ 0.18490037]; G_loss: [ 2.14680886]\n",
      "Iter-1792; D_loss: [ 0.19171779]; G_loss: [ 2.06755614]\n",
      "Iter-2048; D_loss: [ 0.2366225]; G_loss: [ 2.01412725]\n",
      "Iter-2304; D_loss: [ 0.22008039]; G_loss: [ 1.92472565]\n",
      "Iter-2560; D_loss: [ 0.23874795]; G_loss: [ 1.8385874]\n",
      "Iter-2816; D_loss: [ 0.24152446]; G_loss: [ 1.78224099]\n",
      "Iter-3072; D_loss: [ 0.27441183]; G_loss: [ 1.69707835]\n",
      "Iter-3328; D_loss: [ 0.30549517]; G_loss: [ 1.63566005]\n",
      "Iter-3584; D_loss: [ 0.29652339]; G_loss: [ 1.57394469]\n",
      "Iter-3840; D_loss: [ 0.35288817]; G_loss: [ 1.56142092]\n",
      "Iter-4096; D_loss: [ 0.34331608]; G_loss: [ 1.50849056]\n",
      "Iter-4352; D_loss: [ 0.39119351]; G_loss: [ 1.49111593]\n",
      "Iter-4608; D_loss: [ 0.39054191]; G_loss: [ 1.43712759]\n",
      "Iter-4864; D_loss: [ 0.41640222]; G_loss: [ 1.37214351]\n",
      "Iter-5120; D_loss: [ 0.42371264]; G_loss: [ 1.38060236]\n",
      "Iter-5376; D_loss: [ 0.41839308]; G_loss: [ 1.34260416]\n",
      "Iter-5632; D_loss: [ 0.42989108]; G_loss: [ 1.32984757]\n",
      "Iter-5888; D_loss: [ 0.41392893]; G_loss: [ 1.32261539]\n",
      "Iter-6144; D_loss: [ 0.42526111]; G_loss: [ 1.32884157]\n",
      "Iter-6400; D_loss: [ 0.48435351]; G_loss: [ 1.31101012]\n",
      "Iter-6656; D_loss: [ 0.45907536]; G_loss: [ 1.29564404]\n",
      "Iter-6912; D_loss: [ 0.43520203]; G_loss: [ 1.30751884]\n",
      "Iter-7168; D_loss: [ 0.43999386]; G_loss: [ 1.32364047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7424; D_loss: [ 0.41333833]; G_loss: [ 1.33515155]\n",
      "Iter-7680; D_loss: [ 0.42966643]; G_loss: [ 1.33285689]\n",
      "Iter-7936; D_loss: [ 0.38023508]; G_loss: [ 1.38303411]\n",
      "Iter-8192; D_loss: [ 0.45729148]; G_loss: [ 1.41162372]\n",
      "Iter-8448; D_loss: [ 0.41360563]; G_loss: [ 1.41926134]\n",
      "Iter-8704; D_loss: [ 0.41986912]; G_loss: [ 1.45077908]\n",
      "Iter-8960; D_loss: [ 0.3530876]; G_loss: [ 1.48556125]\n",
      "Iter-9216; D_loss: [ 0.34676674]; G_loss: [ 1.51643968]\n",
      "Iter-9472; D_loss: [ 0.33913654]; G_loss: [ 1.55382049]\n",
      "Iter-9728; D_loss: [ 0.32378763]; G_loss: [ 1.57969856]\n",
      "Iter-9984; D_loss: [ 0.28817326]; G_loss: [ 1.6194886]\n",
      "Iter-10240; D_loss: [ 0.30501205]; G_loss: [ 1.65032136]\n",
      "Iter-10496; D_loss: [ 0.29206944]; G_loss: [ 1.67248964]\n",
      "Iter-10752; D_loss: [ 0.28468227]; G_loss: [ 1.70386136]\n",
      "Iter-11008; D_loss: [ 0.29064399]; G_loss: [ 1.72611129]\n",
      "Iter-11264; D_loss: [ 0.27787703]; G_loss: [ 1.76992452]\n",
      "Iter-11520; D_loss: [ 0.28045863]; G_loss: [ 1.77770567]\n",
      "Iter-11776; D_loss: [ 0.2676726]; G_loss: [ 1.79940522]\n",
      "Iter-12032; D_loss: [ 0.2297848]; G_loss: [ 1.81674123]\n",
      "Iter-12288; D_loss: [ 0.2692416]; G_loss: [ 1.82788444]\n",
      "Iter-12544; D_loss: [ 0.22844225]; G_loss: [ 1.84788799]\n",
      "Iter-12800; D_loss: [ 0.21928217]; G_loss: [ 1.87214577]\n",
      "Iter-13056; D_loss: [ 0.21601559]; G_loss: [ 1.87941694]\n",
      "Iter-13312; D_loss: [ 0.21773422]; G_loss: [ 1.8979584]\n",
      "Iter-13568; D_loss: [ 0.21597233]; G_loss: [ 1.91510165]\n",
      "Iter-13824; D_loss: [ 0.22551936]; G_loss: [ 1.92086434]\n",
      "Iter-14080; D_loss: [ 0.18956983]; G_loss: [ 1.94567263]\n",
      "Iter-14336; D_loss: [ 0.20416114]; G_loss: [ 1.96974301]\n",
      "Iter-14592; D_loss: [ 0.19159488]; G_loss: [ 1.98255014]\n",
      "Iter-14848; D_loss: [ 0.18869692]; G_loss: [ 1.99277496]\n",
      "Iter-15104; D_loss: [ 0.18025348]; G_loss: [ 2.01703048]\n",
      "Iter-15360; D_loss: [ 0.19068433]; G_loss: [ 2.02269554]\n",
      "Iter-15616; D_loss: [ 0.20077112]; G_loss: [ 2.04955864]\n",
      "epoch: 3\n",
      "Iter-0; D_loss: [ 0.1718733]; G_loss: [ 2.06456757]\n",
      "Iter-256; D_loss: [ 0.15201141]; G_loss: [ 2.08178663]\n",
      "Iter-512; D_loss: [ 0.1706104]; G_loss: [ 2.10851049]\n",
      "Iter-768; D_loss: [ 0.1486643]; G_loss: [ 2.13310719]\n",
      "Iter-1024; D_loss: [ 0.16572571]; G_loss: [ 2.1471343]\n",
      "Iter-1280; D_loss: [ 0.15494941]; G_loss: [ 2.16549134]\n",
      "Iter-1536; D_loss: [ 0.1643531]; G_loss: [ 2.17688584]\n",
      "Iter-1792; D_loss: [ 0.16535036]; G_loss: [ 2.20795441]\n",
      "Iter-2048; D_loss: [ 0.15737346]; G_loss: [ 2.21965742]\n",
      "Iter-2304; D_loss: [ 0.14903487]; G_loss: [ 2.23708749]\n",
      "Iter-2560; D_loss: [ 0.14050344]; G_loss: [ 2.25834751]\n",
      "Iter-2816; D_loss: [ 0.1372291]; G_loss: [ 2.28001118]\n",
      "Iter-3072; D_loss: [ 0.13975036]; G_loss: [ 2.30033755]\n",
      "Iter-3328; D_loss: [ 0.13062681]; G_loss: [ 2.31169868]\n",
      "Iter-3584; D_loss: [ 0.12640144]; G_loss: [ 2.34026384]\n",
      "Iter-3840; D_loss: [ 0.11860053]; G_loss: [ 2.35756946]\n",
      "Iter-4096; D_loss: [ 0.11750087]; G_loss: [ 2.37867475]\n",
      "Iter-4352; D_loss: [ 0.11801022]; G_loss: [ 2.39474607]\n",
      "Iter-4608; D_loss: [ 0.10960341]; G_loss: [ 2.42463422]\n",
      "Iter-4864; D_loss: [ 0.11688097]; G_loss: [ 2.43274975]\n",
      "Iter-5120; D_loss: [ 0.11234838]; G_loss: [ 2.45178556]\n",
      "Iter-5376; D_loss: [ 0.10747261]; G_loss: [ 2.47752142]\n",
      "Iter-5632; D_loss: [ 0.10571322]; G_loss: [ 2.50422072]\n",
      "Iter-5888; D_loss: [ 0.11188132]; G_loss: [ 2.51613188]\n",
      "Iter-6144; D_loss: [ 0.10913168]; G_loss: [ 2.53594851]\n",
      "Iter-6400; D_loss: [ 0.10326536]; G_loss: [ 2.56070375]\n",
      "Iter-6656; D_loss: [ 0.1024917]; G_loss: [ 2.57472229]\n",
      "Iter-6912; D_loss: [ 0.09734514]; G_loss: [ 2.59446406]\n",
      "Iter-7168; D_loss: [ 0.09435072]; G_loss: [ 2.61597896]\n",
      "Iter-7424; D_loss: [ 0.09437966]; G_loss: [ 2.62557173]\n",
      "Iter-7680; D_loss: [ 0.08848187]; G_loss: [ 2.63426995]\n",
      "Iter-7936; D_loss: [ 0.08611678]; G_loss: [ 2.66672492]\n",
      "Iter-8192; D_loss: [ 0.0901117]; G_loss: [ 2.68458486]\n",
      "Iter-8448; D_loss: [ 0.09171294]; G_loss: [ 2.6922276]\n",
      "Iter-8704; D_loss: [ 0.09214967]; G_loss: [ 2.71783042]\n",
      "Iter-8960; D_loss: [ 0.09061532]; G_loss: [ 2.72883177]\n",
      "Iter-9216; D_loss: [ 0.08727313]; G_loss: [ 2.74790549]\n",
      "Iter-9472; D_loss: [ 0.08262943]; G_loss: [ 2.76456714]\n",
      "Iter-9728; D_loss: [ 0.07917327]; G_loss: [ 2.77499533]\n",
      "Iter-9984; D_loss: [ 0.08514959]; G_loss: [ 2.79732227]\n",
      "Iter-10240; D_loss: [ 0.08104841]; G_loss: [ 2.80723977]\n",
      "Iter-10496; D_loss: [ 0.0836271]; G_loss: [ 2.81735277]\n",
      "Iter-10752; D_loss: [ 0.07966923]; G_loss: [ 2.83578777]\n",
      "Iter-11008; D_loss: [ 0.0820969]; G_loss: [ 2.84213543]\n",
      "Iter-11264; D_loss: [ 0.07731324]; G_loss: [ 2.86561012]\n",
      "Iter-11520; D_loss: [ 0.08232185]; G_loss: [ 2.86074591]\n",
      "Iter-11776; D_loss: [ 0.07481056]; G_loss: [ 2.88433862]\n",
      "Iter-12032; D_loss: [ 0.07071053]; G_loss: [ 2.88909817]\n",
      "Iter-12288; D_loss: [ 0.08474434]; G_loss: [ 2.89040375]\n",
      "Iter-12544; D_loss: [ 0.07617921]; G_loss: [ 2.88973641]\n",
      "Iter-12800; D_loss: [ 0.07131739]; G_loss: [ 2.91058278]\n",
      "Iter-13056; D_loss: [ 0.08078635]; G_loss: [ 2.92237043]\n",
      "Iter-13312; D_loss: [ 0.08001471]; G_loss: [ 2.87010384]\n",
      "Iter-13568; D_loss: [ 0.08297433]; G_loss: [ 2.85085583]\n",
      "Iter-13824; D_loss: [ 0.08614416]; G_loss: [ 2.80354643]\n",
      "Iter-14080; D_loss: [ 0.07899205]; G_loss: [ 2.77770829]\n",
      "Iter-14336; D_loss: [ 0.09328468]; G_loss: [ 2.72528672]\n",
      "Iter-14592; D_loss: [ 0.08796234]; G_loss: [ 2.75351834]\n",
      "Iter-14848; D_loss: [ 0.09271674]; G_loss: [ 2.71858597]\n",
      "Iter-15104; D_loss: [ 0.09178498]; G_loss: [ 2.68363571]\n",
      "Iter-15360; D_loss: [ 0.10163431]; G_loss: [ 2.6800487]\n",
      "Iter-15616; D_loss: [ 0.10905718]; G_loss: [ 2.64500475]\n",
      "epoch: 4\n",
      "Iter-0; D_loss: [ 0.09567662]; G_loss: [ 2.6743021]\n",
      "Iter-256; D_loss: [ 0.08561479]; G_loss: [ 2.69354868]\n",
      "Iter-512; D_loss: [ 0.097363]; G_loss: [ 2.71618819]\n",
      "Iter-768; D_loss: [ 0.08966071]; G_loss: [ 2.72307205]\n",
      "Iter-1024; D_loss: [ 0.08888269]; G_loss: [ 2.76373339]\n",
      "Iter-1280; D_loss: [ 0.08931633]; G_loss: [ 2.7948451]\n",
      "Iter-1536; D_loss: [ 0.08966982]; G_loss: [ 2.80772901]\n",
      "Iter-1792; D_loss: [ 0.09096656]; G_loss: [ 2.83368421]\n",
      "Iter-2048; D_loss: [ 0.08838056]; G_loss: [ 2.84601665]\n",
      "Iter-2304; D_loss: [ 0.08327486]; G_loss: [ 2.88804173]\n",
      "Iter-2560; D_loss: [ 0.0806838]; G_loss: [ 2.8985486]\n",
      "Iter-2816; D_loss: [ 0.07990897]; G_loss: [ 2.92693233]\n",
      "Iter-3072; D_loss: [ 0.07675107]; G_loss: [ 2.91105342]\n",
      "Iter-3328; D_loss: [ 0.07497104]; G_loss: [ 2.9107573]\n",
      "Iter-3584; D_loss: [ 0.07382742]; G_loss: [ 2.91543746]\n",
      "Iter-3840; D_loss: [ 0.06944295]; G_loss: [ 2.92151427]\n",
      "Iter-4096; D_loss: [ 0.07501567]; G_loss: [ 2.9191184]\n",
      "Iter-4352; D_loss: [ 0.07685243]; G_loss: [ 2.89864922]\n",
      "Iter-4608; D_loss: [ 0.07547439]; G_loss: [ 2.87928391]\n",
      "Iter-4864; D_loss: [ 0.079315]; G_loss: [ 2.85503459]\n",
      "Iter-5120; D_loss: [ 0.07897244]; G_loss: [ 2.8144381]\n",
      "Iter-5376; D_loss: [ 0.08004114]; G_loss: [ 2.8328979]\n",
      "Iter-5632; D_loss: [ 0.07897638]; G_loss: [ 2.80458045]\n",
      "Iter-5888; D_loss: [ 0.0809514]; G_loss: [ 2.74663329]\n",
      "Iter-6144; D_loss: [ 0.08736868]; G_loss: [ 2.7504766]\n",
      "Iter-6400; D_loss: [ 0.0930642]; G_loss: [ 2.74836731]\n",
      "Iter-6656; D_loss: [ 0.08618564]; G_loss: [ 2.70656204]\n",
      "Iter-6912; D_loss: [ 0.08519471]; G_loss: [ 2.69749451]\n",
      "Iter-7168; D_loss: [ 0.08397479]; G_loss: [ 2.69956565]\n",
      "Iter-7424; D_loss: [ 0.08829866]; G_loss: [ 2.67391777]\n",
      "Iter-7680; D_loss: [ 0.08734229]; G_loss: [ 2.67514014]\n",
      "Iter-7936; D_loss: [ 0.08866918]; G_loss: [ 2.67178774]\n",
      "Iter-8192; D_loss: [ 0.08982691]; G_loss: [ 2.68582058]\n",
      "Iter-8448; D_loss: [ 0.0969512]; G_loss: [ 2.66513753]\n",
      "Iter-8704; D_loss: [ 0.08845203]; G_loss: [ 2.681494]\n",
      "Iter-8960; D_loss: [ 0.08968619]; G_loss: [ 2.67605758]\n",
      "Iter-9216; D_loss: [ 0.0893325]; G_loss: [ 2.68686461]\n",
      "Iter-9472; D_loss: [ 0.087921]; G_loss: [ 2.70664477]\n",
      "Iter-9728; D_loss: [ 0.08351965]; G_loss: [ 2.71661997]\n",
      "Iter-9984; D_loss: [ 0.08616547]; G_loss: [ 2.72915769]\n",
      "Iter-10240; D_loss: [ 0.08219416]; G_loss: [ 2.74554682]\n",
      "Iter-10496; D_loss: [ 0.08854939]; G_loss: [ 2.75429106]\n",
      "Iter-10752; D_loss: [ 0.08452307]; G_loss: [ 2.79092598]\n",
      "Iter-11008; D_loss: [ 0.07620867]; G_loss: [ 2.80571795]\n",
      "Iter-11264; D_loss: [ 0.07655294]; G_loss: [ 2.84616899]\n",
      "Iter-11520; D_loss: [ 0.08009068]; G_loss: [ 2.86331797]\n",
      "Iter-11776; D_loss: [ 0.07700187]; G_loss: [ 2.89563537]\n",
      "Iter-12032; D_loss: [ 0.06709576]; G_loss: [ 2.93795013]\n",
      "Iter-12288; D_loss: [ 0.07643869]; G_loss: [ 2.95019293]\n",
      "Iter-12544; D_loss: [ 0.06687214]; G_loss: [ 2.98895025]\n",
      "Iter-12800; D_loss: [ 0.06202418]; G_loss: [ 3.03624797]\n",
      "Iter-13056; D_loss: [ 0.06733263]; G_loss: [ 3.06435871]\n",
      "Iter-13312; D_loss: [ 0.05922058]; G_loss: [ 3.08639622]\n",
      "Iter-13568; D_loss: [ 0.06273384]; G_loss: [ 3.12945127]\n",
      "Iter-13824; D_loss: [ 0.06369247]; G_loss: [ 3.14085126]\n",
      "Iter-14080; D_loss: [ 0.05415035]; G_loss: [ 3.18472767]\n",
      "Iter-14336; D_loss: [ 0.05528825]; G_loss: [ 3.22539902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-14592; D_loss: [ 0.05335689]; G_loss: [ 3.2454989]\n",
      "Iter-14848; D_loss: [ 0.05300501]; G_loss: [ 3.25839257]\n",
      "Iter-15104; D_loss: [ 0.05304332]; G_loss: [ 3.28762126]\n",
      "Iter-15360; D_loss: [ 0.0532539]; G_loss: [ 3.29980111]\n",
      "Iter-15616; D_loss: [ 0.05761059]; G_loss: [ 3.32443142]\n",
      "epoch: 5\n",
      "Iter-0; D_loss: [ 0.0491818]; G_loss: [ 3.32456136]\n",
      "Iter-256; D_loss: [ 0.04486523]; G_loss: [ 3.35599756]\n",
      "Iter-512; D_loss: [ 0.0482597]; G_loss: [ 3.36574006]\n",
      "Iter-768; D_loss: [ 0.0452049]; G_loss: [ 3.38436079]\n",
      "Iter-1024; D_loss: [ 0.04421973]; G_loss: [ 3.39307356]\n",
      "Iter-1280; D_loss: [ 0.04952749]; G_loss: [ 3.371135]\n",
      "Iter-1536; D_loss: [ 0.04858022]; G_loss: [ 3.36069989]\n",
      "Iter-1792; D_loss: [ 0.04785639]; G_loss: [ 3.37188411]\n",
      "Iter-2048; D_loss: [ 0.05018205]; G_loss: [ 3.38671422]\n",
      "Iter-2304; D_loss: [ 0.04817586]; G_loss: [ 3.32293534]\n",
      "Iter-2560; D_loss: [ 0.05042559]; G_loss: [ 3.36796474]\n",
      "Iter-2816; D_loss: [ 0.04835045]; G_loss: [ 3.36879516]\n",
      "Iter-3072; D_loss: [ 0.04520224]; G_loss: [ 3.38731623]\n",
      "Iter-3328; D_loss: [ 0.04597715]; G_loss: [ 3.35332298]\n",
      "Iter-3584; D_loss: [ 0.04470597]; G_loss: [ 3.3661356]\n",
      "Iter-3840; D_loss: [ 0.04352264]; G_loss: [ 3.37004209]\n",
      "Iter-4096; D_loss: [ 0.04377154]; G_loss: [ 3.38445091]\n",
      "Iter-4352; D_loss: [ 0.04416469]; G_loss: [ 3.40186596]\n",
      "Iter-4608; D_loss: [ 0.04145343]; G_loss: [ 3.43014145]\n",
      "Iter-4864; D_loss: [ 0.04271313]; G_loss: [ 3.42876029]\n",
      "Iter-5120; D_loss: [ 0.0410904]; G_loss: [ 3.47001219]\n",
      "Iter-5376; D_loss: [ 0.04007046]; G_loss: [ 3.49667382]\n",
      "Iter-5632; D_loss: [ 0.03763375]; G_loss: [ 3.52723742]\n",
      "Iter-5888; D_loss: [ 0.04103813]; G_loss: [ 3.52711391]\n",
      "Iter-6144; D_loss: [ 0.04042181]; G_loss: [ 3.5592432]\n",
      "Iter-6400; D_loss: [ 0.03859194]; G_loss: [ 3.5864799]\n",
      "Iter-6656; D_loss: [ 0.03699261]; G_loss: [ 3.60324717]\n",
      "Iter-6912; D_loss: [ 0.03463227]; G_loss: [ 3.61989403]\n",
      "Iter-7168; D_loss: [ 0.03538787]; G_loss: [ 3.65169597]\n",
      "Iter-7424; D_loss: [ 0.03517516]; G_loss: [ 3.67011952]\n",
      "Iter-7680; D_loss: [ 0.0335122]; G_loss: [ 3.67542171]\n",
      "Iter-7936; D_loss: [ 0.0335901]; G_loss: [ 3.7311151]\n",
      "Iter-8192; D_loss: [ 0.034504]; G_loss: [ 3.76425862]\n",
      "Iter-8448; D_loss: [ 0.03378043]; G_loss: [ 3.77728367]\n",
      "Iter-8704; D_loss: [ 0.03434897]; G_loss: [ 3.83022547]\n",
      "Iter-8960; D_loss: [ 0.0323858]; G_loss: [ 3.84560847]\n",
      "Iter-9216; D_loss: [ 0.03103608]; G_loss: [ 3.88191581]\n",
      "Iter-9472; D_loss: [ 0.02972468]; G_loss: [ 3.92883992]\n",
      "Iter-9728; D_loss: [ 0.02760419]; G_loss: [ 3.95945692]\n",
      "Iter-9984; D_loss: [ 0.0286822]; G_loss: [ 3.98499632]\n",
      "Iter-10240; D_loss: [ 0.02832395]; G_loss: [ 3.99561357]\n",
      "Iter-10496; D_loss: [ 0.03062997]; G_loss: [ 4.02460194]\n",
      "Iter-10752; D_loss: [ 0.0268314]; G_loss: [ 4.01217413]\n",
      "Iter-11008; D_loss: [ 0.02470341]; G_loss: [ 4.0124383]\n",
      "Iter-11264; D_loss: [ 0.02668206]; G_loss: [ 4.00479794]\n",
      "Iter-11520; D_loss: [ 0.02892311]; G_loss: [ 3.99371386]\n",
      "Iter-11776; D_loss: [ 0.02861622]; G_loss: [ 3.99937105]\n",
      "Iter-12032; D_loss: [ 0.02654385]; G_loss: [ 3.96404791]\n",
      "Iter-12288; D_loss: [ 0.02880394]; G_loss: [ 3.93582559]\n",
      "Iter-12544; D_loss: [ 0.02981014]; G_loss: [ 3.91069293]\n",
      "Iter-12800; D_loss: [ 0.02748262]; G_loss: [ 3.90023804]\n",
      "Iter-13056; D_loss: [ 0.03211371]; G_loss: [ 3.88018227]\n",
      "Iter-13312; D_loss: [ 0.02946714]; G_loss: [ 3.80883503]\n",
      "Iter-13568; D_loss: [ 0.0325187]; G_loss: [ 3.84964108]\n",
      "Iter-13824; D_loss: [ 0.03034065]; G_loss: [ 3.81982708]\n",
      "Iter-14080; D_loss: [ 0.03038284]; G_loss: [ 3.83728218]\n",
      "Iter-14336; D_loss: [ 0.03031324]; G_loss: [ 3.83985496]\n",
      "Iter-14592; D_loss: [ 0.02711581]; G_loss: [ 3.85674691]\n",
      "Iter-14848; D_loss: [ 0.03161029]; G_loss: [ 3.85185051]\n",
      "Iter-15104; D_loss: [ 0.03319923]; G_loss: [ 3.88293791]\n",
      "Iter-15360; D_loss: [ 0.02811829]; G_loss: [ 3.89009809]\n",
      "Iter-15616; D_loss: [ 0.03125644]; G_loss: [ 3.93183875]\n",
      "epoch: 6\n",
      "Iter-0; D_loss: [ 0.03083747]; G_loss: [ 3.95234919]\n",
      "Iter-256; D_loss: [ 0.02462035]; G_loss: [ 3.98897266]\n",
      "Iter-512; D_loss: [ 0.02735547]; G_loss: [ 4.02749538]\n",
      "Iter-768; D_loss: [ 0.02385475]; G_loss: [ 4.080832]\n",
      "Iter-1024; D_loss: [ 0.02349612]; G_loss: [ 4.12051058]\n",
      "Iter-1280; D_loss: [ 0.02600185]; G_loss: [ 4.14729977]\n",
      "Iter-1536; D_loss: [ 0.02582516]; G_loss: [ 4.17404604]\n",
      "Iter-1792; D_loss: [ 0.0235185]; G_loss: [ 4.22980642]\n",
      "Iter-2048; D_loss: [ 0.02396837]; G_loss: [ 4.27747536]\n",
      "Iter-2304; D_loss: [ 0.02139471]; G_loss: [ 4.31665468]\n",
      "Iter-2560; D_loss: [ 0.02167656]; G_loss: [ 4.35671616]\n",
      "Iter-2816; D_loss: [ 0.0218409]; G_loss: [ 4.39818621]\n",
      "Iter-3072; D_loss: [ 0.01897243]; G_loss: [ 4.4211092]\n",
      "Iter-3328; D_loss: [ 0.01959511]; G_loss: [ 4.42529154]\n",
      "Iter-3584; D_loss: [ 0.01852095]; G_loss: [ 4.43143845]\n",
      "Iter-3840; D_loss: [ 0.01722178]; G_loss: [ 4.44035292]\n",
      "Iter-4096; D_loss: [ 0.01893079]; G_loss: [ 4.40293646]\n",
      "Iter-4352; D_loss: [ 0.01862915]; G_loss: [ 4.40660286]\n",
      "Iter-4608; D_loss: [ 0.0194675]; G_loss: [ 4.37764311]\n",
      "Iter-4864; D_loss: [ 0.01941621]; G_loss: [ 4.37510061]\n",
      "Iter-5120; D_loss: [ 0.01918393]; G_loss: [ 4.34530878]\n",
      "Iter-5376; D_loss: [ 0.01837633]; G_loss: [ 4.31942749]\n",
      "Iter-5632; D_loss: [ 0.01900668]; G_loss: [ 4.33400631]\n",
      "Iter-5888; D_loss: [ 0.02403642]; G_loss: [ 4.28802824]\n",
      "Iter-6144; D_loss: [ 0.02419894]; G_loss: [ 4.26392031]\n",
      "Iter-6400; D_loss: [ 0.02134432]; G_loss: [ 4.26128006]\n",
      "Iter-6656; D_loss: [ 0.02037293]; G_loss: [ 4.22250128]\n",
      "Iter-6912; D_loss: [ 0.02123392]; G_loss: [ 4.22473478]\n",
      "Iter-7168; D_loss: [ 0.02068492]; G_loss: [ 4.23620319]\n",
      "Iter-7424; D_loss: [ 0.02081139]; G_loss: [ 4.19095993]\n",
      "Iter-7680; D_loss: [ 0.02359192]; G_loss: [ 4.17932653]\n",
      "Iter-7936; D_loss: [ 0.02378179]; G_loss: [ 4.17810154]\n",
      "Iter-8192; D_loss: [ 0.02416442]; G_loss: [ 4.14851856]\n",
      "Iter-8448; D_loss: [ 0.02284825]; G_loss: [ 4.15118361]\n",
      "Iter-8704; D_loss: [ 0.02450967]; G_loss: [ 4.13708735]\n",
      "Iter-8960; D_loss: [ 0.02315873]; G_loss: [ 4.11190653]\n",
      "Iter-9216; D_loss: [ 0.02307252]; G_loss: [ 4.10616255]\n",
      "Iter-9472; D_loss: [ 0.02209876]; G_loss: [ 4.12227249]\n",
      "Iter-9728; D_loss: [ 0.02355218]; G_loss: [ 4.12785482]\n",
      "Iter-9984; D_loss: [ 0.02276039]; G_loss: [ 4.11852837]\n",
      "Iter-10240; D_loss: [ 0.02343105]; G_loss: [ 4.13224506]\n",
      "Iter-10496; D_loss: [ 0.02375829]; G_loss: [ 4.12172699]\n",
      "Iter-10752; D_loss: [ 0.02003518]; G_loss: [ 4.15564156]\n",
      "Iter-11008; D_loss: [ 0.0214023]; G_loss: [ 4.16840935]\n",
      "Iter-11264; D_loss: [ 0.02225737]; G_loss: [ 4.18699837]\n",
      "Iter-11520; D_loss: [ 0.02203026]; G_loss: [ 4.20047331]\n",
      "Iter-11776; D_loss: [ 0.02193228]; G_loss: [ 4.22588539]\n",
      "Iter-12032; D_loss: [ 0.01939199]; G_loss: [ 4.2762475]\n",
      "Iter-12288; D_loss: [ 0.02022369]; G_loss: [ 4.27436924]\n",
      "Iter-12544; D_loss: [ 0.02080058]; G_loss: [ 4.32144451]\n",
      "Iter-12800; D_loss: [ 0.01787889]; G_loss: [ 4.37268448]\n",
      "Iter-13056; D_loss: [ 0.01870096]; G_loss: [ 4.40399551]\n",
      "Iter-13312; D_loss: [ 0.01859339]; G_loss: [ 4.4246912]\n",
      "Iter-13568; D_loss: [ 0.02159712]; G_loss: [ 4.48352337]\n",
      "Iter-13824; D_loss: [ 0.01757362]; G_loss: [ 4.49221134]\n",
      "Iter-14080; D_loss: [ 0.01826723]; G_loss: [ 4.54690218]\n",
      "Iter-14336; D_loss: [ 0.0157934]; G_loss: [ 4.59032297]\n",
      "Iter-14592; D_loss: [ 0.01412045]; G_loss: [ 4.61313295]\n",
      "Iter-14848; D_loss: [ 0.01616883]; G_loss: [ 4.62194061]\n",
      "Iter-15104; D_loss: [ 0.01626275]; G_loss: [ 4.65183735]\n",
      "Iter-15360; D_loss: [ 0.0129861]; G_loss: [ 4.66673803]\n",
      "Iter-15616; D_loss: [ 0.01604658]; G_loss: [ 4.70295382]\n",
      "epoch: 7\n",
      "Iter-0; D_loss: [ 0.01462498]; G_loss: [ 4.72018433]\n",
      "Iter-256; D_loss: [ 0.01313051]; G_loss: [ 4.74014664]\n",
      "Iter-512; D_loss: [ 0.01396298]; G_loss: [ 4.76405001]\n",
      "Iter-768; D_loss: [ 0.01213131]; G_loss: [ 4.78392363]\n",
      "Iter-1024; D_loss: [ 0.01245786]; G_loss: [ 4.80331802]\n",
      "Iter-1280; D_loss: [ 0.01356211]; G_loss: [ 4.81000805]\n",
      "Iter-1536; D_loss: [ 0.01578131]; G_loss: [ 4.80225182]\n",
      "Iter-1792; D_loss: [ 0.01255858]; G_loss: [ 4.83270884]\n",
      "Iter-2048; D_loss: [ 0.01358122]; G_loss: [ 4.86534929]\n",
      "Iter-2304; D_loss: [ 0.01132107]; G_loss: [ 4.87160158]\n",
      "Iter-2560; D_loss: [ 0.01267717]; G_loss: [ 4.91252327]\n",
      "Iter-2816; D_loss: [ 0.01240411]; G_loss: [ 4.92145681]\n",
      "Iter-3072; D_loss: [ 0.01063506]; G_loss: [ 4.95620537]\n",
      "Iter-3328; D_loss: [ 0.01127159]; G_loss: [ 4.93068266]\n",
      "Iter-3584; D_loss: [ 0.01179965]; G_loss: [ 4.94260406]\n",
      "Iter-3840; D_loss: [ 0.00984585]; G_loss: [ 4.94950914]\n",
      "Iter-4096; D_loss: [ 0.01114583]; G_loss: [ 4.96634531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4352; D_loss: [ 0.01137492]; G_loss: [ 4.94225073]\n",
      "Iter-4608; D_loss: [ 0.01077524]; G_loss: [ 4.94183207]\n",
      "Iter-4864; D_loss: [ 0.01111945]; G_loss: [ 4.91877127]\n",
      "Iter-5120; D_loss: [ 0.01219908]; G_loss: [ 4.89935398]\n",
      "Iter-5376; D_loss: [ 0.01073392]; G_loss: [ 4.90966606]\n",
      "Iter-5632; D_loss: [ 0.010571]; G_loss: [ 4.95287848]\n",
      "Iter-5888; D_loss: [ 0.01254489]; G_loss: [ 4.9075799]\n",
      "Iter-6144; D_loss: [ 0.0116777]; G_loss: [ 4.95284081]\n",
      "Iter-6400; D_loss: [ 0.010719]; G_loss: [ 4.97396708]\n",
      "Iter-6656; D_loss: [ 0.01208604]; G_loss: [ 4.98453903]\n",
      "Iter-6912; D_loss: [ 0.01193361]; G_loss: [ 4.984303]\n",
      "Iter-7168; D_loss: [ 0.01029903]; G_loss: [ 5.00475788]\n",
      "Iter-7424; D_loss: [ 0.01213145]; G_loss: [ 4.99706984]\n",
      "Iter-7680; D_loss: [ 0.01116285]; G_loss: [ 4.98260355]\n",
      "Iter-7936; D_loss: [ 0.01092865]; G_loss: [ 4.99696732]\n",
      "Iter-8192; D_loss: [ 0.01380358]; G_loss: [ 5.02067232]\n",
      "Iter-8448; D_loss: [ 0.01204208]; G_loss: [ 4.97761536]\n",
      "Iter-8704; D_loss: [ 0.01251659]; G_loss: [ 4.93849421]\n",
      "Iter-8960; D_loss: [ 0.01268564]; G_loss: [ 4.92953825]\n",
      "Iter-9216; D_loss: [ 0.01222157]; G_loss: [ 4.96020317]\n",
      "Iter-9472; D_loss: [ 0.01265877]; G_loss: [ 4.9697938]\n",
      "Iter-9728; D_loss: [ 0.01275214]; G_loss: [ 4.98741436]\n",
      "Iter-9984; D_loss: [ 0.01150533]; G_loss: [ 4.98725986]\n",
      "Iter-10240; D_loss: [ 0.01241974]; G_loss: [ 5.01250172]\n",
      "Iter-10496; D_loss: [ 0.01068302]; G_loss: [ 5.03468037]\n",
      "Iter-10752; D_loss: [ 0.00943729]; G_loss: [ 5.11792946]\n",
      "Iter-11008; D_loss: [ 0.00883526]; G_loss: [ 5.14771032]\n",
      "Iter-11264; D_loss: [ 0.00875119]; G_loss: [ 5.22207212]\n",
      "Iter-11520; D_loss: [ 0.0106476]; G_loss: [ 5.26326895]\n",
      "Iter-11776; D_loss: [ 0.00838012]; G_loss: [ 5.31943893]\n",
      "Iter-12032; D_loss: [ 0.00789117]; G_loss: [ 5.38864422]\n",
      "Iter-12288; D_loss: [ 0.00747592]; G_loss: [ 5.41575623]\n",
      "Iter-12544; D_loss: [ 0.01043471]; G_loss: [ 5.46131134]\n",
      "Iter-12800; D_loss: [ 0.00728518]; G_loss: [ 5.53368807]\n",
      "Iter-13056; D_loss: [ 0.00881749]; G_loss: [ 5.55329466]\n",
      "Iter-13312; D_loss: [ 0.0084037]; G_loss: [ 5.56420422]\n",
      "Iter-13568; D_loss: [ 0.01082763]; G_loss: [ 5.60733938]\n",
      "Iter-13824; D_loss: [ 0.00642883]; G_loss: [ 5.59171152]\n",
      "Iter-14080; D_loss: [ 0.00706008]; G_loss: [ 5.61562014]\n",
      "Iter-14336; D_loss: [ 0.00724128]; G_loss: [ 5.63207674]\n",
      "Iter-14592; D_loss: [ 0.00617153]; G_loss: [ 5.63563776]\n",
      "Iter-14848; D_loss: [ 0.00947723]; G_loss: [ 5.59468651]\n",
      "Iter-15104; D_loss: [ 0.01027821]; G_loss: [ 5.62562799]\n",
      "Iter-15360; D_loss: [ 0.00665]; G_loss: [ 5.60552931]\n",
      "Iter-15616; D_loss: [ 0.00727975]; G_loss: [ 5.63046026]\n",
      "epoch: 8\n",
      "Iter-0; D_loss: [ 0.00792506]; G_loss: [ 5.603971]\n",
      "Iter-256; D_loss: [ 0.00739627]; G_loss: [ 5.62189627]\n",
      "Iter-512; D_loss: [ 0.00949863]; G_loss: [ 5.58602905]\n",
      "Iter-768; D_loss: [ 0.00710182]; G_loss: [ 5.54804659]\n",
      "Iter-1024; D_loss: [ 0.00667002]; G_loss: [ 5.52590513]\n",
      "Iter-1280; D_loss: [ 0.00813312]; G_loss: [ 5.49301672]\n",
      "Iter-1536; D_loss: [ 0.00971179]; G_loss: [ 5.42039204]\n",
      "Iter-1792; D_loss: [ 0.0087025]; G_loss: [ 5.41772556]\n",
      "Iter-2048; D_loss: [ 0.00928539]; G_loss: [ 5.38779736]\n",
      "Iter-2304; D_loss: [ 0.00797696]; G_loss: [ 5.36827326]\n",
      "Iter-2560; D_loss: [ 0.00894951]; G_loss: [ 5.3731246]\n",
      "Iter-2816; D_loss: [ 0.00776183]; G_loss: [ 5.41753626]\n",
      "Iter-3072; D_loss: [ 0.00766151]; G_loss: [ 5.49176884]\n",
      "Iter-3328; D_loss: [ 0.00759628]; G_loss: [ 5.48823738]\n",
      "Iter-3584; D_loss: [ 0.00704034]; G_loss: [ 5.57075977]\n",
      "Iter-3840; D_loss: [ 0.00541095]; G_loss: [ 5.62367487]\n",
      "Iter-4096; D_loss: [ 0.00605084]; G_loss: [ 5.71207714]\n",
      "Iter-4352; D_loss: [ 0.00664627]; G_loss: [ 5.76379204]\n",
      "Iter-4608; D_loss: [ 0.00541602]; G_loss: [ 5.86277485]\n",
      "Iter-4864; D_loss: [ 0.00592522]; G_loss: [ 5.91525793]\n",
      "Iter-5120; D_loss: [ 0.00492402]; G_loss: [ 5.96393681]\n",
      "Iter-5376; D_loss: [ 0.00515126]; G_loss: [ 6.04287863]\n",
      "Iter-5632; D_loss: [ 0.00525393]; G_loss: [ 6.11212015]\n",
      "Iter-5888; D_loss: [ 0.00623228]; G_loss: [ 6.11421967]\n",
      "Iter-6144; D_loss: [ 0.00615594]; G_loss: [ 6.15151978]\n",
      "Iter-6400; D_loss: [ 0.00414567]; G_loss: [ 6.17087364]\n",
      "Iter-6656; D_loss: [ 0.00509404]; G_loss: [ 6.16938972]\n",
      "Iter-6912; D_loss: [ 0.00467288]; G_loss: [ 6.18257332]\n",
      "Iter-7168; D_loss: [ 0.00408187]; G_loss: [ 6.20861101]\n",
      "Iter-7424; D_loss: [ 0.00486955]; G_loss: [ 6.17247248]\n",
      "Iter-7680; D_loss: [ 0.00428583]; G_loss: [ 6.14930868]\n",
      "Iter-7936; D_loss: [ 0.00687989]; G_loss: [ 6.15269804]\n",
      "Iter-8192; D_loss: [ 0.00578758]; G_loss: [ 6.12325001]\n",
      "Iter-8448; D_loss: [ 0.00531978]; G_loss: [ 6.12114048]\n",
      "Iter-8704; D_loss: [ 0.00532981]; G_loss: [ 6.12625408]\n",
      "Iter-8960; D_loss: [ 0.00670346]; G_loss: [ 6.11929178]\n",
      "Iter-9216; D_loss: [ 0.00583753]; G_loss: [ 6.15750122]\n",
      "Iter-9472; D_loss: [ 0.00478329]; G_loss: [ 6.19067526]\n",
      "Iter-9728; D_loss: [ 0.00432261]; G_loss: [ 6.22261047]\n",
      "Iter-9984; D_loss: [ 0.00517514]; G_loss: [ 6.25791073]\n",
      "Iter-10240; D_loss: [ 0.00526302]; G_loss: [ 6.32232046]\n",
      "Iter-10496; D_loss: [ 0.00384492]; G_loss: [ 6.34206724]\n",
      "Iter-10752; D_loss: [ 0.00323793]; G_loss: [ 6.42173624]\n",
      "Iter-11008; D_loss: [ 0.00339689]; G_loss: [ 6.47591305]\n",
      "Iter-11264; D_loss: [ 0.00406519]; G_loss: [ 6.54941082]\n",
      "Iter-11520; D_loss: [ 0.00305552]; G_loss: [ 6.58613729]\n",
      "Iter-11776; D_loss: [ 0.00291375]; G_loss: [ 6.64140749]\n",
      "Iter-12032; D_loss: [ 0.00332674]; G_loss: [ 6.70269156]\n",
      "Iter-12288; D_loss: [ 0.00260436]; G_loss: [ 6.72371483]\n",
      "Iter-12544; D_loss: [ 0.00461163]; G_loss: [ 6.7755394]\n",
      "Iter-12800; D_loss: [ 0.00290264]; G_loss: [ 6.84011793]\n",
      "Iter-13056; D_loss: [ 0.00415548]; G_loss: [ 6.85426712]\n",
      "Iter-13312; D_loss: [ 0.00241369]; G_loss: [ 6.87401724]\n",
      "Iter-13568; D_loss: [ 0.00438598]; G_loss: [ 6.93284655]\n",
      "Iter-13824; D_loss: [ 0.00247609]; G_loss: [ 6.91114998]\n",
      "Iter-14080; D_loss: [ 0.00299372]; G_loss: [ 6.93796682]\n",
      "Iter-14336; D_loss: [ 0.00250033]; G_loss: [ 6.96692181]\n",
      "Iter-14592; D_loss: [ 0.00279321]; G_loss: [ 6.97585058]\n",
      "Iter-14848; D_loss: [ 0.00413588]; G_loss: [ 6.94346189]\n",
      "Iter-15104; D_loss: [ 0.00333585]; G_loss: [ 6.94266558]\n",
      "Iter-15360; D_loss: [ 0.00240157]; G_loss: [ 6.90853548]\n",
      "Iter-15616; D_loss: [ 0.00288256]; G_loss: [ 6.8860836]\n",
      "epoch: 9\n",
      "Iter-0; D_loss: [ 0.00380685]; G_loss: [ 6.91751051]\n",
      "Iter-256; D_loss: [ 0.0026948]; G_loss: [ 6.8683157]\n",
      "Iter-512; D_loss: [ 0.00314393]; G_loss: [ 6.837255]\n",
      "Iter-768; D_loss: [ 0.00361684]; G_loss: [ 6.80323458]\n",
      "Iter-1024; D_loss: [ 0.00278834]; G_loss: [ 6.73753309]\n",
      "Iter-1280; D_loss: [ 0.00358431]; G_loss: [ 6.64173412]\n",
      "Iter-1536; D_loss: [ 0.00355796]; G_loss: [ 6.53450632]\n",
      "Iter-1792; D_loss: [ 0.00324799]; G_loss: [ 6.43963957]\n",
      "Iter-2048; D_loss: [ 0.00434315]; G_loss: [ 6.41640568]\n",
      "Iter-2304; D_loss: [ 0.00385089]; G_loss: [ 6.20256948]\n",
      "Iter-2560; D_loss: [ 0.00540169]; G_loss: [ 6.17308426]\n",
      "Iter-2816; D_loss: [ 0.00406725]; G_loss: [ 6.08041668]\n",
      "Iter-3072; D_loss: [ 0.00416927]; G_loss: [ 6.04907656]\n",
      "Iter-3328; D_loss: [ 0.00466523]; G_loss: [ 5.95652628]\n",
      "Iter-3584; D_loss: [ 0.00454781]; G_loss: [ 5.93182945]\n",
      "Iter-3840; D_loss: [ 0.00386232]; G_loss: [ 5.93404627]\n",
      "Iter-4096; D_loss: [ 0.00424488]; G_loss: [ 6.00595713]\n",
      "Iter-4352; D_loss: [ 0.00483348]; G_loss: [ 5.99516249]\n",
      "Iter-4608; D_loss: [ 0.00376207]; G_loss: [ 6.12746811]\n",
      "Iter-4864; D_loss: [ 0.00395301]; G_loss: [ 6.17298365]\n",
      "Iter-5120; D_loss: [ 0.00404029]; G_loss: [ 6.22414064]\n",
      "Iter-5376; D_loss: [ 0.00374823]; G_loss: [ 6.31949902]\n",
      "Iter-5632; D_loss: [ 0.00341164]; G_loss: [ 6.43580723]\n",
      "Iter-5888; D_loss: [ 0.00352525]; G_loss: [ 6.46686888]\n",
      "Iter-6144; D_loss: [ 0.00372641]; G_loss: [ 6.55835199]\n",
      "Iter-6400; D_loss: [ 0.0027332]; G_loss: [ 6.63580322]\n",
      "Iter-6656; D_loss: [ 0.00311712]; G_loss: [ 6.68730164]\n",
      "Iter-6912; D_loss: [ 0.00276653]; G_loss: [ 6.76504469]\n",
      "Iter-7168; D_loss: [ 0.00229859]; G_loss: [ 6.86378574]\n",
      "Iter-7424; D_loss: [ 0.00264087]; G_loss: [ 6.90740395]\n",
      "Iter-7680; D_loss: [ 0.00242676]; G_loss: [ 6.95848131]\n",
      "Iter-7936; D_loss: [ 0.00314469]; G_loss: [ 7.04277992]\n",
      "Iter-8192; D_loss: [ 0.00250204]; G_loss: [ 7.09857655]\n",
      "Iter-8448; D_loss: [ 0.00294344]; G_loss: [ 7.12946272]\n",
      "Iter-8704; D_loss: [ 0.00272223]; G_loss: [ 7.17477512]\n",
      "Iter-8960; D_loss: [ 0.00291311]; G_loss: [ 7.20555496]\n",
      "Iter-9216; D_loss: [ 0.00302268]; G_loss: [ 7.2415204]\n",
      "Iter-9472; D_loss: [ 0.00237161]; G_loss: [ 7.27618361]\n",
      "Iter-9728; D_loss: [ 0.00250163]; G_loss: [ 7.30137491]\n",
      "Iter-9984; D_loss: [ 0.00288257]; G_loss: [ 7.31253338]\n",
      "Iter-10240; D_loss: [ 0.00324326]; G_loss: [ 7.33224726]\n",
      "Iter-10496; D_loss: [ 0.00214012]; G_loss: [ 7.32468796]\n",
      "Iter-10752; D_loss: [ 0.00177934]; G_loss: [ 7.35067844]\n",
      "Iter-11008; D_loss: [ 0.00220703]; G_loss: [ 7.37855101]\n",
      "Iter-11264; D_loss: [ 0.0019836]; G_loss: [ 7.3985157]\n",
      "Iter-11520; D_loss: [ 0.00198916]; G_loss: [ 7.41196394]\n",
      "Iter-11776; D_loss: [ 0.00182416]; G_loss: [ 7.43706989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12032; D_loss: [ 0.0023589]; G_loss: [ 7.46758747]\n",
      "Iter-12288; D_loss: [ 0.00142409]; G_loss: [ 7.4667263]\n",
      "Iter-12544; D_loss: [ 0.00280147]; G_loss: [ 7.49060297]\n",
      "Iter-12800; D_loss: [ 0.00162919]; G_loss: [ 7.49815941]\n",
      "Iter-13056; D_loss: [ 0.00247647]; G_loss: [ 7.51520157]\n",
      "Iter-13312; D_loss: [ 0.00143332]; G_loss: [ 7.47885609]\n",
      "Iter-13568; D_loss: [ 0.00239291]; G_loss: [ 7.49562883]\n",
      "Iter-13824; D_loss: [ 0.00173712]; G_loss: [ 7.43355989]\n",
      "Iter-14080; D_loss: [ 0.00240493]; G_loss: [ 7.42631531]\n",
      "Iter-14336; D_loss: [ 0.0016101]; G_loss: [ 7.39134359]\n",
      "Iter-14592; D_loss: [ 0.00169592]; G_loss: [ 7.32188892]\n",
      "Iter-14848; D_loss: [ 0.00240667]; G_loss: [ 7.25284767]\n",
      "Iter-15104; D_loss: [ 0.00237612]; G_loss: [ 7.17829227]\n",
      "Iter-15360; D_loss: [ 0.00174953]; G_loss: [ 7.14638662]\n",
      "Iter-15616; D_loss: [ 0.00262559]; G_loss: [ 7.07986069]\n"
     ]
    }
   ],
   "source": [
    "batch_size = mb_size\n",
    "# Start training\n",
    "for epoch in range(10):\n",
    "    \n",
    "    \n",
    "\n",
    "    print('epoch:',epoch)\n",
    "    #for i in range(XX_train):\n",
    "    # Build mini-batch dataset\n",
    "    #batch_size = images.size(0)\n",
    "    #images = to_var(images.view(batch_size, -1))\n",
    "\n",
    "    it=0\n",
    "    while it+batch_size < len(X_train) :\n",
    "        \n",
    "\n",
    "        start= it\n",
    "        end= it + batch_size\n",
    "\n",
    "\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        X = X_train[start:end]\n",
    "\n",
    "        c = Y_train[start:end]\n",
    "        X = Variable(torch.from_numpy(X))\n",
    "        c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "\n",
    "        # Dicriminator forward-loss-backward-update\n",
    "        G_sample = G(z, c)\n",
    "        D_real = D(X, c)\n",
    "        D_fake = D(G_sample, c)\n",
    "\n",
    "        D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "        D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        reset_grad()\n",
    "\n",
    "        # Generator forward-loss-backward-update\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        G_sample = G(z, c)\n",
    "        D_fake = D(G_sample, c)\n",
    "\n",
    "        G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        reset_grad()\n",
    "\n",
    "        #Print and plot every now and then\n",
    "        #if it % 2 == 0:\n",
    "\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "        it+= batch_size\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF=pd.DataFrame()\n",
    "samples_per_class = 1000\n",
    "#c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "#c[:, np.random.randint(0, 10)] = 1.\n",
    "for i in range(14):\n",
    "    #print(i)\n",
    "    c = np.zeros(shape=[samples_per_class, y_dim], dtype='float32')\n",
    "    c[:, i] = 1.\n",
    "    c_df=pd.DataFrame(c)\n",
    "    df_SF = df_SF.append(c_df,ignore_index = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = df_SF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen = Variable(torch.randn(df_SF.shape[0], Z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14000, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_gen.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gen = Variable(torch.from_numpy(c_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = G(z_gen, c_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14000, 500])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the generated iVectors we will try to check the acc by MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = samples.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1 = c_gen.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "Y_train = pd.DataFrame(Y_train)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = pd.DataFrame(train_X1)\n",
    "train_y1 = pd.DataFrame(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train.append(train_X1, ignore_index=True)\n",
    "train_y = Y_train.append(train_y1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X,  train_y = shuffle(train_X, train_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.values\n",
    "train_y = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               256512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                7182      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 526,350\n",
      "Trainable params: 526,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_filepath = '/home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5'\n",
    "saveBestModel = ModelCheckpoint(best_weights_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epoch=30"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Checking Baseline Accuracy with only training data\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Baseline ERROR %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train_X1 and train_y1 are the augmented data alone to check accuracy only on augmented data \n",
    "#feed the model.fit only with these\n",
    "train_X1 = train_X1.values\n",
    "train_y1 = train_y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3974 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_acc improved from -inf to 0.05611, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6537 - acc: 0.0699 - val_loss: 2.8248 - val_acc: 0.0561\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_acc improved from 0.05611 to 0.06241, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6419 - acc: 0.0701 - val_loss: 2.7979 - val_acc: 0.0624\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_acc did not improve\n",
      "1s - loss: 2.6405 - acc: 0.0715 - val_loss: 2.8027 - val_acc: 0.0579\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_acc did not improve\n",
      "1s - loss: 2.6396 - acc: 0.0751 - val_loss: 2.8220 - val_acc: 0.0544\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_acc did not improve\n",
      "1s - loss: 2.6399 - acc: 0.0707 - val_loss: 2.8020 - val_acc: 0.0554\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_acc did not improve\n",
      "1s - loss: 2.6395 - acc: 0.0721 - val_loss: 2.7953 - val_acc: 0.0591\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_acc improved from 0.06241 to 0.06417, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6393 - acc: 0.0709 - val_loss: 2.7839 - val_acc: 0.0642\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_acc improved from 0.06417 to 0.06517, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6394 - acc: 0.0701 - val_loss: 2.7893 - val_acc: 0.0652\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_acc did not improve\n",
      "1s - loss: 2.6394 - acc: 0.0698 - val_loss: 2.8052 - val_acc: 0.0632\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_acc did not improve\n",
      "1s - loss: 2.6391 - acc: 0.0719 - val_loss: 2.8024 - val_acc: 0.0627\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_acc did not improve\n",
      "1s - loss: 2.6393 - acc: 0.0728 - val_loss: 2.8029 - val_acc: 0.0611\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_acc did not improve\n",
      "1s - loss: 2.6391 - acc: 0.0756 - val_loss: 2.8211 - val_acc: 0.0619\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_acc did not improve\n",
      "1s - loss: 2.6394 - acc: 0.0719 - val_loss: 2.8061 - val_acc: 0.0609\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_acc did not improve\n",
      "1s - loss: 2.6394 - acc: 0.0670 - val_loss: 2.8033 - val_acc: 0.0647\n",
      "Epoch 15/30\n",
      "Epoch 00014: val_acc improved from 0.06517 to 0.06543, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6393 - acc: 0.0704 - val_loss: 2.7977 - val_acc: 0.0654\n",
      "Epoch 16/30\n",
      "Epoch 00015: val_acc improved from 0.06543 to 0.06593, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6393 - acc: 0.0689 - val_loss: 2.8080 - val_acc: 0.0659\n",
      "Epoch 17/30\n",
      "Epoch 00016: val_acc did not improve\n",
      "1s - loss: 2.6392 - acc: 0.0688 - val_loss: 2.8211 - val_acc: 0.0617\n",
      "Epoch 18/30\n",
      "Epoch 00017: val_acc did not improve\n",
      "1s - loss: 2.6390 - acc: 0.0721 - val_loss: 2.8260 - val_acc: 0.0652\n",
      "Epoch 19/30\n",
      "Epoch 00018: val_acc did not improve\n",
      "1s - loss: 2.6393 - acc: 0.0686 - val_loss: 2.8416 - val_acc: 0.0639\n",
      "Epoch 20/30\n",
      "Epoch 00019: val_acc did not improve\n",
      "1s - loss: 2.6392 - acc: 0.0724 - val_loss: 2.8462 - val_acc: 0.0652\n",
      "Epoch 21/30\n",
      "Epoch 00020: val_acc did not improve\n",
      "1s - loss: 2.6391 - acc: 0.0721 - val_loss: 2.8605 - val_acc: 0.0614\n",
      "Epoch 22/30\n",
      "Epoch 00021: val_acc did not improve\n",
      "1s - loss: 2.6390 - acc: 0.0766 - val_loss: 2.8786 - val_acc: 0.0652\n",
      "Epoch 23/30\n",
      "Epoch 00022: val_acc improved from 0.06593 to 0.06719, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6390 - acc: 0.0723 - val_loss: 2.8875 - val_acc: 0.0672\n",
      "Epoch 24/30\n",
      "Epoch 00023: val_acc improved from 0.06719 to 0.06744, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6390 - acc: 0.0718 - val_loss: 2.9157 - val_acc: 0.0674\n",
      "Epoch 25/30\n",
      "Epoch 00024: val_acc improved from 0.06744 to 0.07952, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6391 - acc: 0.0694 - val_loss: 2.9346 - val_acc: 0.0795\n",
      "Epoch 26/30\n",
      "Epoch 00025: val_acc improved from 0.07952 to 0.08002, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6391 - acc: 0.0673 - val_loss: 2.9335 - val_acc: 0.0800\n",
      "Epoch 27/30\n",
      "Epoch 00026: val_acc improved from 0.08002 to 0.08379, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6390 - acc: 0.0734 - val_loss: 2.9560 - val_acc: 0.0838\n",
      "Epoch 28/30\n",
      "Epoch 00027: val_acc did not improve\n",
      "1s - loss: 2.6390 - acc: 0.0755 - val_loss: 2.9765 - val_acc: 0.0795\n",
      "Epoch 29/30\n",
      "Epoch 00028: val_acc improved from 0.08379 to 0.08631, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6389 - acc: 0.0699 - val_loss: 3.0218 - val_acc: 0.0863\n",
      "Epoch 30/30\n",
      "Epoch 00029: val_acc improved from 0.08631 to 0.09185, saving model to /home/satishk/saved_weights/best_weights_2l_MLP_11.hdf5\n",
      "1s - loss: 2.6385 - acc: 0.0709 - val_loss: 3.0982 - val_acc: 0.0918\n"
     ]
    }
   ],
   "source": [
    "#Checking Accuracy with training+augmented data train_X and train_y are 'train + augmented' data\n",
    "history = model.fit(train_X, train_y, batch_size=batch_size, epochs=nb_epoch,verbose=2, \n",
    "                    validation_data=(X_test , y_test),callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR after Data Augmentation %: 0.908152994464\n"
     ]
    }
   ],
   "source": [
    "#Frame label accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('ERROR after Data Augmentation %:', 1-score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
